# Use variables price, bedrooms, accommodates, review_scores_value, review_scores_rating, review_scores_cleanliness, reviews_per_month to predict whether the room has both dining table and wine glasses
data_knn_1 = copenhagen %>%
select(price,
bedrooms,
accommodates,
review_scores_value,
review_scores_rating,
review_scores_cleanliness,
reviews_per_month,
amenities)
# 2. Omit all the NA values
data_knn_1 = na.omit(data_knn_1)
# 3. Create target variable: have both dining table and wine glasses
data_knn_1 = data_knn_1 %>%
mutate(Dine_Wine = grepl("Dining table", amenities, ignore.case = TRUE)
& grepl("Wine glasses", amenities, ignore.case = TRUE))
table(data_knn_1$Dine_Wine)
data_knn_1$Dine_Wine = as.factor(data_knn_1$Dine_Wine)
# 4. Data Partition
set.seed(5)
train.index = sample(c(1:nrow(data_knn_1)), nrow(data_knn_1)*0.6)
train.set = data_knn_1[train.index,]
valid.set = data_knn_1[-train.index,]
# 5. Doing t-tests to test the significance of input variables
train.true = train.set %>%
filter(Dine_Wine == 'TRUE')
train.false = train.set %>%
filter(Dine_Wine == 'FALSE')
price_test = t.test(train.true$price,
train.false$price)
price_test
bedrooms_test = t.test(train.true$bedrooms,
train.false$bedrooms)
bedrooms_test
accommadates_test = t.test(train.true$accommodates,
train.false$accommodates)
accommadates_test
review_value_test = t.test(train.true$review_scores_value,
train.false$review_scores_value)
review_value_test
review_rate_test = t.test(train.true$review_scores_rating,
train.false$review_scores_rating)
review_rate_test
review_clean_test = t.test(train.true$review_scores_cleanliness,
train.false$review_scores_cleanliness)
review_clean_test
review_per_test = t.test(train.true$reviews_per_month,
train.false$reviews_per_month)
review_per_test
# 6. Normalize the data
library(caret)
norm.values = preProcess(train.set[, 1:ncol(train.set)],
method = c("center", "scale"))
train.norm.set = predict(norm.values, train.set[, 1:ncol(train.set)])
valid.norm.set = predict(norm.values, valid.set[, 1:ncol(valid.set)])
knn.norm.raw = predict(norm.values, data_knn_1[, 1:ncol(data_knn_1)])
# Build a KNN model to predict a new record (k=7)
new_rental = data.frame(price = 2800,
bedrooms = 4,
accommodates = 8,
review_scores_value = 4.85,
review_scores_rating = 4.90,
review_scores_cleanliness = 4.80,
reviews_per_month = 0.50)
new_norm_rental = predict(norm.values, new_rental)
library(FNN)
nn = knn(train = train.norm.set[, 1:7],
test = new_norm_rental,
cl = train.norm.set[[9]],
k = 7)
nn
# 7. Try to find the optimal k-value
accuracy.df = data.frame(k = seq(1,20,1), accuracy = rep(0,20))
for (i in 1:20) {
knn.pred = knn(train.norm.set[, 1:7],
valid.norm.set[, 1:7],
cl = train.norm.set[[9]],
k = i)
accuracy.df[i,2] = confusionMatrix(knn.pred, valid.norm.set[[9]])$overall[1]
}
accuracy.df
accuracy.df %>%
filter(accuracy == max(accuracy, na.rm = TRUE))
# 8. It seems that k=16 gives the best result, so run knn again
nn_2 = knn(train = train.norm.set[, 1:7],
test = new_norm_rental,
cl = train.norm.set[[9]],
k = 16)
nn_2
# 9. Compare again the naive benchmark
knn_pred = knn(
train = train.norm.set[, 1:7],
test = valid.norm.set[, 1:7],
cl = train.norm.set[[9]],
k = 16
)
mean(knn_pred == valid.norm.set[[9]])
most_common = names(which.max(table(train.norm.set[[9]])))
naive_pred = rep(most_common, nrow(valid.norm.set))
mean(naive_pred == valid.norm.set[[9]])
# 1. Selecting input categorical variables
naive_raw = copenhagen %>%
select(neighbourhood_cleansed,
property_type,
room_type,
bathrooms_text,
review_scores_value)
# 2. Binning the response variable review_score_value
naive_raw <- naive_raw %>%
mutate(
value_bin = ntile(review_scores_value, 3),
value_label = case_when(
value_bin == 1 ~ "not good",
value_bin == 2 ~ "medium",
value_bin == 3 ~ "good"
)
)
# 3. Look at the data and do necessary adjustments
str(naive_raw)
sapply(naive_raw, n_distinct)
naive_raw %>%
count(property_type, sort = TRUE)
naive_raw %>%
count(bathrooms_text, sort = TRUE)
naive_raw %>%
count(neighbourhood_cleansed, sort = TRUE)
# To keep top 10 values for property_type and bathrooms_text
top10_col1 = naive_raw %>%
count(property_type, sort = TRUE) %>%
slice_head(n = 10)
top10_col2 = naive_raw %>%
count(bathrooms_text, sort = TRUE) %>%
slice_head(n = 10)
naive_clean = naive_raw %>%
filter(
property_type %in% top10_col1$property_type,
bathrooms_text %in% top10_col2$bathrooms_text
)
sapply(naive_clean, n_distinct)
# delete NA value in value_label and delete the original value columns
naive_clean = naive_clean %>%
filter(!is.na(value_label))
sapply(naive_clean, n_distinct)
naive_clean = naive_clean %>%
select(
-review_scores_value,
-value_bin
)
naive_clean$value_label = as.factor(naive_clean$value_label)
class(naive_clean$value_label)
# 4. Visualize value_label by different input variables
pro_chart_function = function(i) {
df = naive_clean %>%
count(!!sym(i), value_label) %>%
group_by(!!sym(i)) %>%
mutate(proportion = n / sum(n)) %>%
ungroup()
chart = ggplot(df, aes(x = !!sym(i), y = proportion, fill = value_label)) +
geom_bar(stat = "identity", position = "fill") +
labs(y = "Proportion", x = paste(i), title = paste("Proportional Bar Plot for Consumer Value Perception by", i)) +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
return(chart)
}
pro_chart_function('neighbourhood_cleansed')
pro_chart_function('property_type')
pro_chart_function('room_type')
pro_chart_function('bathrooms_text')
# 5. Data Partitioning
set.seed(5)
train.index_naive = sample(c(1:nrow(naive_clean)),
nrow(naive_clean)*0.6)
train.set_naive = naive_clean[train.index_naive, ]
valid.set_naive = naive_clean[-train.index_naive, ]
# 6. Build the Naive Bayes Model
library(e1071)
value.nb = naiveBayes(value_label ~ .,
data = train.set_naive,
laplace = 1) # Laplace (add-one) smoothing
value.nb
# 7. Make a prediction based on a fictional rental scenario
fiction_rental = data.frame(
neighbourhood_cleansed = 'Valby',
property_type = 'Entire home',
room_type = 'Entire home/apt',
bathrooms_text = '1.5 shared baths'
)
fic.pred = predict(value.nb, fiction_rental)
fic.pred
# 8. Assessing the model
train_pred = predict(value.nb, train.set_naive)
train_matrix = confusionMatrix(train_pred, train.set_naive$value_label)
train_accuracy = train_matrix$overall['Accuracy']
train_accuracy
train_matrix
valid_pred = predict(value.nb, valid.set_naive)
valid_matrix = confusionMatrix(valid_pred, valid.set_naive$value_label)
valid_accuracy = valid_matrix$overall['Accuracy']
valid_accuracy
valid_matrix
library(caret)
library(rpart)
library(rpart.plot)
tree_data <- copenhagen %>%
select(host_is_superhost,
neighbourhood_cleansed,
host_identity_verified,
property_type,
room_type,
accommodates,
bathrooms,
bedrooms,
# beds,
price,
review_scores_rating,
has_availability,
minimum_nights)
sum(is.na(tree_data))
top_10_property <- tree_data %>%
count(property_type, sort = TRUE) %>%
slice(1:10)
tree_data = tree_data %>%
filter(property_type %in% top_10_property$property_type)
tree_data$host_identity_verified <- factor(tree_data$host_identity_verified)
tree_data$property_type <- factor(tree_data$property_type)
tree_data$neighbourhood_cleansed <- factor(tree_data$neighbourhood_cleansed)
tree_data$host_is_superhost <- factor(tree_data$host_is_superhost)
tree_data$has_availability <- factor(tree_data$has_availability)
# Bin the minimum nights into 3 bins
tree_data$rental_type <- cut(tree_data$minimum_nights,
breaks = c(0, 2, 5, Inf),
labels = c("Short", "Medium", "Long"))
table(tree_data$rental_type)
tree_data <- tree_data %>% select(-minimum_nights)
summary(tree_data)
# data partitioning
set.seed(79)
tree_data.index <- sample(c(1:nrow(tree_data)), nrow(tree_data)*0.6)
train_set <- tree_data[tree_data.index, ]
valid_set <- tree_data[-tree_data.index, ]
tree_model <- rpart(rental_type ~ .,
method = 'class',
data = train_set)
rpart.rules(tree_model)
rpart.plot(tree_model)
#cross validation
five_fold_cv <-
rpart(rental_type ~ .,method = 'class', cp=0.00001, minsplit=5, xval=5,  data = train_set)
a <- printcp(five_fold_cv)
# Re-build model with optimal CP value
pruned.ct <- prune(five_fold_cv,
cp=five_fold_cv$cptable[which.min(five_fold_cv$cptable[,"xerror"]),"CP"])
rpart.plot(pruned.ct,type=5, extra = 2, fallen.leaves=FALSE)
head(rpart.rules(pruned.ct))
#assessing model
# Training Set
train.pred <- predict(pruned.ct, train_set, type="class")
confusionMatrix(train.pred, train_set$rental_type)
# Validation Set
valid.pred <- predict(pruned.ct, valid_set, type="class")
confusionMatrix(valid.pred, valid_set$rental_type)
# Select features
features <- copenhagen %>%
select(price, number_of_reviews, review_scores_rating, minimum_nights, bedrooms,
neighbourhood_cleansed, room_type)
#  Check NAs
colSums(is.na(features))
#  Create new feature: price_per_bedroom
features <- features %>%
mutate(price_per_bedroom = price / bedrooms)
features$price_per_bedroom[is.infinite(features$price_per_bedroom)] <- NA
features <- features %>% drop_na()
# Convert categorical columns to factors
features$neighbourhood_cleansed <- as.factor(features$neighbourhood_cleansed)
features$room_type <- as.factor(features$room_type)
# Create dummy variables
dummies <- dummyVars(~ neighbourhood_cleansed + room_type, data = features)
categorical_data <- as.data.frame(predict(dummies, newdata = features))
# Select numeric variables
numeric_data <- features %>%
select(price_per_bedroom, number_of_reviews, review_scores_rating, minimum_nights)
#  Combine data
final_data <- bind_cols(numeric_data, categorical_data)
# Scale the final dataset
scaled_data <- scale(final_data)
colSums(is.na(features))
set.seed(9)
wss <- vector()
for (k in 1:10) {
kmeans_model <- kmeans(scaled_data, centers = k, nstart = 25)
wss[k] <- kmeans_model$tot.withinss
}
# Elbow plot
plot(1:10, wss, type = "b", pch = 19,
xlab = "Number of Clusters (k)",
ylab = "Total Within-Cluster Sum of Squares",
main = "Elbow Method for Choosing k")
set.seed(9)
kmeans_model <- kmeans(scaled_data, centers = 3, nstart = 25)
features$cluster <- as.factor(kmeans_model$cluster)
# features
View(features %>%
group_by(cluster) %>%
summarise(across(everything(), list(mean = mean), .names = "mean_{col}")))
ggplot(features, aes(x = cluster)) +
geom_bar(fill = "skyblue") +
geom_text(stat = "count", aes(label = ..count..), vjust = -0.5, size = 3) +
labs(title = "Number of Listings per Cluster",
x = "Cluster",
y = "Count of Listings") +
theme_minimal()
ggplot(features %>% filter(price < 10000),
aes(x=room_type, y=price, fill=cluster)) +
geom_bar(position="dodge", stat="identity") +
labs(title = "Distribution of price within different room types per Cluster (price < 10000)",
x = "Room Type",
y = "Price") +
theme_minimal()
ggplot(features %>% filter(price < 5000),
aes(x = price, y = review_scores_rating, color = cluster)) +
geom_point(alpha = 0.7, size = 1.5) +
scale_x_continuous(breaks = seq(0, 5000, by = 500)) +
scale_y_continuous(breaks = seq(0, 5, by = 1)) +
labs(
title = "K-Means Clustering: Price per Bedroom vs Rating (price < 5000)",
x = "Price per Bedroom",
y = "Review Score Rating"
) +
theme_minimal()
copenhagen$price <- as.numeric(gsub("[$,]", "", copenhagen$price))
avgpricebyneighborhood <- copenhagen %>% group_by(neighbourhood_cleansed) %>%
summarise(mean(price))
avgpricebyneighborhood
# features
features %>%
group_by(cluster) %>%
summarise(across(everything(), list(mean = mean), .names = "mean_{col}"))
na_summary <- colSums(is.na(copenhagen)) %>%
sort(decreasing = TRUE) %>%
as.data.frame() %>%
tibble::rownames_to_column(var = "Column") %>%
rename(Missing_Count = ".")
print(na_summary)
na_summary <- colSums(is.na(copenhagen)) %>%
sort(decreasing = TRUE) %>%
as.data.frame() %>%
tibble::rownames_to_column(var = "Column") %>%
rename(Missing_Count = ".") %>%
mutate(Missing_Percent = round((Missing_Count / nrow(copenhagen)) * 100, 2))
print(na_summary)
knitr::kable(na_summary, caption = "Missing Value Summary")
knitr::kable(na_summary, caption = "Missing Value Summary")
DT::datatable(na_summary, caption = "Missing Value Summary")
library(ggplot2)
library(reshape2)
# Compute correlation matrix
cormat <- cor(cordata, use = "pairwise.complete.obs")
# Convert to long format
melted_cormat <- melt(cormat)
# Heatmap
ggplot(melted_cormat, aes(Var1, Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name = "Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
coord_fixed() +
labs(title = "Correlation Heatmap")
library(ggplot2)
library(reshape2)
# Compute correlation matrix
cormat <- cor(cordata, use = "pairwise.complete.obs")
# Convert to long format
melted_cormat <- melt(cormat)
# Heatmap
ggplot(melted_cormat, aes(Var1, Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name = "Correlation") +
theme_minimal() +
labs(title = "Correlation Heatmap")
library(ggplot2)
library(reshape2)
# Compute correlation matrix
cormat <- cor(cordata, use = "pairwise.complete.obs")
# Convert to long format
melted_cormat <- melt(cormat)
# Heatmap
ggplot(melted_cormat, aes(Var1, Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name = "Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
coord_fixed() +
labs(title = "Correlation Heatmap")
library(ggplot2)
library(reshape2)
# Compute correlation matrix
cormat <- cor(cordata)
# Convert to long format
melted_cormat <- melt(cormat)
# Heatmap
ggplot(melted_cormat, aes(Var1, Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name = "Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
coord_fixed() +
labs(title = "Correlation Heatmap")
setwd("~/Documents/portfolio-quarto/projects/pokemon")
library(tidyverse)
library(dplyr)
library(caret)
library(ggplot2)
library(ggdendro)
pokemon <- read.csv('all_pokemon_data.csv')
DT::datatable(head(pokemon, 20),
options = list(pageLength = 5, scrollX = TRUE),
caption = "Preview of Pokémon Dataset")
dim(pokemon)
set.seed(79)
random_sample <- pokemon[sample(nrow(pokemon), 20), ]
sum(is.na(random_sample))
DT::datatable(head(random_sample, 5),
options = list(scrollX = TRUE)
)
summary(random_sample)
rand_sp_numeric <- random_sample %>%
select (Name, Catch.Rate, Special.Defense, Special.Attack, Speed, Health)
rownames(rand_sp_numeric) <- rand_sp_numeric$Name
rand_sp_numeric
rand_sp_numeric <- rand_sp_numeric %>% select(-Name)
summary(rand_sp_numeric)
head(rand_sp_numeric)
norm <- preProcess(rand_sp_numeric, method = c("center", "scale"))
random_sample_norm <- predict(norm, rand_sp_numeric)
random_sample_norm
d.norm <- dist(random_sample_norm, method="euclidean")
# d.norm
hc <- hclust(d.norm, method="average")
ggdendrogram(hc)
ggdendrogram(hc) +
geom_hline(yintercept = 3, color = "red", linetype = "dashed")
clusters <- cutree(hc, k = 5)
clusters
rand_sp_numeric$cluster <- clusters
cluster_mean <- rand_sp_numeric %>% group_by(cluster) %>%
summarise_all(mean)
cluster_mean
rand_sp_numeric$cluster <- factor(rand_sp_numeric$cluster)
ggplot(rand_sp_numeric, aes(x=Speed, y=Catch.Rate, color=cluster)) +
geom_point()
ggplot(rand_sp_numeric, aes(x=Special.Attack, y=Health, color=cluster)) +
geom_point()
ggplot(rand_sp_numeric, aes(x=cluster, y=Speed)) +
geom_boxplot()
rand_sp_numeric[5,]
rand_sp_numeric %>% filter(cluster==2)
random_sample.weighted <- random_sample_norm
random_sample.weighted$Special.Attack <- random_sample.weighted$Special.Attack*30
random_sample.weighted$Special.Defense <- random_sample.weighted$Special.Defense*25
random_sample.weighted$Speed <- random_sample.weighted$Speed*20
random_sample.weighted$Health <- random_sample.weighted$Health*10
random_sample.weighted$Catch.Rate <- random_sample.weighted$Catch.Rate*5
d.norm2 <- dist(random_sample.weighted, method="euclidean")
hc2 <- hclust(d.norm2, method="average")
ggdendrogram(hc2)
clusters2 <- cutree(hc2, k = 4)
clusters2
rand_sp_numeric$cluster <- clusters2
rand_sp_numeric %>% group_by(cluster) %>%
summarise_all(mean)
rand_sp_numeric[5,]
rand_sp_numeric %>% filter(cluster==3)
summary_tbl <- random_sample %>%
summarise(across(everything(), ~ list(summary(.)))) %>%
pivot_longer(everything(), names_to = "Variable", values_to = "Summary") %>%
mutate(Summary = map_chr(Summary, ~ paste(.x, collapse = ", ")))
# Display with DT
DT::datatable(summary_tbl,
options = list(pageLength = 10, scrollX = TRUE),
caption = "Summary Statistics of Pokémon Dataset")
library(skimr)
install.packages("skimr")
library(skimr)
skim(pokemon)
skim(random_sample)
library(skimr)
skim(random_sample)
DT::datatable(head(random_sample, 5),
options = list(scrollX = TRUE, searching = FALSE)
)
DT::datatable(head(random_sample, 5),
options = list(scrollX = TRUE, searching = FALSE, info = FALSE)
)
DT::datatable(head(random_sample, 5),
options = list(scrollX = TRUE, searching = FALSE,info = FALSE)
)
DT::datatable(head(random_sample, 5),
options = list(scrollX = TRUE, searching = FALSE, paging = FALSE, info = FALSE)
)
DT::datatable(head(random_sample, 5),
options = list(scrollX = TRUE, searching = FALSE, paging = FALSE)
)
DT::datatable(head(random_sample, 5),
options = list(scrollX = TRUE, searching = FALSE, paging = FALSE, info = FALSE)
)
