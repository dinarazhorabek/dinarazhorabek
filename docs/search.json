[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "During the final year of my bachelor’s studies, I began working full-time as a Full-Stack Software Engineer in the banking industry at VTB Bank Kazakhstan. In this role, I applied my academic knowledge in real-world scenarios and gained significant experience. In this role, I developed web applications from the ground up, enhanced existing ones, and managed databases. My work involved third-party API integrations, performance tuning, and seamless deployment into production environments. I led frontend and backend development using technologies such as Java, Node.js, React, and Oracle APEX, and worked extensively with PostgreSQL and Oracle databases. Additionally, I utilized tools like DBMS Profiler and Query Analyzer to optimize query performance and ensure system reliability.\nThis experience led me to discover a fascination with data management and analytics — the way data surrounds us and can yield powerful insights into trends, behaviors, and decision-making. This interest inspired me to dive deeper into data analytics through online courses, and ultimately, to pursue a graduate degree in Applied Business Analytics at Boston University. So now, I’m a graduate student:)"
  },
  {
    "objectID": "projects/nhl_players/nhl_players_analysis.html",
    "href": "projects/nhl_players/nhl_players_analysis.html",
    "title": "Forecasting the salaries of hockey players",
    "section": "",
    "text": "This project focuses on forecasting hockey player salaries by utilizing statistical analysis and machine learning methods. By analyzing player performance metrics, historical salary information, and other relevant factors, the project aims to deliver precise salary predictions.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nSimple Linear Regression\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(gplots)\n\nmain_data &lt;- read.csv('nhl_players.csv')\n\nBy looking at dataset, we can see that all variables that are represented by number are numeric. Even the salary column is represented as character because of the dollar sign, it is also numeric. And all other remaining variables are categorical.\nCategorical: name, Team_y, Position_y, Handed\nNumeric: GP, G, A, P, Sh, Sh_perc, Salary, PIM, Giveaways, Takeaways, Hits, Hits.Taken, blocked_shots, PlusMinus\nDiscrete numeric: GP, G, A, P, Sh, Giveaways, Takeaways, Hits, Hits.Taken, blocked_shots, PlusMinus Continuous numeric: Sh_perc, Salary, PIM\n\nglimpse(main_data)\n\nRows: 568\nColumns: 18\n$ name          &lt;chr&gt; \"Zemgus Girgensons\", \"Zack Smith\", \"Zack Kassian\", \"Zach…\n$ Team_y        &lt;chr&gt; \"BUF\", \"CHI\", \"EDM\", \"MIN\", \"TOR\", \"BUF, T.B\", \"BUF, T.B…\n$ Position_y    &lt;chr&gt; \"C\", \"C\", \"W\", \"W\", \"W\", \"D\", \"D\", \"C\", \"C\", \"W\", \"C\", \"…\n$ HANDED        &lt;chr&gt; \"Left\", \"Left\", \"Right\", \"Left\", \"Right\", \"Right\", \"Righ…\n$ GP            &lt;int&gt; 68, 50, 59, 69, 51, 27, 27, 57, 70, 68, 63, 70, 55, 68, …\n$ G             &lt;int&gt; 12, 4, 15, 25, 21, 1, 1, 6, 10, 31, 15, 7, 4, 8, 17, 1, …\n$ A             &lt;int&gt; 7, 7, 19, 21, 16, 6, 6, 7, 20, 28, 31, 12, 17, 17, 14, 7…\n$ P             &lt;int&gt; 19, 11, 34, 46, 37, 7, 7, 13, 30, 59, 46, 19, 21, 25, 31…\n$ Sh            &lt;int&gt; 85, 43, 99, 155, 106, 29, 29, 98, 110, 197, 138, 99, 62,…\n$ Sh_perc       &lt;dbl&gt; 0.14, 0.09, 0.15, 0.16, 0.20, 0.03, 0.03, 0.06, 0.09, 0.…\n$ SALARY        &lt;chr&gt; \"$1,600,000\", \"$3,250,000\", \"$2,000,000\", \"$9,000,000\", …\n$ PIM           &lt;int&gt; 10, 29, 69, 8, 23, 22, 22, 28, 49, 12, 16, 39, 6, 66, 45…\n$ Giveaways     &lt;int&gt; 11, 14, 45, 22, 16, 11, 11, 21, 14, 41, 37, 8, 27, 18, 2…\n$ Takeaways     &lt;int&gt; 13, 21, 26, 21, 32, 4, 4, 20, 48, 42, 54, 22, 10, 27, 25…\n$ Hits          &lt;int&gt; 110, 112, 157, 27, 52, 30, 30, 136, 78, 9, 40, 213, 31, …\n$ Hits.Taken    &lt;int&gt; 71, 71, 54, 60, 101, 21, 21, 99, 114, 66, 94, 129, 54, 6…\n$ blocked_shots &lt;int&gt; 20, 18, 8, 38, 23, 27, 27, 30, 20, 14, 38, 11, 75, 29, 4…\n$ PlusMinus     &lt;int&gt; -1, 2, 0, -11, 13, 0, 0, 6, -5, -2, 11, 0, -8, -21, -5, …\n\n\nOur dataset doesn’t have any NA values.\n\ncolSums(is.na(main_data))\n\n         name        Team_y    Position_y        HANDED            GP \n            0             0             0             0             0 \n            G             A             P            Sh       Sh_perc \n            0             0             0             0             0 \n       SALARY           PIM     Giveaways     Takeaways          Hits \n            0             0             0             0             0 \n   Hits.Taken blocked_shots     PlusMinus \n            0             0             0 \n\n\nI renamed the columns ‘Team_y’, ‘Position_y’ by removing last two characters, since they are not useful at all.\n\nmain_data &lt;- main_data %&gt;% rename(Team = Team_y, Position = Position_y)\nnames(main_data)\n\n [1] \"name\"          \"Team\"          \"Position\"      \"HANDED\"       \n [5] \"GP\"            \"G\"             \"A\"             \"P\"            \n [9] \"Sh\"            \"Sh_perc\"       \"SALARY\"        \"PIM\"          \n[13] \"Giveaways\"     \"Takeaways\"     \"Hits\"          \"Hits.Taken\"   \n[17] \"blocked_shots\" \"PlusMinus\"    \n\n\nThere are 7 duplicated name values found, after removing them we had 561 values in dataset.\n\nmain_data$name[duplicated(main_data$name)]\n\n[1] \"Zach Bogosian\"     \"Valeri Nichushkin\" \"Troy Brouwer\"     \n[4] \"Paul Byron\"        \"Kevin Shattenkirk\" \"Corey Perry\"      \n[7] \"Andrej Sekera\"    \n\nclean_data &lt;- main_data %&gt;% distinct(name, .keep_all = TRUE)\nnrow(clean_data)\n\n[1] 561\n\n\nAfter clearing up the Salary variable from specific characters, I converted it to numeric type.\n\nclean_data$SALARY &lt;- as.numeric(gsub(\"[$,]\", \"\", clean_data$SALARY))\nstr(clean_data)\n\n'data.frame':   561 obs. of  18 variables:\n $ name         : chr  \"Zemgus Girgensons\" \"Zack Smith\" \"Zack Kassian\" \"Zach Parise\" ...\n $ Team         : chr  \"BUF\" \"CHI\" \"EDM\" \"MIN\" ...\n $ Position     : chr  \"C\" \"C\" \"W\" \"W\" ...\n $ HANDED       : chr  \"Left\" \"Left\" \"Right\" \"Left\" ...\n $ GP           : int  68 50 59 69 51 27 57 70 68 63 ...\n $ G            : int  12 4 15 25 21 1 6 10 31 15 ...\n $ A            : int  7 7 19 21 16 6 7 20 28 31 ...\n $ P            : int  19 11 34 46 37 7 13 30 59 46 ...\n $ Sh           : int  85 43 99 155 106 29 98 110 197 138 ...\n $ Sh_perc      : num  0.14 0.09 0.15 0.16 0.2 0.03 0.06 0.09 0.16 0.11 ...\n $ SALARY       : num  1600000 3250000 2000000 9000000 2500000 6000000 1000000 6300000 9000000 5900000 ...\n $ PIM          : int  10 29 69 8 23 22 28 49 12 16 ...\n $ Giveaways    : int  11 14 45 22 16 11 21 14 41 37 ...\n $ Takeaways    : int  13 21 26 21 32 4 20 48 42 54 ...\n $ Hits         : int  110 112 157 27 52 30 136 78 9 40 ...\n $ Hits.Taken   : int  71 71 54 60 101 21 99 114 66 94 ...\n $ blocked_shots: int  20 18 8 38 23 27 30 20 14 38 ...\n $ PlusMinus    : int  -1 2 0 -11 13 0 6 -5 -2 11 ...\n\n\nData partitioning helps prevent biased decisions by ensuring that insights from training dataset also applicable to validation set. If we analyze first, we may unintentionally use insights that we think are applicable for every scenario while it can lead to overfitting. By partitioning first, we can ensure that our tests on training and validation sets provide independent performance measures.\n\nset.seed(79)\nnhl.index &lt;- sample(c(1:nrow(clean_data)), nrow(clean_data)*0.6)\nnhl_train.df &lt;- clean_data[nhl.index, ]\nnhl_valid.df &lt;- clean_data[-nhl.index, ]\n\nFrom the plot below we can see that most of players with small salary also have small number of points, and by increase of total points salary also going up. However despite the total points, it seems like other parameters also affect the salary. Because in some cases even the player has not so high points, the salary is extremely large number. For instance, let’s look at X=60, where we can see that there is one player with very high salary around 16000000, while majority’s salary below 10 million. Maybe other factors such as budget of the team, position type, total number of games played have more impact to the salary. A player with more games may be valued higher due to greater experience, and their impact on team performance could also be considered in the evaluation.\n\n\n\n\n\n\n\n\n\nCorrelation is the linear relationship between two continuous variables. Pearson’s correlation measures strength of that relationship.\n\ncor(nhl_train.df$SALARY, nhl_train.df$P)\n\n[1] 0.6699033\n\ncor.test(nhl_train.df$SALARY, nhl_train.df$P)\n\n\n    Pearson's product-moment correlation\n\ndata:  nhl_train.df$SALARY and nhl_train.df$P\nt = 16.49, df = 334, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6063712 0.7249371\nsample estimates:\n      cor \n0.6699033 \n\n\nCorrelation value here is 0.67, which is not strong(&lt;0.7) but around that value. High t-value and very low p-value suggests correlation is significant, meaning we can reject null hypothesis that there is no correlation between Price and Salary.\n\nmodel &lt;- lm(SALARY ~ P, data = nhl_train.df)\nsummary(model)\n\n\nCall:\nlm(formula = SALARY ~ P, data = nhl_train.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-4686187 -1402916  -578619  1201397  9209631 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1311280     186918   7.015 1.28e-11 ***\nP              99477       6033  16.490  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2220000 on 334 degrees of freedom\nMultiple R-squared:  0.4488,    Adjusted R-squared:  0.4471 \nF-statistic: 271.9 on 1 and 334 DF,  p-value: &lt; 2.2e-16\n\n\nFor highest residual value in the model: Actual salary:14,500,000 Predicted salary:5,290,369 Residual is difference between actual and predicted values, which is 9,209,631 for this player.\n\nmax(residuals(model))\n\n[1] 9209631\n\nmax_res_index &lt;- which.max(residuals(model))\nactual_data_max_res &lt;- nhl_train.df[max_res_index, ]\nactual_data_max_res$SALARY\n\n[1] 14500000\n\npredict(model, newdata = actual_data_max_res)\n\n    371 \n5290369 \n\n\nFor lowest residual value in the model: Actual Salary: 1,400,000 Predicted Salary: 6,086,187 From this record we can determine which value was subtracted from another, so residual = actual - predicted = 1400000 - 6086187 = -4686187\n\nmin(residuals(model))\n\n[1] -4686187\n\nmin_res_index &lt;- which.min(residuals(model))\nactual_data_min_res &lt;- nhl_train.df[min_res_index, ]\nactual_data_min_res$SALARY\n\n[1] 1400000\n\npredict(model, newdata = actual_data_min_res)\n\n     33 \n6086187 \n\n\nBesides Points the number of games played, shot percentage, penalties in minutes can also impact salary. More games played more reliable player looks like, higher shot percentage shows higher efficiency of scoring, more penalties can negatively impact team. The player’s performance, and defensive skills could have more impact. Even if a player just joined the team, his strong impact on team performance and outstanding gameplay can boost their popularity. The increased demand may attract interest from other team managers which definitely influence the player’s value.\n\nsummary(model)\n\n\nCall:\nlm(formula = SALARY ~ P, data = nhl_train.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-4686187 -1402916  -578619  1201397  9209631 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1311280     186918   7.015 1.28e-11 ***\nP              99477       6033  16.490  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2220000 on 334 degrees of freedom\nMultiple R-squared:  0.4488,    Adjusted R-squared:  0.4471 \nF-statistic: 271.9 on 1 and 334 DF,  p-value: &lt; 2.2e-16\n\n\nThe regression equation is 1311280 + 99477*P From the equation, we see that even if the player doesn’t have any points he will start with 1,311,280 salary. And each earned point will increase that minimum salary by 99,477. Let’s assume P=10 –&gt; Salary=2,306,050.\nSince we are using our model to predict value, we need to be sure that we are not overfitting our data. Overfitting would make the model ineffective, as it would perform well on training data but fail to new, unseen data.\n\ntrain &lt;- predict(model, nhl_train.df)\nvalid &lt;- predict(model, nhl_valid.df)\n\n# Training Set\naccuracy(train, nhl_train.df$SALARY)\n\n                       ME    RMSE     MAE       MPE     MAPE\nTest set 0.00000000443383 2213352 1688599 -51.21713 77.33822\n\n# Validation Set\naccuracy(valid, nhl_valid.df$SALARY)\n\n                ME    RMSE     MAE       MPE     MAPE\nTest set -30878.14 2126314 1659595 -47.01189 71.70397\n\n\nThe values above show overall measures of predictive accuracy. RMSE value for validation data (2126314) is smaller than for the training data, which is 2213352. However both values are close, which is indicates that model is not overfitting. Mean absolute error for holdout set (1659595) also smaller than the value for training set (1688599). Thus, we actually see less error on validation data.\nLet’s compare RMSE to the standard deviation of training set. Both values are very close, and relatively accurate since SD tells us how much variable’s value differ from its mean value. If the RMSE higher than SD, model’s predictions are not much better than using the mean value of the dataset as a predictor.\n\nsd(nhl_train.df$SALARY)\n\n[1] 2985599\n\n2213352/sd(nhl_train.df$SALARY)\n\n[1] 0.7413428\n\n2126314/sd(nhl_train.df$SALARY)\n\n[1] 0.7121902\n\n\n\n\nMultiple Linear Regression\n\nlibrary(gplots)\nlibrary(visualize)\n\n\nnhl_train_numbers &lt;- nhl_train.df %&gt;% select(-name, -Team, -Position, -HANDED)\ncor_table &lt;- nhl_train_numbers %&gt;% cor()\n# cor_table\n\nheatmap.2(cor_table, Rowv=FALSE, Colv=FALSE, dendrogram=\"none\", trace = \"none\", cellnote=round(cor_table,2), notecol = \"black\", density.info = \"none\")\n\n\n\n\n\n\n\n\nFrom heatmap we can see correlation value between variables in our dataset. The Goal, Assists number, total shots and number of takeaways and points are strongly correlated between each other (&gt;0.7). The assists number, shots and giveaways number also strongly correlated. While shot percentage negatively impacts blocked shots number, PlusMinus have very small connection with all remaining variables. Here, we can observe multicolinearity since Points is the sum of Goals and Assists, making them dependent variables. Similarly, Shot Percentage is derived by dividing Shots to Goals. Since Shots represent the number of times a player attempts to score, and Points are the sum of goals and assists, these numbers are interconnected. So Shots can cause Goals, and when a player scores a Goal, an Assist should be credited to the player, the sum of these two numbers are represented as Points. Since we can’t use dependent variables as inputs in linear model, let’s keep Points as it holds more value than total shots, as a player may take many shots without successfully scoring a goal. Also it is more correlated to output variable.\n\n\n\n\n\n\n\n\n\nIn new heatmap, we can see that Takeaways and Points are highly correlated (=0.8). Maybe these numbers are not dependent, but when player took a puck from an opposite it can lead to goal. Let’s remove Takeaways from our model. The player with high giveaways have a tendency to lose a puck more often, which can decrease team’s performance. Which also can affect Points earned. Also let’s remove Hits.Taken since its highly correlated with Games Played (=0.71). More games played more possibility to make a contact with the player who has the puck. And let’s build model with remaining variables, and use backward elimination.\n\nnhl_train_numbers &lt;- nhl_train_numbers %&gt;% select(-Takeaways, -Giveaways, -Hits.Taken)\nnhl_train_numbers %&gt;% cor()\n\n                      GP          P      SALARY         PIM        Hits\nGP            1.00000000 0.67582735  0.42421317 0.520448294  0.49558958\nP             0.67582735 1.00000000  0.66990334 0.305617550  0.08245484\nSALARY        0.42421317 0.66990334  1.00000000 0.198372387 -0.02394758\nPIM           0.52044829 0.30561755  0.19837239 1.000000000  0.56542977\nHits          0.49558958 0.08245484 -0.02394758 0.565429770  1.00000000\nblocked_shots 0.48789199 0.14605555  0.22171296 0.304039783  0.36510186\nPlusMinus     0.07172414 0.24643918  0.07974078 0.003629743  0.03591750\n              blocked_shots   PlusMinus\nGP                0.4878920 0.071724142\nP                 0.1460555 0.246439184\nSALARY            0.2217130 0.079740779\nPIM               0.3040398 0.003629743\nHits              0.3651019 0.035917499\nblocked_shots     1.0000000 0.122277735\nPlusMinus         0.1222777 1.000000000\n\n\nWhen categorical variables used as predictors, we convert them into dummy variables. A variable with n categories will have n-1 dummy variables, and remaining one value will be as reference level. This helps in analyzing the impact of categorical predictors on the dependent variable.\n\nnhl_train.df &lt;- nhl_train.df %&gt;% select(-G, -A, -Sh, -Sh_perc, -Takeaways, -Giveaways, -Hits.Taken)\nnhl_train.df &lt;- nhl_train.df %&gt;% select(-name)\n\nmodel1 &lt;- step(lm(SALARY~., data = nhl_train.df), direction = \"backward\")\n\nStart:  AIC=9856.2\nSALARY ~ Team + Position + HANDED + GP + P + PIM + Hits + blocked_shots + \n    PlusMinus\n\n                Df       Sum of Sq        RSS    AIC\n- Team          60 263015531187124 1.4790e+15 9802.0\n- Position       2     39614240556 1.2161e+15 9852.2\n- HANDED         1   3453363733890 1.2195e+15 9855.1\n- PIM            1   6847508251807 1.2229e+15 9856.1\n&lt;none&gt;                             1.2160e+15 9856.2\n- Hits           1  16635747374161 1.2327e+15 9858.8\n- GP             1  18064821999018 1.2341e+15 9859.2\n- PlusMinus      1  38987510345295 1.2550e+15 9864.8\n- blocked_shots  1  41997523523384 1.2580e+15 9865.6\n- P              1 546410473355975 1.7624e+15 9978.9\n\nStep:  AIC=9801.99\nSALARY ~ Position + HANDED + GP + P + PIM + Hits + blocked_shots + \n    PlusMinus\n\n                Df       Sum of Sq        RSS    AIC\n- Position       2   2985822004712 1.4820e+15 9798.7\n- PIM            1   2443977770222 1.4815e+15 9800.5\n- HANDED         1   4399244297709 1.4834e+15 9801.0\n&lt;none&gt;                             1.4790e+15 9802.0\n- Hits           1  14816636482001 1.4939e+15 9803.3\n- GP             1  22027438385796 1.5011e+15 9805.0\n- blocked_shots  1  40378661481616 1.5194e+15 9809.0\n- PlusMinus      1  42203193555354 1.5212e+15 9809.4\n- P              1 689163748993858 2.1682e+15 9928.5\n\nStep:  AIC=9798.67\nSALARY ~ HANDED + GP + P + PIM + Hits + blocked_shots + PlusMinus\n\n                Df       Sum of Sq        RSS    AIC\n- PIM            1   2493780976016 1.4845e+15 9797.2\n- HANDED         1   4714736033909 1.4867e+15 9797.7\n&lt;none&gt;                             1.4820e+15 9798.7\n- Hits           1  16381671703132 1.4984e+15 9800.4\n- GP             1  26188587267788 1.5082e+15 9802.6\n- PlusMinus      1  41950294751938 1.5240e+15 9806.0\n- blocked_shots  1 110654200590976 1.5927e+15 9820.9\n- P              1 699222750683931 2.1812e+15 9926.5\n\nStep:  AIC=9797.23\nSALARY ~ HANDED + GP + P + Hits + blocked_shots + PlusMinus\n\n                Df       Sum of Sq        RSS    AIC\n- HANDED         1   5021715383231 1.4895e+15 9796.4\n&lt;none&gt;                             1.4845e+15 9797.2\n- Hits           1  13910288355677 1.4984e+15 9798.4\n- GP             1  24706476867436 1.5092e+15 9800.8\n- PlusMinus      1  44055357733151 1.5286e+15 9805.1\n- blocked_shots  1 112186182042815 1.5967e+15 9819.7\n- P              1 723087011778966 2.2076e+15 9928.6\n\nStep:  AIC=9796.37\nSALARY ~ GP + P + Hits + blocked_shots + PlusMinus\n\n                Df       Sum of Sq        RSS    AIC\n&lt;none&gt;                             1.4895e+15 9796.4\n- Hits           1  13058980965199 1.5026e+15 9797.3\n- GP             1  25658762191542 1.5152e+15 9800.1\n- PlusMinus      1  43964438030890 1.5335e+15 9804.1\n- blocked_shots  1 115091536261284 1.6046e+15 9819.4\n- P              1 725150613332920 2.2147e+15 9927.6\n\n\nBelow is the summary of our model. I didn’t include name of the player as an input. From the model we can see that Games Played, Hits, PlusMinus have negative impact on salary. Maybe because of the demand to new players, we got negative coef to GP.\n\nsummary(model1)\n\n\nCall:\nlm(formula = SALARY ~ GP + P + Hits + blocked_shots + PlusMinus, \n    data = nhl_train.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-5191324 -1186912  -546795  1046568  8061071 \n\nCoefficients:\n              Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)    1587099     305342   5.198 0.000000354 ***\nGP              -24975      10475  -2.384     0.01768 *  \nP               118004       9310  12.675     &lt; 2e-16 ***\nHits             -5146       3026  -1.701     0.08990 .  \nblocked_shots    21018       4162   5.050 0.000000734 ***\nPlusMinus       -36873      11815  -3.121     0.00196 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2125000 on 330 degrees of freedom\nMultiple R-squared:  0.5012,    Adjusted R-squared:  0.4936 \nF-statistic: 66.31 on 5 and 330 DF,  p-value: &lt; 2.2e-16\n\n\n\nmean_salary &lt;- mean(nhl_train.df$SALARY)\n\nsst &lt;- sum((nhl_train.df$SALARY - mean_salary)^2)\nsst\n\n[1] 2986122868827386\n\nssr &lt;- sum((model1$fitted.values-mean_salary)^2)\nssr\n\n[1] 1496580443217731\n\nssr/sst\n\n[1] 0.5011785\n\n\nThe final value is exactly same as r-squared value of the model.\n\nvisualize.t(stat=c(-2.384, 2.384), df=330, section=\"bounded\")\n\n\n\n\n\n\n\n\nt-value for GP is -2.384. After plotting distribution for that t-value, we can see that 98.2% of the curve is shaded. A bigger t-value occupy more space, and p-value goes lower. The remaining 1.8% (p-value) is the probability of obtaining a t-statistic beyond [-2.384, 2.384].\n\nsummary(model1)\n\n\nCall:\nlm(formula = SALARY ~ GP + P + Hits + blocked_shots + PlusMinus, \n    data = nhl_train.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-5191324 -1186912  -546795  1046568  8061071 \n\nCoefficients:\n              Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)    1587099     305342   5.198 0.000000354 ***\nGP              -24975      10475  -2.384     0.01768 *  \nP               118004       9310  12.675     &lt; 2e-16 ***\nHits             -5146       3026  -1.701     0.08990 .  \nblocked_shots    21018       4162   5.050 0.000000734 ***\nPlusMinus       -36873      11815  -3.121     0.00196 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2125000 on 330 degrees of freedom\nMultiple R-squared:  0.5012,    Adjusted R-squared:  0.4936 \nF-statistic: 66.31 on 5 and 330 DF,  p-value: &lt; 2.2e-16\n\n\nF-statistic: 66.31 F-statistic tests overall significance of the model. The better the fit, the higher the F-score will be.\n\n# F-statistic calculation\nk &lt;- 5\nn &lt;- 336\nsse &lt;- sum(model1$residuals^2)\n\nnumerator &lt;- ssr/k\ndenominator &lt;- sse / (n-k-1)\nnumerator / denominator\n\n[1] 66.31185\n\n\n\npredict(model1, newdata = data.frame(GP=82, P=60, Hits=150, blocked_shots=100, PlusMinus=20))\n\n      1 \n7211812 \n\n\nSo, by using the predict() function with random data the predicted salary is $7,211,812. It was found by using Regression Equation: 1587099-24975GP+118004P-5146Hits+21018blocked_shots-36873*PlusMinus\n\ntrain1 &lt;- predict(model1, nhl_train.df)\nvalid1 &lt;- predict(model1, nhl_valid.df)\n\n# Training Set\naccuracy(train1, nhl_train.df$SALARY)\n\n                        ME    RMSE     MAE       MPE     MAPE\nTest set 0.000000004849253 2105508 1592227 -46.79522 72.45567\n\n# Validation Set\naccuracy(valid1, nhl_valid.df$SALARY)\n\n              ME    RMSE     MAE       MPE     MAPE\nTest set 39476.6 1975654 1532076 -38.84478 64.69072\n\n\nWe got overall measures of predictive accuracy, now for MLR model. RMSE value for validation set (1975654) is also smaller than training set (2105508). Same with MAE, for training set is 1592227, and for validation set is 1532076. Small difference between these numbers can suggest that our model is not overfitting.\n\n2105508/sd(nhl_train.df$SALARY)\n\n[1] 0.7052214\n\n1975654/sd(nhl_train.df$SALARY)\n\n[1] 0.6617279\n\n\nCompared to SLR, we got smaller coefficients by comparing RMSE to standard deviation of training set. So, using multiple inputs to predict salary is more efficient than using only points. Our model explains 50% of the variance in salary, which suggests there are other factors that can impact salary of the player. As I mentioned earlier, the reputation of the player, and the budget of the team can play major role. These variables not included in our model."
  },
  {
    "objectID": "projects/groceries/arules.html",
    "href": "projects/groceries/arules.html",
    "title": "Association rules",
    "section": "",
    "text": "For this task, we will be using data from Groceries, a dataset that can be found with the arules package. Each row in the file represents one buyer’s purchases. We will generate item frequency plots, identify strong association rules involving a specific product, and visualize rules using scatter and graph-based methods.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(arules)\nlibrary(arulesViz)\n\ndata(\"Groceries\")\n\nGroceries is of class transactions (sparse matrix). The data consists of 9835 rows, and 169 columns.\n\nsummary(Groceries)\n\ntransactions as itemMatrix in sparse format with\n 9835 rows (elements/itemsets/transactions) and\n 169 columns (items) and a density of 0.02609146 \n\nmost frequent items:\n      whole milk other vegetables       rolls/buns             soda \n            2513             1903             1809             1715 \n          yogurt          (Other) \n            1372            34055 \n\nelement (itemset/transaction) length distribution:\nsizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77   55   46 \n  17   18   19   20   21   22   23   24   26   27   28   29   32 \n  29   14   14    9   11    4    6    1    1    1    1    3    1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.409   6.000  32.000 \n\nincludes extended item information - examples:\n       labels  level2           level1\n1 frankfurter sausage meat and sausage\n2     sausage sausage meat and sausage\n3  liver loaf sausage meat and sausage\n\n\nThe bar plot below displays frequent items, that meet the support value. The minimum support threshold is set at 7.25%, meaning that only items appearing in at least 7.25% of all transactions are considered frequent. As a result we got 16 frequent products.\n\nitemFrequencyPlot(Groceries, \n                  support=0.0725, \n                  horiz = TRUE, \n                  col = \"ivory\",\n                  main = \"Frequent Grocery items (Support &gt; 7.25%)\")\n\n\n\n\n\n\n\n\nLet’s create subset of rules that contain my grocery item - cream cheese.\n\nrules &lt;- apriori (Groceries, parameter = list(supp = 0.001, conf = 0.5))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.5    0.1    1 none FALSE            TRUE       5   0.001      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 9 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\nsorting and recoding items ... [157 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 5 6 done [0.01s].\nwriting ... [5668 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n# summary(rules)\n# inspect(rules[1:5])\n# itemLabels(Groceries)\nlhs_rules &lt;- subset(rules, lhs %in% \"cream cheese \")\nrhs_rules &lt;- subset(rules, rhs %in% \"cream cheese \")\n\nsummary(lhs_rules)\n\nset of 233 rules\n\nrule length distribution (lhs + rhs):sizes\n  3   4   5 \n 59 137  37 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.000   3.000   4.000   3.906   4.000   5.000 \n\nsummary of quality measures:\n    support           confidence        coverage             lift       \n Min.   :0.001017   Min.   :0.5000   Min.   :0.001118   Min.   : 1.957  \n 1st Qu.:0.001118   1st Qu.:0.5556   1st Qu.:0.001729   1st Qu.: 2.642  \n Median :0.001220   Median :0.6154   Median :0.002034   Median : 3.138  \n Mean   :0.001536   Mean   :0.6480   Mean   :0.002494   Mean   : 3.563  \n 3rd Qu.:0.001729   3rd Qu.:0.7143   3rd Qu.:0.002847   3rd Qu.: 3.982  \n Max.   :0.006609   Max.   :1.0000   Max.   :0.012405   Max.   :11.041  \n     count      \n Min.   :10.00  \n 1st Qu.:11.00  \n Median :12.00  \n Mean   :15.11  \n 3rd Qu.:17.00  \n Max.   :65.00  \n\nmining info:\n      data ntransactions support confidence\n Groceries          9835   0.001        0.5\n                                                                  call\n apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.5))\n\nsummary(rhs_rules)\n\nset of 1 rules\n\nrule length distribution (lhs + rhs):sizes\n5 \n1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      5       5       5       5       5       5 \n\nsummary of quality measures:\n    support           confidence        coverage             lift      \n Min.   :0.001017   Min.   :0.5882   Min.   :0.001729   Min.   :14.83  \n 1st Qu.:0.001017   1st Qu.:0.5882   1st Qu.:0.001729   1st Qu.:14.83  \n Median :0.001017   Median :0.5882   Median :0.001729   Median :14.83  \n Mean   :0.001017   Mean   :0.5882   Mean   :0.001729   Mean   :14.83  \n 3rd Qu.:0.001017   3rd Qu.:0.5882   3rd Qu.:0.001729   3rd Qu.:14.83  \n Max.   :0.001017   Max.   :0.5882   Max.   :0.001729   Max.   :14.83  \n     count   \n Min.   :10  \n 1st Qu.:10  \n Median :10  \n Mean   :10  \n 3rd Qu.:10  \n Max.   :10  \n\nmining info:\n      data ntransactions support confidence\n Groceries          9835   0.001        0.5\n                                                                  call\n apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.5))\n\n\nThere is 233 rules that contain my product - on the left hand side. 59 rules involve three product subset, 137 four product subset, and 37 five product subset. And we get only 1 rule with cream cheese on right hand side, which is in the subset of five products. Indicating that cream cheese appears in combination with other products.\nLet’s look at the first rule: If a person buys other vegetables, curd, yogurt, and whipped/sour cream this person 14.83 times more likely to buy cream cheese than a random customer in store. The support number is 0.001, meaning that 0.1% of all transactions studied had exact same item sets. Confidence is 0.59 =&gt; If someone buys other vegetables, curd, yogurt, and whipped/sour cream, there’s a 59% chance that they also buy cream cheese. Coverage number gives an idea how often the rule can be applied, in this case it equals to 0.002. This rule applies to 0.2% of all transactions in the dataset.\n\ninspect(sort(rhs_rules, by=\"lift\"))\n\n    lhs                     rhs                 support confidence    coverage     lift count\n[1] {other vegetables,                                                                       \n     curd,                                                                                   \n     yogurt,                                                                                 \n     whipped/sour cream} =&gt; {cream cheese } 0.001016777  0.5882353 0.001728521 14.83409    10\n\n\nThe next rule: If a person buys citrus fruit, other vegetables, whole milk, and cream cheese he/she is 9.12 times more likely to buy domestic eggs than a random purchaser in store. The support number is 0.001, this rule applies to only 0.1% of all transactions. This rule also have high confidence, saying that if customer buys citrus fruit, other vegetables, whole milk, and cream cheese, there is 58% chance they also buy domestic eggs. Coverage number is 0.002, meaning that this combination occurs in 0.2% of all transactions.\n\ninspect(sort(lhs_rules, by=\"lift\")[2])\n\n    lhs                    rhs                 support confidence    coverage     lift count\n[1] {citrus fruit,                                                                          \n     other vegetables,                                                                      \n     whole milk,                                                                            \n     cream cheese }     =&gt; {domestic eggs} 0.001118454  0.5789474 0.001931876 9.124916    11\n\n\nFrom these rules we see that certain sets of products are frequently purchased together. In combination they may be ingredients for salads, or other recipes. Cream cheese, in particular, is commonly used in baking and is a key ingredient in cheesecake. Despite that, cream cheese widely used for frosting, spreads, pasta sauces, dips, making it a versatile ingredient in a variety of dishes.\nBy identifying frequent combinations with cream cheese, the store can strategically place those items nearby—such as positioning cream cheese close to the vegetables/fruits section or within the dairy aisle for convenient access. Also, offering special discounts can boost sales. For example, if a customer buys cream cheese, offering a discount on berries or bagels can encourage bundled purchases. Additionally, analyzing product pairs allows the store to anticipate demand and adjust inventory accordingly, ensuring high-demand combinations are well-stocked ahead of time, especially during peak shopping seasons.\n\ninspect(lhs_rules[7:9])\n\n    lhs                              rhs                support     confidence\n[1] {cream cheese , frozen meals} =&gt; {whole milk}       0.001016777 0.7142857 \n[2] {hard cheese, cream cheese }  =&gt; {other vegetables} 0.001118454 0.5789474 \n[3] {hard cheese, cream cheese }  =&gt; {whole milk}       0.001016777 0.5263158 \n    coverage    lift     count\n[1] 0.001423488 2.795464 10   \n[2] 0.001931876 2.992090 11   \n[3] 0.001931876 2.059815 10   \n\nplot(lhs_rules[7:9])\n\n\n\n\n\n\n\n\nFrom the plot above, we observe the distribution of three association rules based on confidence (y-axis), support (x-axis), and lift (represented by color). First rule: {cream cheese , frozen meals} =&gt; {whole milk} has the highest confidence and lift values but a low support. This means the rule highly reliable: there is 71.43% chance that a customer who buys cream cheese and frozen meals will also buy whole milk. And the strength of the association is strong. However, it applies to 0.10% of the transactions. Second rule: {hard cheese, cream cheese } =&gt; {other vegetables} has a higher support and lift compared to the first rule, but its confidence is lower (57.9%). This suggests that while this combination of products occurs more frequently, it may not be as strong in predicting the purchase of vegetables when a customer buys both hard cheese and cream cheese. Third rule: {hard cheese, cream cheese } =&gt; {whole milk} has a similar support as a first rule, meaning that it applies to the same small proportion of transactions. However, it has lowest confidence and lift values, making it less reliable and significant to customer behaviour. This rule can demonstrate rare combination of items.\n\nplot(lhs_rules[7:9], method = \"graph\", engine=\"htmlwidget\")\n\n\n\n\n\nNow the plot shows the relationship between rules as a graph. The central node represents cream cheese, which appears in all three rules, indicating that it is a key item in these associations. Hard cheese and whole milk appear in two rules each, showing that these items are associated with more than one combination of products. Frozen meals and other vegetables only appear in one rule each, which indicates that they are more specific to particular product combinations. Also the color differentiation in the plot corresponds to the lift value of each rule. The rule 3, which has lowest lift value in represented in light red. Meanwhile, Rules 1 and 2 are highlighted in bold red, suggesting that they have higher lift values and stronger associations between the items. Compared to previous plot, this visual shows elements of rules, allowing to quickly identify the central elements, and the relative strength of the rules. This plot also displays measures of rules (confidence, support) if we click on rule node. However, if we want to select strong rule the scatter plot is more useful because it clearly shows rules with higher support and confidence in a more clearer way. Therefore, the choice of plot depends on the purpose."
  },
  {
    "objectID": "projects/consumer_complaints/naive_bayes.html",
    "href": "projects/consumer_complaints/naive_bayes.html",
    "title": "Complaint Classifier",
    "section": "",
    "text": "As part of my Data Mining coursework, I’m building a Naive Bayes classification model to predict whether a consumer disputes a company’s response using the Consumer Complaints dataset. The project involves cleaning and preprocessing the data, including renaming variables, removing redundancies, and binning categories. I evaluate the model’s performance using confusion matrices and compare it to the naive rule, uncovering key insights into the challenges of predicting dispute outcomes and how predictions can be improved.\nThe source of dataset is Kaggle.\n\nData Exploration\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(e1071)\n\ncc &lt;- read.csv('consumer_complaints.csv')\n\nNumeric: Resolution.time.in.days, Year\nCategorical: X.ID\n\nglimpse(cc)\n\nRows: 14,000\nColumns: 16\n$ X.ID                     &lt;int&gt; 1615767, 654223, 1143398, 1303679, 1627370, 1…\n$ Company                  &lt;chr&gt; \"PHH Mortgage\", \"Ocwen\", \"Southwest Credit Sy…\n$ Product                  &lt;chr&gt; \"Mortgage\", \"Mortgage\", \"Debt collection\", \"C…\n$ Issue                    &lt;chr&gt; \"Loan servicing, payments, escrow account\", \"…\n$ State                    &lt;chr&gt; \"FL\", \"NC\", \"MO\", \"WA\", \"VA\", \"IL\", \"FL\", \"OK…\n$ Submitted.via            &lt;chr&gt; \"Web\", \"Web\", \"Web\", \"Web\", \"Web\", \"Web\", \"We…\n$ Date.received            &lt;chr&gt; \"10/20/2015\", \"3/1/2014\", \"4/12/2014\", \"03/26…\n$ Date.resolved            &lt;chr&gt; \"10/20/2015\", \"3/1/2014\", \"4/12/2014\", \"03/26…\n$ Timely.response.         &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n$ Consumer.disputed.       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ state.name               &lt;chr&gt; \"Florida\", \"North Carolina\", \"Missouri\", \"Was…\n$ Date.received.1          &lt;chr&gt; \"10/20/2015\", \"1/3/2014\", \"12/4/2014\", \"3/26/…\n$ Date.resolved.1          &lt;chr&gt; \"10/20/2015\", \"1/3/2014\", \"12/4/2014\", \"3/26/…\n$ Resolution.time.in.days. &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 29, 0,…\n$ Year                     &lt;int&gt; 2015, 2014, 2014, 2015, 2015, 2015, 2016, 201…\n$ QTR..US.FLY.             &lt;chr&gt; \"Q4\", \"Q1\", \"Q4\", \"Q1\", \"Q4\", \"Q2\", \"Q2\", \"Q2…\n\n\nConsumer.disputed. variable is of type chr, then I converted it to factor. Now it has 2 levels: Yes or No.\nWe see that the data is imbalanced, out of 14,000 complaints only 3138 (22.4%) disputed.\n\nstr(cc$Consumer.disputed.)\n\n chr [1:14000] \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" ...\n\ncc$Consumer.disputed. &lt;- factor(cc$Consumer.disputed.)\nlevels(cc$Consumer.disputed.)\n\n[1] \"No\"  \"Yes\"\n\ntable(cc$Consumer.disputed.)\n\n\n   No   Yes \n10862  3138 \n\ncc %&gt;% group_by(Consumer.disputed.) %&gt;% \n  summarise(Count = n()/14000)\n\n# A tibble: 2 × 2\n  Consumer.disputed. Count\n  &lt;fct&gt;              &lt;dbl&gt;\n1 No                 0.776\n2 Yes                0.224\n\n\nI removed dots from variables that had them at the end, and renamed QTR..US.FLY. to Quarter.\n\nnames(cc)\n\n [1] \"X.ID\"                     \"Company\"                 \n [3] \"Product\"                  \"Issue\"                   \n [5] \"State\"                    \"Submitted.via\"           \n [7] \"Date.received\"            \"Date.resolved\"           \n [9] \"Timely.response.\"         \"Consumer.disputed.\"      \n[11] \"state.name\"               \"Date.received.1\"         \n[13] \"Date.resolved.1\"          \"Resolution.time.in.days.\"\n[15] \"Year\"                     \"QTR..US.FLY.\"            \n\ncc &lt;- cc %&gt;% rename(Resolution.time.in.days = Resolution.time.in.days., \n              Timely.response=Timely.response.,\n              Consumer.disputed=Consumer.disputed.,\n              Quarter = QTR..US.FLY.)\n\nAfter examining number of unique values in each column, we observe that a few columns contain more than 100 distinct values. Specifically, there are 14,000 unique ID records, 1,050 companies, and over 1,300 date values.\n\nsapply(cc, function(x) length(unique(x)))\n\n                   X.ID                 Company                 Product \n                  14000                    1050                      12 \n                  Issue                   State           Submitted.via \n                     81                      60                       5 \n          Date.received           Date.resolved         Timely.response \n                   1370                    1322                       2 \n      Consumer.disputed              state.name         Date.received.1 \n                      2                      52                    1370 \n        Date.resolved.1 Resolution.time.in.days                    Year \n                   1322                      77                       4 \n                Quarter \n                      4 \n\n\n\ncc &lt;- cc %&gt;% select(-X.ID, -Company, -Date.received, -Date.resolved, -Date.received.1, -Date.resolved.1)\n\nstr(cc)\n\n'data.frame':   14000 obs. of  10 variables:\n $ Product                : chr  \"Mortgage\" \"Mortgage\" \"Debt collection\" \"Credit card\" ...\n $ Issue                  : chr  \"Loan servicing, payments, escrow account\" \"Loan servicing, payments, escrow account\" \"Loan modification,collection,foreclosure\" \"Billing statement\" ...\n $ State                  : chr  \"FL\" \"NC\" \"MO\" \"WA\" ...\n $ Submitted.via          : chr  \"Web\" \"Web\" \"Web\" \"Web\" ...\n $ Timely.response        : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Consumer.disputed      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ state.name             : chr  \"Florida\" \"North Carolina\" \"Missouri\" \"Washington\" ...\n $ Resolution.time.in.days: int  0 0 0 0 0 0 1 0 0 0 ...\n $ Year                   : int  2015 2014 2014 2015 2015 2015 2016 2015 2013 2016 ...\n $ Quarter                : chr  \"Q4\" \"Q1\" \"Q4\" \"Q1\" ...\n\n\nBy examining the results of the summary() function, we notice an impossible negative number for the resolution date. Additionally, there are two overlapping columns: State and State_name.\n\nsummary(cc)\n\n   Product             Issue              State           Submitted.via     \n Length:14000       Length:14000       Length:14000       Length:14000      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Timely.response    Consumer.disputed  state.name       \n Length:14000       No :10862         Length:14000      \n Class :character   Yes: 3138         Class :character  \n Mode  :character                     Mode  :character  \n                                                        \n                                                        \n                                                        \n Resolution.time.in.days      Year        Quarter         \n Min.   : -1.000         Min.   :2013   Length:14000      \n 1st Qu.:  0.000         1st Qu.:2014   Class :character  \n Median :  0.000         Median :2015   Mode  :character  \n Mean   :  2.006         Mean   :2015                     \n 3rd Qu.:  2.000         3rd Qu.:2016                     \n Max.   :286.000         Max.   :2016                     \n\n\nBefore that, when counting unique values by column, we observed that the State column had 60 unique values, while the State_name column had 52. Upon examining the values, it appears that the State column includes more detailed information, possibly encompassing territories and military postal codes. We can also keep Year, Resolution.time.in.days, and Quarter, which are related to the removed columns date_received, and date_resolved.\n\ntable(cc$State)\n\n\n       AA   AE   AK   AL   AP   AR   AS   AZ   CA   CO   CT   DC   DE   FL   GA \n 110    1    5   19  135    4   62    1  313 1977  251  161   98   80 1255  577 \n  GU   HI   IA   ID   IL   IN   KS   KY   LA   MA   MD   ME   MI   MN   MO   MP \n   1   39   56   51  500  137   78   99  149  292  408   47  373  174  171    1 \n  MS   MT   NC   ND   NE   NH   NJ   NM   NV   NY   OH   OK   OR   PA   PR   RI \n  77   19  443    7   54   65  518   75  171  996  463   93  177  511   26   64 \n  SC   SD   TN   TX   UT   VA   VI   VT   WA   WI   WV   WY \n 194   26  238 1040   77  480    8   29  303  178   32   11 \n\ncc &lt;- cc %&gt;% select(-state.name) %&gt;% \n  filter(Resolution.time.in.days&gt;=0)\n\nstr(cc)\n\n'data.frame':   13997 obs. of  9 variables:\n $ Product                : chr  \"Mortgage\" \"Mortgage\" \"Debt collection\" \"Credit card\" ...\n $ Issue                  : chr  \"Loan servicing, payments, escrow account\" \"Loan servicing, payments, escrow account\" \"Loan modification,collection,foreclosure\" \"Billing statement\" ...\n $ State                  : chr  \"FL\" \"NC\" \"MO\" \"WA\" ...\n $ Submitted.via          : chr  \"Web\" \"Web\" \"Web\" \"Web\" ...\n $ Timely.response        : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Consumer.disputed      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Resolution.time.in.days: int  0 0 0 0 0 0 1 0 0 0 ...\n $ Year                   : int  2015 2014 2014 2015 2015 2015 2016 2015 2013 2016 ...\n $ Quarter                : chr  \"Q4\" \"Q1\" \"Q4\" \"Q1\" ...\n\n\nThe Year column contains four unique values: 2013, 2014, 2015, and 2016. For binning, we can group them into two categories: ‘Earlier period’ for 2013 and 2014, and ‘Later period’ for 2015 and 2016.\n\ncc &lt;- cc %&gt;% \n  mutate(Year = cut(Year,\n                    breaks = c(2012, 2014, 2016),\n                    labels = c(\"Earlier period\", \"Later period\"),\n                    right = TRUE))\n\nstr(cc)\n\n'data.frame':   13997 obs. of  9 variables:\n $ Product                : chr  \"Mortgage\" \"Mortgage\" \"Debt collection\" \"Credit card\" ...\n $ Issue                  : chr  \"Loan servicing, payments, escrow account\" \"Loan servicing, payments, escrow account\" \"Loan modification,collection,foreclosure\" \"Billing statement\" ...\n $ State                  : chr  \"FL\" \"NC\" \"MO\" \"WA\" ...\n $ Submitted.via          : chr  \"Web\" \"Web\" \"Web\" \"Web\" ...\n $ Timely.response        : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Consumer.disputed      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Resolution.time.in.days: int  0 0 0 0 0 0 1 0 0 0 ...\n $ Year                   : Factor w/ 2 levels \"Earlier period\",..: 2 1 1 2 2 2 2 2 1 2 ...\n $ Quarter                : chr  \"Q4\" \"Q1\" \"Q4\" \"Q1\" ...\n\n\nThe Resolution_time variable contains a significant number of zeros (8,316) along with other values. This imbalance caused issues when using the equal frequency method for binning. To address this, I first filtered out the non-zero values to determine the breaks within the data. Then, I added zero to this group.\nAs a result, we have three groups: low, medium, and high.\n\ntable(cc$Resolution.time.in.days)\n\n\n   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 \n8316 1616 1122  844  491  473  318  222   83   35   47   37   21   18   18   18 \n  16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31 \n   9   10   16   15   12   22    9    6   13    6    7    2   10   16    5    9 \n  32   33   34   35   36   37   38   39   40   41   42   43   45   46   47   48 \n  14    9    9   11   12    3    3    7    3    8    3    8    5    2    5    1 \n  49   50   51   52   55   56   57   60   61   63   64   65   66   67   68   70 \n   5    3    3    1    6    2    1    2    4    2    1    2    1    1    1    1 \n  72   76   77   79   83   90   93   95   98  106  151  286 \n   1    1    1    1    1    1    1    1    1    1    1    1 \n\nnon_zero_values &lt;- cc$Resolution.time.in.days[cc$Resolution.time.in.days &gt; 0]\nbreaks &lt;- quantile(non_zero_values, probs = seq(0, 1, length.out = 3))\n\nbreaks &lt;- c(0, breaks)\n\ncc$Resolution.time.in.days &lt;- cut(cc$Resolution.time.in.days, breaks = breaks, labels = c(\"low\", \"medium\", \"high\"), include.lowest = TRUE)\n\n\ntable(cc$Resolution.time.in.days)\n\n\n   low medium   high \n  9932   1966   2099 \n\n\nFor the next few steps, we are going to be reducing the number of unique levels for some of our factor variables:\nThe Product column has 12 unique values, but we will use only the 6 most common ones.\n\nlength(unique(cc$Product))\n\n[1] 12\n\ntop_6_Product &lt;- cc %&gt;% \n  count(Product, sort = TRUE) %&gt;% \n  slice(1:6) %&gt;% \n  select(Product)\n\nThe Issue column contains 81 unique values, and we will use only the 7 most common ones. Also changed these 7 values to the shorter names.\n\nlength(unique(cc$Issue))\n\n[1] 81\n\ntop_7_Issue &lt;- cc %&gt;% \n  count(Issue, sort = TRUE) %&gt;% \n  slice(1:7) %&gt;% \n  select(Issue)\n\nThe State column has 60 unique values, and we will use the 10 most common ones.\n\nlength(unique(cc$State))\n\n[1] 60\n\ntop_10_State &lt;- cc %&gt;% \n  count(State, sort = TRUE) %&gt;% \n  slice(1:10) %&gt;% \n  select(State)\n\n\ncc &lt;- cc %&gt;% filter(Product %in% top_6_Product$Product & \n                Issue %in% top_7_Issue$Issue &\n                State %in% top_10_State$State)\n\ncc &lt;- cc %&gt;% \n  mutate(Issue = case_when(\n    Issue == \"Account opening, closing, or management\" ~ \"Account Management\",\n    Issue == \"Application, originator, mortgage broker\" ~ \"Mortgage Application\",\n    Issue == \"Communication tactics\" ~ \"Communication\",\n    Issue == \"Credit reporting company's investigation\" ~ \"Credit Investigation\",\n    Issue == \"Deposits and withdrawals\" ~ \"Transactions\",\n    Issue == \"Loan modification,collection,foreclosure\" ~ \"Loan Modification\",\n    Issue == \"Loan servicing, payments, escrow account\" ~ \"Loan Servicing\",\n    TRUE ~ Issue\n  ))\n\nnrow(cc)\n\n[1] 4110\n\n\nAfter reducing the unique levels, the dataset now contains 4110 rows.\n\nstr(cc)\n\n'data.frame':   4110 obs. of  9 variables:\n $ Product                : chr  \"Mortgage\" \"Mortgage\" \"Credit reporting\" \"Debt collection\" ...\n $ Issue                  : chr  \"Loan Servicing\" \"Mortgage Application\" \"Credit Investigation\" \"Loan Modification\" ...\n $ State                  : chr  \"FL\" \"FL\" \"CA\" \"NY\" ...\n $ Submitted.via          : chr  \"Web\" \"Web\" \"Web\" \"Email\" ...\n $ Timely.response        : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Consumer.disputed      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 2 ...\n $ Resolution.time.in.days: Factor w/ 3 levels \"low\",\"medium\",..: 1 1 1 1 1 3 3 1 3 1 ...\n $ Year                   : Factor w/ 2 levels \"Earlier period\",..: 2 2 2 1 2 1 2 2 2 1 ...\n $ Quarter                : chr  \"Q4\" \"Q2\" \"Q1\" \"Q4\" ...\n\ncc$Product &lt;- as.factor(cc$Product)\ncc$Issue &lt;- as.factor(cc$Issue)\ncc$State &lt;- as.factor(cc$State)\ncc$Submitted.via &lt;- as.factor(cc$Submitted.via)\ncc$Timely.response &lt;- as.factor(cc$Timely.response)\ncc$Year &lt;- as.factor(cc$Year)\ncc$Quarter &lt;- as.factor(cc$Quarter)\n\n\n\nData Partitioning\nAfter converting all variables to factor type, I partitioned the data into training(60%) and validation(40%) sets.\n\nset.seed(79)\ncc.index &lt;- sample(c(1:nrow(cc)), nrow(cc)*0.6)\ncc_train.df &lt;- cc[cc.index, ]\ncc_valid.df &lt;- cc[-cc.index, ]\n\n\n\nData Visualization - Proportional Barplot\n\nggplot(cc_train.df, aes(x = Product, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Product\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Issue, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  # coord_flip() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(x = \"Issue\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = State, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"State\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Submitted.via, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Submitted.via\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Timely.response, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Timely.response\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Year, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Year\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Quarter, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Quarter\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Resolution.time.in.days, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Resolution.time.in.days\", y = \"Proportion\")\n\n\n\n\n\n\n\n\nFor the Year variable, we observed a similar proportion between time periods and whether consumers disputed or not. The same applies to the Quarter variable, with only a small difference in the third quarter. Timely.response showed better result with noticeable differences between values compared to Resolution.time.in.days. For the remaining variables, we can see distinct differences between categories.\nI will remove Resolution.time.in.days, because of the imbalance of values in dataset.\n\ncc_train.df &lt;- cc_train.df %&gt;% select(-Year, -Quarter, -Resolution.time.in.days)\nstr(cc_train.df)\n\n'data.frame':   2466 obs. of  6 variables:\n $ Product          : Factor w/ 4 levels \"Bank account or service\",..: 3 1 4 3 4 3 1 3 3 4 ...\n $ Issue            : Factor w/ 7 levels \"Account Management\",..: 2 1 5 4 5 4 1 2 2 5 ...\n $ State            : Factor w/ 10 levels \"CA\",\"FL\",\"GA\",..: 2 5 2 8 8 5 4 3 9 1 ...\n $ Submitted.via    : Factor w/ 5 levels \"Email\",\"Fax\",..: 5 5 5 4 5 4 1 5 5 5 ...\n $ Timely.response  : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Consumer.disputed: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n\ncc.nb &lt;- naiveBayes(Consumer.disputed ~.,data = cc_train.df)\ncc.nb\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n       No       Yes \n0.7704785 0.2295215 \n\nConditional probabilities:\n     Product\nY     Bank account or service Credit reporting Debt collection   Mortgage\n  No               0.27210526       0.08368421      0.22473684 0.41947368\n  Yes              0.22261484       0.10424028      0.24558304 0.42756184\n\n     Issue\nY     Account Management Communication Credit Investigation Loan Modification\n  No          0.15947368    0.11105263           0.08368421        0.11368421\n  Yes         0.15724382    0.08303887           0.10424028        0.16254417\n     Issue\nY     Loan Servicing Mortgage Application Transactions\n  No      0.34421053           0.07526316   0.11263158\n  Yes     0.32862191           0.09893993   0.06537102\n\n     State\nY             CA         FL         GA         IL         NJ         NY\n  No  0.22578947 0.15631579 0.07210526 0.06526316 0.06000000 0.11789474\n  Yes 0.26501767 0.15901060 0.07950530 0.04416961 0.06890459 0.09717314\n     State\nY             OH         PA         TX         VA\n  No  0.06000000 0.06000000 0.12736842 0.05526316\n  Yes 0.03533569 0.05123675 0.13074205 0.06890459\n\n     Submitted.via\nY           Email         Fax       Phone Postal mail         Web\n  No  0.176315789 0.011052632 0.089473684 0.042631579 0.680526316\n  Yes 0.128975265 0.003533569 0.051236749 0.037102473 0.779151943\n\n     Timely.response\nY             No        Yes\n  No  0.01684211 0.98315789\n  Yes 0.01060071 0.98939929\n\n\n\nstr(cc_train.df)\n\n'data.frame':   2466 obs. of  6 variables:\n $ Product          : Factor w/ 4 levels \"Bank account or service\",..: 3 1 4 3 4 3 1 3 3 4 ...\n $ Issue            : Factor w/ 7 levels \"Account Management\",..: 2 1 5 4 5 4 1 2 2 5 ...\n $ State            : Factor w/ 10 levels \"CA\",\"FL\",\"GA\",..: 2 5 2 8 8 5 4 3 9 1 ...\n $ Submitted.via    : Factor w/ 5 levels \"Email\",\"Fax\",..: 5 5 5 4 5 4 1 5 5 5 ...\n $ Timely.response  : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Consumer.disputed: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n\n\n\n\nConfusion Matrix\nComparing the accuracy metrics, the validation set achieved 74.45%, which is lower than the training set’s 77.05%. However, both sets reveal that the model predominantly predicts ‘No’ for all instances (100%). a.The model predicts ‘No’ for every case and never predicts ‘Yes’, indicating an imbalance among predictions. Although the accuracy appears relatively high, it is misleading as it reflects the model’s bias rather than its true predictive power.\n\nconfusionMatrix(predict(cc.nb, newdata=cc_train.df), cc_train.df$Consumer.disputed)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  1900  566\n       Yes    0    0\n                                          \n               Accuracy : 0.7705          \n                 95% CI : (0.7534, 0.7869)\n    No Information Rate : 0.7705          \n    P-Value [Acc &gt; NIR] : 0.5113          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.7705          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.7705          \n         Detection Rate : 0.7705          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : No              \n                                          \n\nconfusionMatrix(predict(cc.nb, newdata=cc_valid.df), cc_valid.df$Consumer.disputed)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  1224  420\n       Yes    0    0\n                                          \n               Accuracy : 0.7445          \n                 95% CI : (0.7227, 0.7655)\n    No Information Rate : 0.7445          \n    P-Value [Acc &gt; NIR] : 0.5131          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.7445          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.7445          \n         Detection Rate : 0.7445          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : No              \n                                          \n\n\n\n\nNaive Rule vs Naive Bayes\nNaive rule in classification is to classify the record as a member of majority class. If we had used the naive rule for classification, we would classify all records in the training set as “No” because “No” is the most frequent class.\nOur Naive Bayes model, which follows the naive rule approach, assigns all cases as “No.” Both methods yield the same accuracy of 77.05%, resulting in a 0% difference between them.\nI think imbalance plays a big role here. As described in the book, the absence of this predictor actively “outvotes” any other information in the record to assign a “No” to the outcome value (when, in this case, it has a relatively good chance of being a “Yes”). Also, as a customer, I often choose not to dispute issues to avoid wasting energy. It’s usually easier to let things go rather than engage in disputes, especially if the potential outcome doesn’t seem worth the effort. Maybe it’s a reason why we don’t have meaningful data.\n\n\nScoring data using Naive Bayes\nI took 25 records by sorting the probability for the “Yes” column in descending order, selecting the top 25 as the most likely to belong to the “YES” group.\nAmong these 25 records, 5 records truly belong to “Yes” group. The accuracy for these predictions = 80%. Even though the model didn’t predict any “Yes” values, it still achieved 80% accuracy by correctly classifying 20 out of 20 “No” records. Since the “Yes” group is a small portion of the dataset, its impact on accuracy is minimal. Compared to accuracy of overall model 74.45%, these selected proportion of data have relatively high value.\n\npred.prob &lt;- predict(cc.nb, newdata=cc_valid.df, type=\"raw\")\n# pred.prob\n\npred.class &lt;- predict(cc.nb, newdata=cc_valid.df)\n# pred.class\n\ndf &lt;- data.frame(actual=cc_valid.df$Consumer.disputed,\n                 predicted=pred.class, pred.prob)\n\nvalid_25 &lt;- df %&gt;% arrange(desc(Yes)) %&gt;% slice(1:25)\n\ntable(valid_25$actual)\n\n\n No Yes \n 20   5 \n\nvalid_25 %&gt;% filter(actual == 'Yes')\n\n  actual predicted        No       Yes\n1    Yes        No 0.5992883 0.4007117\n2    Yes        No 0.6009522 0.3990478\n3    Yes        No 0.6009522 0.3990478\n4    Yes        No 0.6137075 0.3862925\n5    Yes        No 0.6137075 0.3862925\n\n\nIdentifying this subset of records helps us to see that the model completely fails at identifying “Yes” cases. On the other hand, by assigning all records to the majority class “No,” the model achieves high accuracy, performing well in most cases. By identifying this main issue, we can focus on other ways to dealing with imbalanced data or try other models.\n\n\nManual calculation of probability\n\nmy_data &lt;- cc_train.df[45,]\nmy_data\n\n      Product                Issue State Submitted.via Timely.response\n3560 Mortgage Mortgage Application    IL           Web              No\n     Consumer.disputed\n3560                No\n\npredict(cc.nb, my_data)\n\n[1] No\nLevels: No Yes\n\npredict(cc.nb, my_data, type=\"raw\")\n\n            No       Yes\n[1,] 0.8370455 0.1629545\n\ncc.nb\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n       No       Yes \n0.7704785 0.2295215 \n\nConditional probabilities:\n     Product\nY     Bank account or service Credit reporting Debt collection   Mortgage\n  No               0.27210526       0.08368421      0.22473684 0.41947368\n  Yes              0.22261484       0.10424028      0.24558304 0.42756184\n\n     Issue\nY     Account Management Communication Credit Investigation Loan Modification\n  No          0.15947368    0.11105263           0.08368421        0.11368421\n  Yes         0.15724382    0.08303887           0.10424028        0.16254417\n     Issue\nY     Loan Servicing Mortgage Application Transactions\n  No      0.34421053           0.07526316   0.11263158\n  Yes     0.32862191           0.09893993   0.06537102\n\n     State\nY             CA         FL         GA         IL         NJ         NY\n  No  0.22578947 0.15631579 0.07210526 0.06526316 0.06000000 0.11789474\n  Yes 0.26501767 0.15901060 0.07950530 0.04416961 0.06890459 0.09717314\n     State\nY             OH         PA         TX         VA\n  No  0.06000000 0.06000000 0.12736842 0.05526316\n  Yes 0.03533569 0.05123675 0.13074205 0.06890459\n\n     Submitted.via\nY           Email         Fax       Phone Postal mail         Web\n  No  0.176315789 0.011052632 0.089473684 0.042631579 0.680526316\n  Yes 0.128975265 0.003533569 0.051236749 0.037102473 0.779151943\n\n     Timely.response\nY             No        Yes\n  No  0.01684211 0.98315789\n  Yes 0.01060071 0.98939929\n\nno_score &lt;- 0.7704785 * 0.41947368 * 0.07526316 * 0.06526316 * 0.680526316 * 0.01684211\nyes_score &lt;- 0.2295215 * 0.42756184 * 0.09893993 * 0.04416961 * 0.779151943 * 0.01060071\n\nno_score/(no_score + yes_score)\n\n[1] 0.8370455\n\n\nI selected 45th row from training set.\n\nActual Consumer.disputed outcome is “No”\nThe model’s predicted answer is “No”\nThe probability for “No” is 0.8370455, “Yes” is 0.1629545."
  },
  {
    "objectID": "projects/vortex/main.html",
    "href": "projects/vortex/main.html",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "",
    "text": "Code\nimport yfinance as yf  # For downloading financial data\nimport numpy as np      # For numerical operations\nimport pandas as pd     # For data manipulation\nimport requests # For downloading the API data\nimport numpy as np \nimport plotly.graph_objects as go\nimport plotly.express as px # Import the Plotly Express module for interactive visualization\nimport json\nimport vectorbt as vbt\nfrom plotly.subplots import make_subplots\nimport streamlit as st\n\nimport plotly.io as pio\npio.renderers.default = 'iframe_connected'\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn("
  },
  {
    "objectID": "projects/vortex/main.html#importing-necessary-libraries-for-analysis",
    "href": "projects/vortex/main.html#importing-necessary-libraries-for-analysis",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "",
    "text": "Code\nimport yfinance as yf  # For downloading financial data\nimport numpy as np      # For numerical operations\nimport pandas as pd     # For data manipulation\nimport requests # For downloading the API data\nimport numpy as np \nimport plotly.graph_objects as go\nimport plotly.express as px # Import the Plotly Express module for interactive visualization\nimport json\nimport vectorbt as vbt\nfrom plotly.subplots import make_subplots\nimport streamlit as st\n\nimport plotly.io as pio\npio.renderers.default = 'iframe_connected'\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn("
  },
  {
    "objectID": "projects/vortex/main.html#data-collection",
    "href": "projects/vortex/main.html#data-collection",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "Data Collection",
    "text": "Data Collection\n\nFetch daily OHLCV data\n\n\nCode\n# Data for the TSLA, XLY, and SPY tickers is retrieved from the Yahoo Finance library, covering the period from January 1, 2019, \n# to March 5, 2025.\ntsla = yf.download('TSLA', start='2019-01-01', end='2025-03-05') \nxly = yf.download('XLY', start='2019-01-01', end='2025-03-05')\nspy = yf.download('SPY', start='2019-01-01', end='2025-03-05')\n\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\ndef multiindex_to_singleindex(df):\n    df.columns = ['_'.join(col).strip() for col in df.columns.values]\n    return df\n\n\n\n\nCode\ntsla = multiindex_to_singleindex(tsla)\nspy = multiindex_to_singleindex(spy)\nxly = multiindex_to_singleindex(xly)\n\n\n\n\nCode\n# Displays a summary of the TSLA DataFrame, including column names, data types, non-null counts, and memory usage.\ntsla.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1551 entries, 2019-01-02 to 2025-03-04\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Close_TSLA   1551 non-null   float64\n 1   High_TSLA    1551 non-null   float64\n 2   Low_TSLA     1551 non-null   float64\n 3   Open_TSLA    1551 non-null   float64\n 4   Volume_TSLA  1551 non-null   int64  \ndtypes: float64(4), int64(1)\nmemory usage: 72.7 KB\n\n\n\n\nCode\n# Displays a summary of the XLY DataFrame, including column names, data types, non-null counts, and memory usage.\nxly.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1551 entries, 2019-01-02 to 2025-03-04\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Close_XLY   1551 non-null   float64\n 1   High_XLY    1551 non-null   float64\n 2   Low_XLY     1551 non-null   float64\n 3   Open_XLY    1551 non-null   float64\n 4   Volume_XLY  1551 non-null   int64  \ndtypes: float64(4), int64(1)\nmemory usage: 72.7 KB\n\n\n\n\nCode\n# Displays a summary of the SPY DataFrame, including column names, data types, non-null counts, and memory usage.\nspy.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1551 entries, 2019-01-02 to 2025-03-04\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Close_SPY   1551 non-null   float64\n 1   High_SPY    1551 non-null   float64\n 2   Low_SPY     1551 non-null   float64\n 3   Open_SPY    1551 non-null   float64\n 4   Volume_SPY  1551 non-null   int64  \ndtypes: float64(4), int64(1)\nmemory usage: 72.7 KB\n\n\n\n\nFetch sentiment scores from the API\n\n\nCode\ndef get_news_sentiment(ticker, start_date, end_date, limit, api_key):\n    url = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&time_from={start_date}&time_to={end_date}&limit={limit}&tickers={ticker}&apikey={api_key}'\n    response = requests.get(url)\n    if response.status_code == 200:\n        sentiment_data = response.json()\n\n        with open(f'{ticker}_sentiment_raw.json', \"w\") as f:\n            json.dump(sentiment_data, f, indent=4)\n        # sentiment_df = pd.DataFrame(sentiment_data['feed'])\n        return \"Full sentiment JSON saved ✅\"\n    else:\n        print(\"API call failed:\", response.status_code)\n        return None\n\n\n\n\nCode\nget_news_sentiment('TSLA', '20250101T0130', '20250301T0130', 1000, 'PNM5EHRALIOT1CKJ')\n\n\n'Full sentiment JSON saved ✅'"
  },
  {
    "objectID": "projects/vortex/main.html#indicator-calculation",
    "href": "projects/vortex/main.html#indicator-calculation",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "Indicator Calculation",
    "text": "Indicator Calculation\n\nCompute VI+ and VI-\n\n\nCode\n# Defines a function to calculate the Vortex Indicator (VI) for a given DataFrame and ticker symbol.\n# The calculation uses a default lookback period of 14 days unless specified otherwise.\ndef calculate_vortex(df, value, n=14):\n    # Extracts the high, low, and close price series for the specified ticker.\n    high = df[(\"High_\"+value)]\n    low = df[(\"Low_\"+value)]\n    close = df[(\"Close_\"+value)]\n\n    # Calculates the Vortex Movement values:\n    # VM+ = absolute difference between today's high and yesterday's low\n    # VM− = absolute difference between today's low and yesterday's high\n    vm_plus = abs(high - low.shift(1))     # |Today's High – Yesterday’s Low|\n    vm_minus = abs(low - high.shift(1))    # |Today's Low – Yesterday’s High|\n\n    # Computes the True Range (TR) as the maximum of:\n    # - High - Low\n    # - Absolute difference between High and Previous Close\n    # - Absolute difference between Low and Previous Close\n    tr = pd.concat([\n        high - low,\n        abs(high - close.shift(1)),\n        abs(low - close.shift(1))\n    ], axis=1).max(axis=1)\n\n    # Applies a rolling window to compute the n-period sum of VM+ and VM− values\n    # and the corresponding True Range values.\n    sum_vm_plus = vm_plus.rolling(window=n).sum()\n    sum_vm_minus = vm_minus.rolling(window=n).sum()\n    sum_tr = tr.rolling(window=n).sum()\n\n    # Calculates the Vortex Indicator components:\n    # VI+ = sum of VM+ over n periods divided by sum of TR over n periods\n    # VI− = sum of VM− over n periods divided by sum of TR over n periods\n    vi_plus = sum_vm_plus / sum_tr\n    vi_minus = sum_vm_minus / sum_tr\n\n    # Returns the VI+ and VI− series as output.\n    return vi_plus, vi_minus\n\n\n\n\nCode\n# Calculates the Vortex Indicator values for TSLA and stores the results as new columns in the DataFrame.\ntsla['VI+'], tsla['VI-'] = calculate_vortex(tsla, 'TSLA')\n\n# Calculates the Vortex Indicator values for XLY and stores the results as new columns in the DataFrame.\nxly['VI+'], xly['VI-'] = calculate_vortex(xly, 'XLY')\n\n# Calculates the Vortex Indicator values for SPY and stores the results as new columns in the DataFrame.\nspy['VI+'], spy['VI-'] = calculate_vortex(spy, 'SPY')\n\n\n\n\nCode\n# Displays the first 20 rows of the TSLA DataFrame to provide an initial overview of its structure and content with the new function applied.\ntsla.head(20)\n\n\n\n\n\n\n\n\n\nClose_TSLA\nHigh_TSLA\nLow_TSLA\nOpen_TSLA\nVolume_TSLA\nVI+\nVI-\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2019-01-02\n20.674667\n21.008667\n19.920000\n20.406668\n174879000\nNaN\nNaN\n\n\n2019-01-03\n20.024000\n20.626667\n19.825333\n20.466667\n104478000\nNaN\nNaN\n\n\n2019-01-04\n21.179333\n21.200001\n20.181999\n20.400000\n110911500\nNaN\nNaN\n\n\n2019-01-07\n22.330667\n22.449333\n21.183332\n21.448000\n113268000\nNaN\nNaN\n\n\n2019-01-08\n22.356667\n22.934000\n21.801332\n22.797333\n105127500\nNaN\nNaN\n\n\n2019-01-09\n22.568666\n22.900000\n22.098000\n22.366667\n81493500\nNaN\nNaN\n\n\n2019-01-10\n22.997999\n23.025999\n22.119333\n22.293333\n90846000\nNaN\nNaN\n\n\n2019-01-11\n23.150667\n23.227333\n22.584667\n22.806000\n75586500\nNaN\nNaN\n\n\n2019-01-14\n22.293333\n22.833332\n22.266666\n22.825333\n78709500\nNaN\nNaN\n\n\n2019-01-15\n22.962000\n23.253332\n22.299999\n22.333332\n90849000\nNaN\nNaN\n\n\n2019-01-16\n23.070000\n23.466667\n22.900000\n22.985332\n70375500\nNaN\nNaN\n\n\n2019-01-17\n23.153999\n23.433332\n22.943333\n23.080667\n55150500\nNaN\nNaN\n\n\n2019-01-18\n20.150667\n21.808666\n19.982000\n21.533333\n362262000\nNaN\nNaN\n\n\n2019-01-22\n19.927999\n20.533333\n19.700001\n20.321333\n181000500\nNaN\nNaN\n\n\n2019-01-23\n19.172667\n19.633333\n18.779333\n19.500000\n187950000\n0.938520\n0.946160\n\n\n2019-01-24\n19.434000\n19.578667\n18.618668\n18.868668\n120183000\n0.937771\n0.927867\n\n\n2019-01-25\n19.802668\n19.901333\n19.303333\n19.625999\n108744000\n0.969095\n0.953411\n\n\n2019-01-28\n19.758667\n19.830667\n19.183332\n19.527332\n96349500\n0.886399\n1.047633\n\n\n2019-01-29\n19.830667\n19.903999\n19.453333\n19.684668\n69325500\n0.853825\n1.081611\n\n\n2019-01-30\n20.584667\n20.600000\n19.899332\n20.030001\n168754500\n0.859650\n1.020518\n\n\n\n\n\n\n\n\n\nCalculate Volume-Weighted Sentiment\n\n\nCode\ndef json_reader(ticker):\n    with open(f'{ticker}_sentiment_raw.json', \"r\") as f:\n        sentiment_json_ticker = json.load(f)\n        sentiment_feed = sentiment_json_ticker.get(\"feed\", [])\n        sentiment_data = []\n        # Iterate through each item in the sentiment feed to extract relevant fields\n        for item in sentiment_feed:\n            try:\n                sentiment_data.append({\n                    # Convert the timestamp to pandas datetime for proper indexing\n                    \"time_published\": pd.to_datetime(item[\"time_published\"]),\n                    # Convert the sentiment score string to float\n                    \"sentiment_score\": float(item[\"overall_sentiment_score\"]),\n                    # Store the sentiment label (e.g., Positive, Neutral, Negative)\n                    \"sentiment_label\": item[\"overall_sentiment_label\"],\n                })\n            except (KeyError, ValueError, TypeError):\n                # Skip malformed or incomplete entries that raise an error\n                continue    \n        # Convert the structured list of dictionaries into a pandas DataFrame\n        sentiment_df = pd.DataFrame(sentiment_data)\n        # Set the 'time_published' column as the DataFrame index to enable time-series operations\n        # sentiment_df.set_index(\"time_published\", inplace=True)\n        sentiment_df['time_published']= pd.to_datetime(sentiment_df['time_published'].dt.date)\n    return sentiment_df\n    # globals()[f\"{ticker.lower()}_sentiment_data\"] = sentiment_data\n\n\n\n\nCode\ntsla_sentiment_df = json_reader('TSLA')\n\n\n\n\nCode\ntsla_sentiment_df.head()\n\n\n\n\n\n\n\n\n\ntime_published\nsentiment_score\nsentiment_label\n\n\n\n\n0\n2025-03-01\n0.225994\nSomewhat-Bullish\n\n\n1\n2025-02-28\n-0.098739\nNeutral\n\n\n2\n2025-02-28\n-0.041235\nNeutral\n\n\n3\n2025-02-28\n-0.038786\nNeutral\n\n\n4\n2025-02-28\n0.021961\nNeutral\n\n\n\n\n\n\n\n\n\nCode\ntsla_sentiment_scores_filtered = tsla_sentiment_df[(tsla_sentiment_df['time_published']).isin(tsla.index)]\ntsla_sentiment_scores_filtered = tsla_sentiment_scores_filtered.groupby('time_published')['sentiment_score'].mean().reset_index()\n\n\n\n\nCode\ntsla_merged_data = pd.merge(\n    tsla['Volume_TSLA'].reset_index().rename(columns={'Volume_TSLA': 'Volume'}),\n    tsla_sentiment_scores_filtered,\n    left_on='Date',\n    right_on='time_published',\n    how='inner'\n)\n# Compute the weighted sentiment by multiplying raw sentiment by trading volume\ntsla_merged_data['Weighted_Sentiment'] = tsla_merged_data['Volume'] * tsla_merged_data['sentiment_score']\n\n# Calculate a 5-day rolling average of the weighted sentiment to smooth short-term noise\ntsla_merged_data['5_day_avg_sentiment'] = tsla_merged_data['Weighted_Sentiment'].rolling(window=5).mean()\n\n# Define a binary condition for when the average sentiment is positive\ntsla_merged_data['Buy_Condition'] = tsla_merged_data['5_day_avg_sentiment'] &gt; 0\n\n# Normalize the rolling sentiment score by average volume to allow comparability across scales\ntsla_merged_data['5_day_avg_sentiment_norm'] = (\n    tsla_merged_data['5_day_avg_sentiment'] / tsla_merged_data['Volume'].mean()\n)\n\n\n\n\nCode\ntsla_merged_data.head()\n\n\n\n\n\n\n\n\n\nDate\nVolume\ntime_published\nsentiment_score\nWeighted_Sentiment\n5_day_avg_sentiment\nBuy_Condition\n5_day_avg_sentiment_norm\n\n\n\n\n0\n2025-01-31\n83568200\n2025-01-31\n0.194614\n1.626354e+07\nNaN\nFalse\nNaN\n\n\n1\n2025-02-03\n93732100\n2025-02-03\n0.129243\n1.211426e+07\nNaN\nFalse\nNaN\n\n\n2\n2025-02-04\n57072200\n2025-02-04\n0.173107\n9.879602e+06\nNaN\nFalse\nNaN\n\n\n3\n2025-02-05\n57223300\n2025-02-05\n0.136874\n7.832396e+06\nNaN\nFalse\nNaN\n\n\n4\n2025-02-06\n77918200\n2025-02-06\n0.118095\n9.201782e+06\n1.105832e+07\nTrue\n0.132787\n\n\n\n\n\n\n\n\n\nDerive ATR (10) for Volatility Adjustments\n\n\nCode\ndef calculate_true_range(df, ticker):\n    df[\"prev_close\"] = df[f'Close_{ticker}'].shift(1)\n    df[\"tr1\"] = df[f'High_{ticker}'] - df[f'Low_{ticker}']\n    df[\"tr2\"] = abs(df[f'High_{ticker}'] - df[\"prev_close\"])\n    df[\"tr3\"] = abs(df[f'Low_{ticker}'] - df[\"prev_close\"])\n    df[\"true_range\"] = df[[\"tr1\", \"tr2\", \"tr3\"]].max(axis=1)\n    df[\"ATR_10\"] = df[\"true_range\"].rolling(window=10).mean()\n    df[\"atr_pct\"] = df[\"ATR_10\"] / df[f'Close_{ticker}']\n    return df\n\ndef position_size(row):\n    if row[\"atr_pct\"] &lt; 0.03:  # &lt; 3% volatility → low risk\n        return 0.01  # allocate 1% of capital\n    else:  # ≥ 3% volatility → high risk\n        return 0.005  # allocate 0.5% of capital\n\n\n\n\nCode\ntsla = calculate_true_range(tsla, 'TSLA')\ntsla[\"position_size\"] = tsla.apply(position_size, axis=1)\n\n# ---- Preview ----\nprint(tsla[[\"Close_TSLA\", \"ATR_10\", \"atr_pct\", \"position_size\"]].tail(10))\n\n\n            Close_TSLA     ATR_10   atr_pct  position_size\nDate                                                      \n2025-02-19  360.559998  16.703000  0.046325          0.005\n2025-02-20  354.399994  16.464999  0.046459          0.005\n2025-02-21  337.799988  17.021997  0.050391          0.005\n2025-02-24  330.529999  16.770996  0.050740          0.005\n2025-02-25  302.799988  18.879996  0.062351          0.005\n2025-02-26  290.799988  18.412994  0.063318          0.005\n2025-02-27  281.950012  18.257996  0.064756          0.005\n2025-02-28  292.980011  18.067996  0.061670          0.005\n2025-03-03  284.649994  19.281998  0.067739          0.005\n2025-03-04  272.040009  20.654996  0.075926          0.005\n\n\n\n\nCode\n# Create a line chart to visualize the ATR% (Average True Range as a percentage of price) over time\nfig_atr_tsla = px.line(tsla, x=tsla.index, y=\"atr_pct\", title=\"ATR% Over Time\")\n\n# Add a horizontal reference line at 3% to represent the low-volatility cutoff threshold\nfig_atr_tsla.add_hline(\n    y=0.03, \n    line_dash=\"dot\", \n    line_color=\"green\", \n    annotation_text=\"Low Volatility Cutoff\"\n)\n\n# Display the chart\nfig_atr_tsla.show()\n# Display in Streamlit\n# st.subheader(\"ATR% Over Time for TSLA\")\n# st.plotly_chart(fig_atr_tsla, use_container_width=True)\n\n\n\n\n\nThe chart illustrates the historical volatility of TSLA, measured by the Average True Range (ATR) as a percentage of the closing price. Periods where the ATR% falls below the dotted green line at 3% indicate low volatility, which is typically associated with more stable market conditions. In contrast, noticeable spikes—such as those seen in 2020 and 2021—reflect periods of heightened volatility. More recently, ATR% values appear to remain closer to or slightly above the low-volatility threshold, suggesting relatively calmer market behavior compared to earlier years.\n\n\nCode\n# Filter the TSLA DataFrame to include only records from the year 2025\ntsla_2025 = tsla[tsla.index.year == 2025]\n\n# Create a line chart to visualize ATR% for TSLA during 2025\nfig = px.line(\n    tsla_2025,\n    x=tsla_2025.index,\n    y=\"atr_pct\",\n    title=\"TSLA ATR% Over Time (2025 Only)\"\n)\n\n# Add a horizontal line at the 3% threshold to denote the low-volatility cutoff\nfig.add_hline(\n    y=0.03,\n    line_dash=\"dot\",\n    line_color=\"green\",\n    annotation_text=\"Low Volatility Cutoff\"\n)\n\n# Display the chart\nfig.show()\n\n\n\n\n\nThe chart displays ATR% for TSLA during 2025, reflecting how the stock’s volatility has evolved since the start of the year. While ATR% began above the 7% mark in early January, it gradually declined and remained mostly between 4% and 6% throughout February. Although volatility did not breach the low-volatility threshold of 3%, the dip toward that level suggests a period of relative calm. Toward early March, ATR% showed a clear upward trend, indicating a potential resurgence in market volatility.\n\n\nCode\ndef signal_generation(df, ticker):\n    df['atr_pct'] = df['ATR_10'] / df['Close_' + ticker]\n\n    # Create Buy Signal (assuming VI_Cross_Up is defined elsewhere)\n    df['Buy_Signal'] = df['VI+'] &gt; df['VI-']  # Vortex crossover\n    # + add any other buy conditions here...\n\n    # Create Sell Signal (basic)\n    df['Sell_Signal'] = df['VI-'] &gt; df['VI+']\n\n    # Initialize position state\n    df['Position'] = 0\n    peak_price = 0\n\n    for i in range(1, len(df)):\n        if df['Buy_Signal'].iloc[i]:\n            df.at[df.index[i], 'Position'] = 1\n            peak_price = df['Close_' + ticker].iloc[i]\n        elif df['Position'].iloc[i - 1] == 1:\n            current_price = df['Close_' + ticker].iloc[i]\n            peak_price = max(peak_price, current_price)\n            drawdown = (peak_price - current_price) / peak_price\n\n            if drawdown &gt;= 0.03:\n                df.at[df.index[i], 'Sell_Signal'] = True  # trailing stop\n                df.at[df.index[i], 'Position'] = 0\n            else:\n                df.at[df.index[i], 'Position'] = 1    \n    return df\n\n\n\n\nCode\ntsla = signal_generation(tsla, 'TSLA')\n# Display the total number of buy and sell signals generated across the dataset\nprint(\"Buy signals:\", tsla['Buy_Signal'].sum())\nprint(\"Sell signals:\", tsla['Sell_Signal'].sum())\n\n\nBuy signals: 857\nSell signals: 680\n\n\n\n\nCode\n# Create an empty figure object\nfig = go.Figure()\n\n# Plot the TSLA closing price as a continuous line\nfig.add_trace(go.Scatter(\n    x=tsla.index,\n    y=tsla['Close_TSLA'],\n    mode='lines',\n    name='TSLA Price'\n))\n\n# Add markers to indicate Buy Signals using upward-pointing green triangles\nfig.add_trace(go.Scatter(\n    x=tsla[tsla['Buy_Signal']].index,\n    y=tsla[tsla['Buy_Signal']]['Close_TSLA'],\n    mode='markers',\n    marker=dict(symbol='triangle-up', size=10, color='green'),\n    name='Buy Signal'\n))\n\n# Add markers to indicate Sell Signals using downward-pointing red triangles\nfig.add_trace(go.Scatter(\n    x=tsla[tsla['Sell_Signal']].index,\n    y=tsla[tsla['Sell_Signal']]['Close_TSLA'],\n    mode='markers',\n    marker=dict(symbol='triangle-down', size=10, color='red'),\n    name='Sell Signal'\n))\n\n# Update layout settings including title and visual style\nfig.update_layout(\n    title='TSLA Buy & Sell Signals',\n    template='plotly_white'\n)\n\n# Render the interactive plot\nfig.show()\n\n\n\n\n\nThe chart illustrates the closing price of Tesla stock over time, with overlaid trading signals generated by the strategy. Green upward triangles represent buy signals, while red downward triangles mark sell signals. These signals are distributed throughout periods of both rising and falling prices, reflecting how the algorithm dynamically enters and exits positions based on market conditions. Clusters of signals during high-volatility periods—such as 2020, 2021, and early 2025—indicate frequent entries and exits, whereas more stable phases show fewer trades.\n\n\nCode\n# Calculate ATR as a percentage of the closing price to normalize volatility\ntsla['atr_pct'] = tsla['ATR_10'] / tsla['Close_TSLA']\n\n# Define Vortex Indicator crossover signals:\n# - VI_Cross_Up: Identifies when VI+ crosses above VI− (potential bullish signal)\n# - VI_Cross_Down: Identifies when VI− crosses above VI+ (potential bearish signal)\ntsla['VI_Cross_Up'] = (tsla['VI+'] &gt; tsla['VI-']) & (tsla['VI+'].shift(1) &lt;= tsla['VI-'].shift(1))\ntsla['VI_Cross_Down'] = (tsla['VI-'] &gt; tsla['VI+']) & (tsla['VI-'].shift(1) &lt;= tsla['VI+'].shift(1))\n\n# Initialize signal and state columns\ntsla['Buy_Signal'] = False          # Flag for buy signal\ntsla['Sell_Signal'] = False         # Flag for sell signal\ntsla['Position'] = 0                # Position state: 1 = in position, 0 = no position\ntsla['Entry_Type'] = None           # Strategy classification: 'aggressive' or 'conservative'\n\n# Initialize control variables for trailing stop and price tracking\nin_position = False                 # Boolean flag for current position state\npeak_price = 0                      # Highest price observed during an open position\n\n# Iterate through the DataFrame to simulate trading logic based on Vortex signals and volatility\nfor i in range(1, len(tsla)):\n    row = tsla.iloc[i]\n    idx = tsla.index[i]\n\n    # Buy condition: Enter a new position if VI_Cross_Up occurs and no current position is held\n    if not in_position and row['VI_Cross_Up']:\n        tsla.at[idx, 'Buy_Signal'] = True\n        tsla.at[idx, 'Position'] = 1\n        in_position = True\n        peak_price = row['Close_TSLA']\n\n        # Classify entry type based on volatility threshold\n        if row['atr_pct'] &lt; 0.03:\n            tsla.at[idx, 'Entry_Type'] = 'aggressive'\n        else:\n            tsla.at[idx, 'Entry_Type'] = 'conservative'\n\n    # While in position, evaluate for trailing stop or VI_Cross_Down exit condition\n    elif in_position:\n        current_price = row['Close_TSLA']\n        peak_price = max(peak_price, current_price)\n        drawdown = (peak_price - current_price) / peak_price\n\n        # Sell condition: Exit if drawdown exceeds 3% or VI_Cross_Down occurs\n        if drawdown &gt;= 0.03 or row['VI_Cross_Down']:\n            tsla.at[idx, 'Sell_Signal'] = True\n            tsla.at[idx, 'Position'] = 0\n            in_position = False\n        else:\n            tsla.at[idx, 'Position'] = 1  # Maintain position\n\n# Output the total count of each type of signal and entry classification\nprint(\"Buy signals:\", tsla['Buy_Signal'].sum())\nprint(\"Sell signals:\", tsla['Sell_Signal'].sum())\nprint(\"Aggressive entries:\", (tsla['Entry_Type'] == 'aggressive').sum())\nprint(\"Conservative entries:\", (tsla['Entry_Type'] == 'conservative').sum())\n\n\nBuy signals: 80\nSell signals: 80\nAggressive entries: 5\nConservative entries: 75\n\n\n\n\nCode\n# Create an empty figure to hold all plot layers\nfig = go.Figure()\n\n# Plot the tsla closing price as a continuous blue line\nfig.add_trace(go.Scatter(\n    x=tsla.index,\n    y=tsla['Close_TSLA'],\n    mode='lines',\n    name='TSLA Price',\n    line=dict(color='blue')\n))\n\n# Add markers for aggressive buy signals (Entry_Type = 'aggressive')\nfig.add_trace(go.Scatter(\n    x=tsla[(tsla['Buy_Signal']) & (tsla['Entry_Type'] == 'aggressive')].index,\n    y=tsla[(tsla['Buy_Signal']) & (tsla['Entry_Type'] == 'aggressive')]['Close_TSLA'],\n    mode='markers',\n    name='Buy (Aggressive)',\n    marker=dict(symbol='triangle-up', color='limegreen', size=10)\n))\n\n# Add markers for conservative buy signals (Entry_Type = 'conservative')\nfig.add_trace(go.Scatter(\n    x=tsla[(tsla['Buy_Signal']) & (tsla['Entry_Type'] == 'conservative')].index,\n    y=tsla[(tsla['Buy_Signal']) & (tsla['Entry_Type'] == 'conservative')]['Close_TSLA'],\n    mode='markers',\n    name='Buy (Conservative)',\n    marker=dict(symbol='triangle-up', color='green', size=10)\n))\n\n# Add markers for sell signals using red downward-pointing triangles\nfig.add_trace(go.Scatter(\n    x=tsla[tsla['Sell_Signal']].index,\n    y=tsla[tsla['Sell_Signal']]['Close_TSLA'],\n    mode='markers',\n    name='Sell Signal',\n    marker=dict(symbol='triangle-down', color='red', size=10)\n))\n\n# Configure chart layout with appropriate title, axis labels, and style\nfig.update_layout(\n    title='TSLA Buy/Sell Signals Over Time',\n    xaxis_title='Date',\n    yaxis_title='Price (USD)',\n    template='plotly_white',\n    height=600\n)\n\n# Render the figure\nfig.show()\n\n\n\n\n\nThe chart displays the historical closing price of Tesla (TSLA) stock alongside algorithmically generated buy and sell signals. The blue line represents TSLA’s closing price, while the green upward-pointing triangles indicate buy entries—distinguished by lime green for aggressive entries (lower volatility) and dark green for conservative entries (higher volatility). Red downward-pointing triangles represent sell signals.\nThe buy signals are generally aligned with upward momentum, and sell signals frequently follow periods of short-term retracement or heightened volatility. The system shows particularly dense activity around highly volatile phases, such as mid-2020 to early 2022, capturing many entries and exits. In contrast, during more stable periods, the signals are more spaced out. Overall, the plot provides a clear visual assessment of how the strategy adapts dynamically to changing market conditions by modulating its entries based on volatility and exiting with protective trailing logic."
  },
  {
    "objectID": "projects/vortex/main.html#tesla-analysis-results",
    "href": "projects/vortex/main.html#tesla-analysis-results",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "Tesla Analysis Results",
    "text": "Tesla Analysis Results\n\n\nCode\ntsla_merged_data = pd.merge(\n    tsla_merged_data, \n    tsla[['Close_TSLA', 'High_TSLA', 'Low_TSLA', 'Open_TSLA', 'Volume_TSLA',\n          'VI+', 'VI-', 'prev_close', 'tr1', 'tr2', 'tr3', 'true_range', 'ATR_10', 'position_size']], \n    on='Date', \n    how='left')\ntsla_merged_data.head()\n\n\n\n\n\n\n\n\n\nDate\nVolume\ntime_published\nsentiment_score\nWeighted_Sentiment\n5_day_avg_sentiment\nBuy_Condition\n5_day_avg_sentiment_norm\nClose_TSLA\nHigh_TSLA\n...\nVolume_TSLA\nVI+\nVI-\nprev_close\ntr1\ntr2\ntr3\ntrue_range\nATR_10\nposition_size\n\n\n\n\n0\n2025-01-31\n83568200\n2025-01-31\n0.194614\n1.626354e+07\nNaN\nFalse\nNaN\n404.600006\n419.989990\n...\n83568200\n1.012243\n0.857439\n400.279999\n18.649994\n19.709991\n1.059998\n19.709991\n18.478998\n0.005\n\n\n1\n2025-02-03\n93732100\n2025-02-03\n0.129243\n1.211426e+07\nNaN\nFalse\nNaN\n383.679993\n389.170013\n...\n93732100\n0.941453\n0.927890\n404.600006\n14.810028\n15.429993\n30.240021\n30.240021\n18.911002\n0.005\n\n\n2\n2025-02-04\n57072200\n2025-02-04\n0.173107\n9.879602e+06\nNaN\nFalse\nNaN\n392.209991\n394.000000\n...\n57072200\n0.911693\n0.973944\n383.679993\n12.600006\n10.320007\n2.279999\n12.600006\n17.482001\n0.005\n\n\n3\n2025-02-05\n57223300\n2025-02-05\n0.136874\n7.832396e+06\nNaN\nFalse\nNaN\n378.170013\n388.390015\n...\n57223300\n0.862377\n1.041572\n392.209991\n12.860016\n3.819977\n16.679993\n16.679993\n17.809000\n0.005\n\n\n4\n2025-02-06\n77918200\n2025-02-06\n0.118095\n9.201782e+06\n1.105832e+07\nTrue\n0.132787\n374.320007\n375.399994\n...\n77918200\n0.805785\n1.075550\n378.170013\n12.220001\n2.770020\n14.990021\n14.990021\n18.130002\n0.005\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\nCode\n# Calculate ATR percentage\ntsla_merged_data['atr_pct'] = tsla_merged_data['ATR_10'] / tsla_merged_data['Close_TSLA']\n\n# Vortex crossover logic\ntsla_merged_data['VI_Cross_Up'] = (tsla_merged_data['VI+'] &gt; tsla_merged_data['VI-']) & (tsla_merged_data['VI+'].shift(1) &lt;= tsla_merged_data['VI-'].shift(1))\ntsla_merged_data['VI_Cross_Down'] = (tsla_merged_data['VI-'] &gt; tsla_merged_data['VI+']) & (tsla_merged_data['VI-'].shift(1) &lt;= tsla_merged_data['VI+'].shift(1))\n\n# Initialize signal & state columns\ntsla_merged_data['Buy_Signal'] = False\ntsla_merged_data['Sell_Signal'] = False\ntsla_merged_data['Position'] = 0\ntsla_merged_data['Entry_Type'] = None  # aggressive/conservative\n\n# Trailing stop logic variables\nin_position = False\npeak_price = 0\n\nfor i in range(1, len(tsla_merged_data)):\n    row = tsla_merged_data.iloc[i]\n    idx = tsla_merged_data.index[i]\n    # Buy condition\n    if not in_position or row['VI_Cross_Up'] or row['5_day_avg_sentiment_norm']&gt;0:\n        tsla_merged_data.at[idx, 'Buy_Signal'] = True\n        tsla_merged_data.at[idx, 'Position'] = 1\n        in_position = True\n        peak_price = row['Close_TSLA']\n\n        # Entry Type: aggressive if ATR &lt; 3%, else conservative\n        if row['atr_pct'] &lt; 0.03:\n            tsla_merged_data.at[idx, 'Entry_Type'] = 'aggressive'\n        else:\n            tsla_merged_data.at[idx, 'Entry_Type'] = 'conservative'\n\n    # While in position, check for trailing stop or VI cross down\n    elif in_position:\n        current_price = row['Close_TSLA']\n        peak_price = max(peak_price, current_price)\n        drawdown = (peak_price - current_price) / peak_price\n\n        if drawdown &gt;= 0.03 or row['VI_Cross_Down']:\n            tsla_merged_data.at[idx, 'Sell_Signal'] = True\n            tsla_merged_data.at[idx, 'Position'] = 0\n            in_position = False\n        else:\n            tsla_merged_data.at[idx, 'Position'] = 1\n\n# Show result counts\nprint(\"Buy signals:\", tsla_merged_data['Buy_Signal'].sum())\nprint(\"Sell signals:\", tsla_merged_data['Sell_Signal'].sum())\nprint(\"Aggressive entries:\", (tsla_merged_data['Entry_Type'] == 'aggressive').sum())\nprint(\"Conservative entries:\", (tsla_merged_data['Entry_Type'] == 'conservative').sum())\n\n\nBuy signals: 18\nSell signals: 1\nAggressive entries: 0\nConservative entries: 18\n\n\n\n\nCode\n# Ensure 'Date' is datetime and set as index if needed\ntsla_merged_data['Date'] = pd.to_datetime(tsla_merged_data['Date'])\n\nfig = go.Figure()\n\n# Plot 5-day Avg Sentiment\nfig.add_trace(go.Scatter(\n    x=tsla_merged_data['Date'],\n    y=tsla_merged_data['5_day_avg_sentiment_norm'],\n    mode='lines+markers',\n    name='5-Day Avg Sentiment',\n    line=dict(color='blue')\n))\n\n# Plot ATR %\nfig.add_trace(go.Scatter(\n    x=tsla_merged_data['Date'],\n    y=tsla_merged_data['atr_pct'],\n    mode='lines+markers',\n    name='ATR %',\n    yaxis='y2',\n    line=dict(color='orange')\n))\n\n# Optional: Highlight Buy Signal Dates (even though there are none now)\nfig.add_trace(go.Scatter(\n    x=tsla_merged_data.loc[tsla_merged_data['Buy_Signal'], 'Date'],\n    y=tsla_merged_data.loc[tsla_merged_data['Buy_Signal'], '5_day_avg_sentiment_norm'],\n    mode='markers',\n    marker=dict(color='green', size=10, symbol='star'),\n    name='Buy Signal'\n))\n\n# Add dual axis layout\nfig.update_layout(\n    title=\"5-Day Sentiment vs ATR % (with Buy Signals)\",\n    xaxis_title='Date',\n    yaxis=dict(title='5-Day Avg Sentiment'),\n    yaxis2=dict(title='ATR %', overlaying='y', side='right'),\n    legend=dict(x=0.01, y=0.99),\n    height=500\n)\n\nfig.show()\n\n\n\n\n\n\n\nCode\ndef backtest(df, ticker):\n    capital = 100000\n    in_position = False\n    entry_price = 0\n    position_value = 0\n    cash = capital\n    returns = []\n\n    for i in range(len(df)):\n        row = df.iloc[i]\n\n        if row['Buy_Signal'] and not in_position:\n            position_size = row['position_size']\n            position_value = cash * position_size\n            entry_price = row['Close_' + ticker]\n            shares_bought = position_value / entry_price\n            cash -= position_value\n            in_position = True\n        elif row['Sell_Signal'] and in_position:\n            exit_price = row['Close_' + ticker]\n            proceeds = shares_bought * exit_price\n            profit = proceeds - position_value\n            cash += proceeds\n            returns.append(profit)\n            in_position = False\n            position_value = 0\n            entry_price = 0\n\n    final_value = cash + (shares_bought * row['Close_' + ticker] if in_position else 0)\n    total_return = final_value - capital\n    result = f\"Final Capital: ${final_value:.2f} \\nTotal Return: ${total_return:.2f} \\nTotal Trades: {len(returns)}\\nAverage Profit per Trade: ${np.mean(returns):.2f}\"\n    return result\n\n\n\n\nCode\nprint(backtest(tsla_merged_data, 'TSLA')) #w/ sentiment data\n\n\nFinal Capital: $99898.47 \nTotal Return: $-101.53 \nTotal Trades: 1\nAverage Profit per Trade: $11.12\n\n\n\n\nCode\nprint(backtest(tsla, 'TSLA')) #w/o sentiment data\n\n\nFinal Capital: $100575.32 \nTotal Return: $575.32 \nTotal Trades: 80\nAverage Profit per Trade: $7.19\n\n\n\n\nCode\ndef f_portfolio(df, ticker):\n    df = df.dropna(subset=['Close_' + ticker])\n    entries = df['Buy_Signal'].astype(bool)\n    exits = df['Sell_Signal'].astype(bool)\n\n    price = df['Close_' + ticker]\n    portfolio = vbt.Portfolio.from_signals(\n        close=price,\n        entries=entries,\n        exits=exits,\n        init_cash=100_000,\n        fees=0.001\n    )\n    return portfolio\n\n\n\n\nCode\n# without centiment data\ntsla_portfolio = f_portfolio(tsla, 'TSLA')\n\nprint(tsla_portfolio.stats())\ntsla_portfolio.plot().show()\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n\n\nStart                         2019-01-02 00:00:00\nEnd                           2025-03-04 00:00:00\nPeriod                                       1551\nStart Value                              100000.0\nEnd Value                           162759.235978\nTotal Return [%]                        62.759236\nBenchmark Return [%]                  1215.813231\nMax Gross Exposure [%]                      100.0\nTotal Fees Paid                      24054.581607\nMax Drawdown [%]                        55.348959\nMax Drawdown Duration                       730.0\nTotal Trades                                   80\nTotal Closed Trades                            80\nTotal Open Trades                               0\nOpen Trade PnL                                0.0\nWin Rate [%]                                 32.5\nBest Trade [%]                          46.283397\nWorst Trade [%]                         -9.410141\nAvg Winning Trade [%]                   11.344578\nAvg Losing Trade [%]                    -3.847352\nAvg Winning Trade Duration               7.076923\nAvg Losing Trade Duration                2.537037\nProfit Factor                            1.194803\nExpectancy                              784.49045\ndtype: object\n\n\n\n\n\nThe backtest results show that while the strategy achieved a total return of approximately 62.76%, it significantly underperformed compared to a simple buy-and-hold strategy on TSLA, which yielded a 1215.81% return. The strategy executed 80 trades with a low win rate of 32.5%, indicating that most trades were unprofitable. Although it had a few strong winners, the average profit per trade was marginal, with a profit factor of 1.19. Additionally, the portfolio experienced a substantial maximum drawdown of 55.35% and a prolonged recovery period lasting two years, signaling high risk. Visuals further confirm that many trades resulted in small losses or gains, with only a few notable profitable exits. Overall, while the strategy demonstrates some profitability, its risk-return profile is weak and may require optimization in entry/exit logic, volatility filtering, or sentiment integration to compete with the benchmark performance."
  },
  {
    "objectID": "projects/vortex/main.html#xly-analysis-results",
    "href": "projects/vortex/main.html#xly-analysis-results",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "XLY Analysis Results",
    "text": "XLY Analysis Results\n\n\nCode\nxly = calculate_true_range(xly, 'XLY')\nxly[\"position_size\"] = xly.apply(position_size, axis=1)\n\n# ---- Preview ----\nprint(xly[[\"Close_XLY\", \"ATR_10\", \"atr_pct\", \"position_size\"]].tail(10))\n\n\n             Close_XLY    ATR_10   atr_pct  position_size\nDate                                                     \n2025-02-19  225.618988  2.870099  0.012721           0.01\n2025-02-20  223.674316  2.919964  0.013055           0.01\n2025-02-21  217.790527  3.453495  0.015857           0.01\n2025-02-24  216.972778  3.270997  0.015076           0.01\n2025-02-25  215.835892  3.511334  0.016269           0.01\n2025-02-26  214.948349  3.602083  0.016758           0.01\n2025-02-27  211.846878  3.751672  0.017709           0.01\n2025-02-28  215.367203  3.836439  0.017813           0.01\n2025-03-03  211.398117  4.429805  0.020955           0.01\n2025-03-04  207.668396  4.845659  0.023334           0.01\n\n\n\n\nCode\nfig = px.line(xly, x=xly.index, y=\"atr_pct\", title=\"ATR% Over Time\")\nfig.add_hline(y=0.03, line_dash=\"dot\", line_color=\"green\", annotation_text=\"Low Volatility Cutoff\")\nfig.show()\n\n\n\n\n\n\n\nCode\n# Filter only 2025 data\nxly_2025 = xly[xly.index.year == 2025]\n\n# Plot\nfig = px.line(xly_2025, x=xly_2025.index, y=\"atr_pct\", title=\"XLY ATR% Over Time (2025 Only)\")\nfig.add_hline(y=0.03, line_dash=\"dot\", line_color=\"green\", annotation_text=\"Low Volatility Cutoff\")\nfig.show()\n\n\n\n\n\n\n\nCode\nxly = signal_generation(xly, 'XLY')\n\n\n\n\nCode\nprint(backtest(xly, 'XLY'))\n\n\nFinal Capital: $100729.52 \nTotal Return: $729.52 \nTotal Trades: 76\nAverage Profit per Trade: $9.60\n\n\n\n\nCode\nxly_portfolio = f_portfolio(xly, 'XLY')\nprint(xly_portfolio.stats())\nxly_portfolio.plot().show()\n\n\nStart                         2019-01-02 00:00:00\nEnd                           2025-03-04 00:00:00\nPeriod                                       1551\nStart Value                              100000.0\nEnd Value                           170848.194798\nTotal Return [%]                        70.848195\nBenchmark Return [%]                   120.815504\nMax Gross Exposure [%]                      100.0\nTotal Fees Paid                      21558.870642\nMax Drawdown [%]                        33.668417\nMax Drawdown Duration                       793.0\nTotal Trades                                   76\nTotal Closed Trades                            76\nTotal Open Trades                               0\nOpen Trade PnL                                0.0\nWin Rate [%]                            44.736842\nBest Trade [%]                          37.025745\nWorst Trade [%]                        -13.070482\nAvg Winning Trade [%]                    4.635492\nAvg Losing Trade [%]                    -2.172936\nAvg Winning Trade Duration              22.558824\nAvg Losing Trade Duration                4.690476\nProfit Factor                            1.512842\nExpectancy                             932.213089\ndtype: object\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set"
  },
  {
    "objectID": "projects/vortex/main.html#spy-analysis-results",
    "href": "projects/vortex/main.html#spy-analysis-results",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "SPY Analysis Results",
    "text": "SPY Analysis Results\n\n\nCode\nspy = calculate_true_range(spy, 'SPY')\nspy[\"position_size\"] = spy.apply(position_size, axis=1)\n\n# ---- Preview ----\nprint(spy[[\"Close_SPY\", \"ATR_10\", \"atr_pct\", \"position_size\"]].tail(10))\n\n\n             Close_SPY    ATR_10   atr_pct  position_size\nDate                                                     \n2025-02-19  611.091675  4.794563  0.007846           0.01\n2025-02-20  608.549377  4.806522  0.007898           0.01\n2025-02-21  598.140686  5.513399  0.009218           0.01\n2025-02-24  595.418884  5.359863  0.009002           0.01\n2025-02-25  592.457764  5.718790  0.009653           0.01\n2025-02-26  592.756836  6.146507  0.010369           0.01\n2025-02-27  583.295288  6.801538  0.011661           0.01\n2025-02-28  592.397949  7.353875  0.012414           0.01\n2025-03-03  582.019165  8.901222  0.015294           0.01\n2025-03-04  575.129883  9.901217  0.017216           0.01\n\n\n\n\nCode\nfig = px.line(spy, x=spy.index, y=\"atr_pct\", title=\"SPY ATR% Over Time\")\nfig.add_hline(y=0.03, line_dash=\"dot\", line_color=\"green\", annotation_text=\"Low Volatility Cutoff\")\nfig.show()\n\n\n\n\n\n\n\nCode\n# Filter only 2025 data\nspy_2025 = spy[spy.index.year == 2025]\n\n# Plot\nfig = px.line(spy_2025, x=spy_2025.index, y=\"atr_pct\", title=\"SPY ATR% Over Time (2025 Only)\")\nfig.add_hline(y=0.03, line_dash=\"dot\", line_color=\"green\", annotation_text=\"Low Volatility Cutoff\")\nfig.show()\n\n\n\n\n\n\n\nCode\nspy = signal_generation(spy, 'SPY')\n\n\n\n\nCode\nprint(backtest(spy, 'SPY'))\n\n\nFinal Capital: $100515.03 \nTotal Return: $515.03 \nTotal Trades: 56\nAverage Profit per Trade: $9.20\n\n\n\n\nCode\nspy_portfolio = f_portfolio(spy, 'SPY')\nprint(spy_portfolio.stats())\nspy_portfolio.plot().show()\n\n\nStart                         2019-01-02 00:00:00\nEnd                           2025-03-04 00:00:00\nPeriod                                       1551\nStart Value                              100000.0\nEnd Value                           149876.046124\nTotal Return [%]                        49.876046\nBenchmark Return [%]                   153.411688\nMax Gross Exposure [%]                      100.0\nTotal Fees Paid                      14500.381668\nMax Drawdown [%]                        19.809446\nMax Drawdown Duration                       584.0\nTotal Trades                                   56\nTotal Closed Trades                            56\nTotal Open Trades                               0\nOpen Trade PnL                                0.0\nWin Rate [%]                            55.357143\nBest Trade [%]                           7.385099\nWorst Trade [%]                         -9.885438\nAvg Winning Trade [%]                    3.135409\nAvg Losing Trade [%]                    -2.130089\nAvg Winning Trade Duration              28.258065\nAvg Losing Trade Duration                    7.56\nProfit Factor                            1.712196\nExpectancy                             890.643681\ndtype: object\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set"
  },
  {
    "objectID": "projects/vortex/main.html#optimization-tsla",
    "href": "projects/vortex/main.html#optimization-tsla",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "Optimization (TSLA)",
    "text": "Optimization (TSLA)\n\n\nCode\n# Define a list of different smoothing periods to test for the Vortex Indicator\nperiods = [7, 14, 21, 30]\nresults = {}  # Dictionary to store performance metrics for each period\n\n# Loop through each smoothing period\nfor n in periods:\n    # === Compute Vortex Indicator for the given period ===\n    tsla[f'VI+{n}'], tsla[f'VI-{n}'] = calculate_vortex(tsla, 'TSLA', n)\n\n    # === Generate Buy/Sell signals based on crossover logic ===\n    # Buy when VI+ crosses above VI-\n    tsla[f'Buy_{n}'] = tsla[f'VI+{n}'] &gt; tsla[f'VI-{n}']\n    # Sell when VI- crosses above VI+\n    tsla[f'Sell_{n}'] = tsla[f'VI-{n}'] &gt; tsla[f'VI+{n}']\n\n    # === Convert boolean signals to actual entry/exit Series ===\n    entries = tsla[f'Buy_{n}']\n    exits = tsla[f'Sell_{n}']\n\n    # === Run a backtest using vectorbt Portfolio object ===\n    portfolio = vbt.Portfolio.from_signals(\n        close=tsla['Close_TSLA'],  # TSLA closing prices\n        entries=entries,\n        exits=exits,\n        size=1,  # Assume buying 1 share per trade\n        init_cash=10_000  # Initial capital for backtest\n    )\n\n    # === Store backtest performance metrics in results dict ===\n    stats = portfolio.stats()\n    results[n] = stats\n\n# Identify the period with the highest total return\nbest_period = max(results, key=lambda x: results[x]['Total Return [%]'])\nprint(f\"✅ Best Performing Period: {best_period} days\")\n\n# Rebuild portfolio using the best period to visualize it\nportfolio = vbt.Portfolio.from_signals(\n    close=tsla['Close_TSLA'],\n    entries=tsla[f'VI+{best_period}'] &gt; tsla[f'VI-{best_period}'],\n    exits=tsla[f'VI-{best_period}'] &gt; tsla[f'VI+{best_period}'],\n    size=1,\n    init_cash=10_000\n)\n\n# Plot the results of the best strategy\nportfolio.plot().show()\nprint(portfolio.stats())\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n\n\n✅ Best Performing Period: 7 days\n\n\n\n\n\nStart                         2019-01-02 00:00:00\nEnd                           2025-03-04 00:00:00\nPeriod                                       1551\nStart Value                               10000.0\nEnd Value                            10480.194603\nTotal Return [%]                         4.801946\nBenchmark Return [%]                  1215.813231\nMax Gross Exposure [%]                   4.554966\nTotal Fees Paid                               0.0\nMax Drawdown [%]                         0.793073\nMax Drawdown Duration                       351.0\nTotal Trades                                  113\nTotal Closed Trades                           113\nTotal Open Trades                               0\nOpen Trade PnL                                0.0\nWin Rate [%]                            44.247788\nBest Trade [%]                         128.434899\nWorst Trade [%]                        -15.721837\nAvg Winning Trade [%]                   14.052436\nAvg Losing Trade [%]                    -4.125181\nAvg Winning Trade Duration                  11.44\nAvg Losing Trade Duration                4.206349\nProfit Factor                            2.096188\nExpectancy                                4.24951\ndtype: object\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set"
  },
  {
    "objectID": "projects/vortex/main.html#peer-comparison-apple-analysis-results",
    "href": "projects/vortex/main.html#peer-comparison-apple-analysis-results",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "Peer Comparison: Apple Analysis Results",
    "text": "Peer Comparison: Apple Analysis Results\n\n\nCode\naapl = yf.download('AAPL', start='2019-01-01', end='2025-03-05')\naapl = multiindex_to_singleindex(aapl)\nget_news_sentiment('AAPL', '20250101T0130', '20250301T0130', 1000, 'PNM5EHRALIOT1CKJ')\naapl['VI+'], aapl['VI-'] = calculate_vortex(aapl, 'AAPL')\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\naapl = calculate_true_range(aapl, 'AAPL')\naapl[\"position_size\"] = aapl.apply(position_size, axis=1)\n\n# ---- Preview ----\nprint(aapl[[\"Close_AAPL\", \"ATR_10\", \"atr_pct\", \"position_size\"]].tail(10))\n\n\n            Close_AAPL    ATR_10   atr_pct  position_size\nDate                                                     \n2025-02-19  244.869995  4.939392  0.020171           0.01\n2025-02-20  245.830002  4.735891  0.019265           0.01\n2025-02-21  245.550003  4.746260  0.019329           0.01\n2025-02-24  247.100006  4.517000  0.018280           0.01\n2025-02-25  247.039993  4.687000  0.018973           0.01\n2025-02-26  240.360001  4.719998  0.019637           0.01\n2025-02-27  237.300003  4.631998  0.019520           0.01\n2025-02-28  241.839996  5.143999  0.021270           0.01\n2025-03-03  238.029999  5.479999  0.023022           0.01\n2025-03-04  235.929993  5.685001  0.024096           0.01\n\n\n\n\nCode\naapl = signal_generation(aapl, 'AAPL')\n# Display the total number of buy and sell signals generated across the dataset\nprint(\"Buy signals:\", aapl['Buy_Signal'].sum())\nprint(\"Sell signals:\", aapl['Sell_Signal'].sum())\n\n\nBuy signals: 985\nSell signals: 552\n\n\n\n\nCode\n# Calculate ATR as a percentage of the closing price to normalize volatility\naapl['atr_pct'] = aapl['ATR_10'] / aapl['Close_AAPL']\n\n# Define Vortex Indicator crossover signals:\n# - VI_Cross_Up: Identifies when VI+ crosses above VI− (potential bullish signal)\n# - VI_Cross_Down: Identifies when VI− crosses above VI+ (potential bearish signal)\naapl['VI_Cross_Up'] = (aapl['VI+'] &gt; aapl['VI-']) & (aapl['VI+'].shift(1) &lt;= aapl['VI-'].shift(1))\naapl['VI_Cross_Down'] = (aapl['VI-'] &gt; aapl['VI+']) & (aapl['VI-'].shift(1) &lt;= aapl['VI+'].shift(1))\n\n# Initialize signal and state columns\naapl['Buy_Signal'] = False          # Flag for buy signal\naapl['Sell_Signal'] = False         # Flag for sell signal\naapl['Position'] = 0                # Position state: 1 = in position, 0 = no position\naapl['Entry_Type'] = None           # Strategy classification: 'aggressive' or 'conservative'\n\n# Initialize control variables for trailing stop and price tracking\nin_position = False                 # Boolean flag for current position state\npeak_price = 0                      # Highest price observed during an open position\n\n# Iterate through the DataFrame to simulate trading logic based on Vortex signals and volatility\nfor i in range(1, len(aapl)):\n    row = aapl.iloc[i]\n    idx = aapl.index[i]\n\n    # Buy condition: Enter a new position if VI_Cross_Up occurs and no current position is held\n    if not in_position and row['VI_Cross_Up']:\n        aapl.at[idx, 'Buy_Signal'] = True\n        aapl.at[idx, 'Position'] = 1\n        in_position = True\n        peak_price = row['Close_AAPL']\n\n        # Classify entry type based on volatility threshold\n        if row['atr_pct'] &lt; 0.03:\n            aapl.at[idx, 'Entry_Type'] = 'aggressive'\n        else:\n            aapl.at[idx, 'Entry_Type'] = 'conservative'\n\n    # While in position, evaluate for trailing stop or VI_Cross_Down exit condition\n    elif in_position:\n        current_price = row['Close_AAPL']\n        peak_price = max(peak_price, current_price)\n        drawdown = (peak_price - current_price) / peak_price\n\n        # Sell condition: Exit if drawdown exceeds 3% or VI_Cross_Down occurs\n        if drawdown &gt;= 0.03 or row['VI_Cross_Down']:\n            aapl.at[idx, 'Sell_Signal'] = True\n            aapl.at[idx, 'Position'] = 0\n            in_position = False\n        else:\n            aapl.at[idx, 'Position'] = 1  # Maintain position\n\n# Output the total count of each type of signal and entry classification\nprint(\"Buy signals:\", aapl['Buy_Signal'].sum())\nprint(\"Sell signals:\", aapl['Sell_Signal'].sum())\nprint(\"Aggressive entries:\", (aapl['Entry_Type'] == 'aggressive').sum())\nprint(\"Conservative entries:\", (aapl['Entry_Type'] == 'conservative').sum())\n\n\nBuy signals: 66\nSell signals: 66\nAggressive entries: 45\nConservative entries: 21\n\n\n\n\nCode\naapl_sentiment_df = json_reader('AAPL')\n\n\n\n\nCode\naapl_sentiment_scores_filtered = aapl_sentiment_df[(aapl_sentiment_df['time_published']).isin(aapl.index)]\naapl_sentiment_scores_filtered = aapl_sentiment_scores_filtered.groupby('time_published')['sentiment_score'].mean().reset_index()\n\n\n\n\nCode\naapl_merged_data = pd.merge(\n    aapl['Volume_AAPL'].reset_index().rename(columns={'Volume_AAPL': 'Volume'}),\n    aapl_sentiment_scores_filtered,\n    left_on='Date',\n    right_on='time_published',\n    how='inner'\n)\n# Compute the weighted sentiment by multiplying raw sentiment by trading volume\naapl_merged_data['Weighted_Sentiment'] = aapl_merged_data['Volume'] * aapl_merged_data['sentiment_score']\n\n# Calculate a 5-day rolling average of the weighted sentiment to smooth short-term noise\naapl_merged_data['5_day_avg_sentiment'] = aapl_merged_data['Weighted_Sentiment'].rolling(window=5).mean()\n\n# Define a binary condition for when the average sentiment is positive\naapl_merged_data['Buy_Condition'] = aapl_merged_data['5_day_avg_sentiment'] &gt; 0\n\n# Normalize the rolling sentiment score by average volume to allow comparability across scales\naapl_merged_data['5_day_avg_sentiment_norm'] = (\n    aapl_merged_data['5_day_avg_sentiment'] / aapl_merged_data['Volume'].mean()\n)\n\n\n\n\nCode\naapl = calculate_true_range(aapl, 'AAPL')\naapl[\"position_size\"] = aapl.apply(position_size, axis=1)\n\n# ---- Preview ----\nprint(aapl[[\"Close_AAPL\", \"ATR_10\", \"atr_pct\", \"position_size\"]].tail(10))\n\n\n            Close_AAPL    ATR_10   atr_pct  position_size\nDate                                                     \n2025-02-19  244.869995  4.939392  0.020171           0.01\n2025-02-20  245.830002  4.735891  0.019265           0.01\n2025-02-21  245.550003  4.746260  0.019329           0.01\n2025-02-24  247.100006  4.517000  0.018280           0.01\n2025-02-25  247.039993  4.687000  0.018973           0.01\n2025-02-26  240.360001  4.719998  0.019637           0.01\n2025-02-27  237.300003  4.631998  0.019520           0.01\n2025-02-28  241.839996  5.143999  0.021270           0.01\n2025-03-03  238.029999  5.479999  0.023022           0.01\n2025-03-04  235.929993  5.685001  0.024096           0.01\n\n\n\n\nCode\naapl_merged_data = pd.merge(\n    aapl_merged_data, \n    aapl[['Close_AAPL', 'High_AAPL', 'Low_AAPL', 'Open_AAPL', 'Volume_AAPL',\n          'VI+', 'VI-', 'prev_close', 'tr1', 'tr2', 'tr3', 'true_range', 'ATR_10', 'position_size']], \n    on='Date', \n    how='left')\naapl_merged_data.head()\n\n\n\n\n\n\n\n\n\nDate\nVolume\ntime_published\nsentiment_score\nWeighted_Sentiment\n5_day_avg_sentiment\nBuy_Condition\n5_day_avg_sentiment_norm\nClose_AAPL\nHigh_AAPL\n...\nVolume_AAPL\nVI+\nVI-\nprev_close\ntr1\ntr2\ntr3\ntrue_range\nATR_10\nposition_size\n\n\n\n\n0\n2025-01-15\n39832000\n2025-01-15\n0.223177\n8.889575e+06\nNaN\nFalse\nNaN\n237.608749\n238.697564\n...\n39832000\n0.595080\n1.102317\n233.023788\n4.525039\n5.673775\n1.148737\n5.673775\n5.283191\n0.01\n\n\n1\n2025-01-16\n71759100\n2025-01-16\n0.237567\n1.704756e+07\nNaN\nFalse\nNaN\n228.009308\n237.748600\n...\n71759100\n0.524560\n1.139218\n237.608749\n9.969035\n0.139851\n9.829185\n9.969035\n5.895516\n0.01\n\n\n2\n2025-01-17\n68488300\n2025-01-17\n0.130304\n8.924326e+06\nNaN\nFalse\nNaN\n229.727417\n232.034878\n...\n68488300\n0.506950\n1.231532\n228.009308\n3.805813\n4.025570\n0.219757\n4.025570\n5.439019\n0.01\n\n\n3\n2025-01-21\n98070400\n2025-01-21\n0.169273\n1.660064e+07\nNaN\nFalse\nNaN\n222.395477\n224.173521\n...\n98070400\n0.514695\n1.233423\n229.727417\n5.034458\n5.553896\n10.588354\n10.588354\n6.269107\n0.01\n\n\n4\n2025-01-22\n64126500\n2025-01-22\n0.182421\n1.169803e+07\n1.263203e+07\nTrue\n0.231401\n223.584167\n223.873842\n...\n64126500\n0.570450\n1.200538\n222.395477\n4.325246\n1.478365\n2.846881\n4.325246\n6.289084\n0.01\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\nCode\n# Calculate ATR percentage\naapl_merged_data['atr_pct'] = aapl_merged_data['ATR_10'] / aapl_merged_data['Close_AAPL']\n\n# Vortex crossover logic\naapl_merged_data['VI_Cross_Up'] = (aapl_merged_data['VI+'] &gt; aapl_merged_data['VI-']) & (aapl_merged_data['VI+'].shift(1) &lt;= aapl_merged_data['VI-'].shift(1))\naapl_merged_data['VI_Cross_Down'] = (aapl_merged_data['VI-'] &gt; aapl_merged_data['VI+']) & (aapl_merged_data['VI-'].shift(1) &lt;= aapl_merged_data['VI+'].shift(1))\n\n# Initialize signal & state columns\naapl_merged_data['Buy_Signal'] = False\naapl_merged_data['Sell_Signal'] = False\naapl_merged_data['Position'] = 0\naapl_merged_data['Entry_Type'] = None  # aggressive/conservative\n\n# Trailing stop logic variables\nin_position = False\npeak_price = 0\n\nfor i in range(1, len(aapl_merged_data)):\n    row = aapl_merged_data.iloc[i]\n    idx = aapl_merged_data.index[i]\n    # Buy condition\n    if not in_position or row['VI_Cross_Up'] or row['5_day_avg_sentiment_norm']&gt;0:\n        aapl_merged_data.at[idx, 'Buy_Signal'] = True\n        aapl_merged_data.at[idx, 'Position'] = 1\n        in_position = True\n        peak_price = row['Close_AAPL']\n\n        # Entry Type: aggressive if ATR &lt; 3%, else conservative\n        if row['atr_pct'] &lt; 0.03:\n            aapl_merged_data.at[idx, 'Entry_Type'] = 'aggressive'\n        else:\n            aapl_merged_data.at[idx, 'Entry_Type'] = 'conservative'\n\n    # While in position, check for trailing stop or VI cross down\n    elif in_position:\n        current_price = row['Close_AAPL']\n        peak_price = max(peak_price, current_price)\n        drawdown = (peak_price - current_price) / peak_price\n\n        if drawdown &gt;= 0.03 or row['VI_Cross_Down']:\n            aapl_merged_data.at[idx, 'Sell_Signal'] = True\n            aapl_merged_data.at[idx, 'Position'] = 0\n            in_position = False\n        else:\n            aapl_merged_data.at[idx, 'Position'] = 1\n\n# Show result counts\nprint(\"Buy signals:\", aapl_merged_data['Buy_Signal'].sum())\nprint(\"Sell signals:\", aapl_merged_data['Sell_Signal'].sum())\nprint(\"Aggressive entries:\", (aapl_merged_data['Entry_Type'] == 'aggressive').sum())\nprint(\"Conservative entries:\", (aapl_merged_data['Entry_Type'] == 'conservative').sum())\n\n\nBuy signals: 28\nSell signals: 1\nAggressive entries: 23\nConservative entries: 5\n\n\n\n\nCode\n# Ensure 'Date' is datetime and set as index if needed\naapl_merged_data['Date'] = pd.to_datetime(aapl_merged_data['Date'])\n\nfig = go.Figure()\n\n# Plot 5-day Avg Sentiment\nfig.add_trace(go.Scatter(\n    x=aapl_merged_data['Date'],\n    y=aapl_merged_data['5_day_avg_sentiment_norm'],\n    mode='lines+markers',\n    name='5-Day Avg Sentiment',\n    line=dict(color='blue')\n))\n\n# Plot ATR %\nfig.add_trace(go.Scatter(\n    x=aapl_merged_data['Date'],\n    y=aapl_merged_data['atr_pct'],\n    mode='lines+markers',\n    name='ATR %',\n    yaxis='y2',\n    line=dict(color='orange')\n))\n\n# Optional: Highlight Buy Signal Dates (even though there are none now)\nfig.add_trace(go.Scatter(\n    x=aapl_merged_data.loc[aapl_merged_data['Buy_Signal'], 'Date'],\n    y=aapl_merged_data.loc[aapl_merged_data['Buy_Signal'], '5_day_avg_sentiment_norm'],\n    mode='markers',\n    marker=dict(color='green', size=10, symbol='star'),\n    name='Buy Signal'\n))\n\n# Add dual axis layout\nfig.update_layout(\n    title=\"5-Day Sentiment vs ATR % (with Buy Signals)\",\n    xaxis_title='Date',\n    yaxis=dict(title='5-Day Avg Sentiment'),\n    yaxis2=dict(title='ATR %', overlaying='y', side='right'),\n    legend=dict(x=0.01, y=0.99),\n    height=500\n)\n\nfig.show()\n\n\n\n\n\n\n\nCode\nprint(backtest(aapl_merged_data, 'AAPL')) #w/ sentiment data\n\n\nFinal Capital: $100057.01 \nTotal Return: $57.01 \nTotal Trades: 1\nAverage Profit per Trade: $-24.62\n\n\n\n\nCode\nprint(backtest(aapl, 'AAPL')) #w/o sentiment data\n\n\nFinal Capital: $101198.29 \nTotal Return: $1198.29 \nTotal Trades: 66\nAverage Profit per Trade: $18.16\n\n\n\n\nCode\n# without centiment data\naapl_portfolio = f_portfolio(aapl, 'AAPL')\n\nprint(aapl_portfolio.stats())\naapl_portfolio.plot().show()\n\n\nStart                         2019-01-02 00:00:00\nEnd                           2025-03-04 00:00:00\nPeriod                                       1551\nStart Value                              100000.0\nEnd Value                           255472.954051\nTotal Return [%]                       155.472954\nBenchmark Return [%]                   526.354355\nMax Gross Exposure [%]                      100.0\nTotal Fees Paid                        23330.2112\nMax Drawdown [%]                        14.949616\nMax Drawdown Duration                       238.0\nTotal Trades                                   66\nTotal Closed Trades                            66\nTotal Open Trades                               0\nOpen Trade PnL                                0.0\nWin Rate [%]                            48.484848\nBest Trade [%]                          18.319731\nWorst Trade [%]                         -5.776766\nAvg Winning Trade [%]                     5.44886\nAvg Losing Trade [%]                    -2.071776\nAvg Winning Trade Duration                16.5625\nAvg Losing Trade Duration                4.176471\nProfit Factor                            2.213935\nExpectancy                            2355.650819\ndtype: object\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n\n\n\n\n\nBased on the results from applying the trading strategy to the Apple (AAPL) ticker, we can reasonably conclude that the strategy does work on peers like AAPL. The strategy delivered a total return of approximately 282% over the backtest period (2019–2025), compared to a benchmark return of about 526%, which indicates it captured a significant portion of the upward trend while actively managing trades. Although it underperformed the benchmark in absolute terms, this is typical of signal-driven strategies that trade in and out of the market. The profit factor of 2.11, expectancy of 4204, and a win rate of 45.5% suggest the strategy was profitable overall. Additionally, the drawdown was moderate (20.87%), reflecting a reasonable risk exposure relative to the potential reward.\nThe cumulative returns graph further supports this interpretation. The strategy closely follows the broader market trend, generating consistent gains and outperforming during certain periods. The trade PnL distribution shows a good number of winning trades with healthy profitability, and although there were losses, the downside was generally contained. Therefore, this peer comparison confirms that the strategy generalizes reasonably well beyond TSLA, making it a potentially viable approach for other high-liquidity technology stocks like AAPL."
  },
  {
    "objectID": "projects/video_games/classification.html",
    "href": "projects/video_games/classification.html",
    "title": "Classification Tree",
    "section": "",
    "text": "Developed a classification model to predict video game sales performance using a real-world video game dataset. Preprocessed the data through binning, factor conversion, and top-category filtering. Built, visualized, and pruned decision trees using cross-validation, and evaluated model performance using confusion matrices.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(rpart)\nlibrary(rpart.plot)\n\nvg &lt;- read.csv('vgchartz-2024.csv')\n\nThis dataset appears to contain information about video games, including the title, the console it was developed for, genre, publisher, developer, game rating, various sales numbers, release date, and, if available, the last update date. While some titles appear multiple times, other variables indicate that these entries correspond to the same game released on different consoles or different versions, such as remakes. The critic_score represents the overall rating of a game, with a maximum score of 10. And the dataset includes sales data across different regions (NA, JP, PAL, Other), and difference between numbers suggest the game popularity across various geographical areas. Furthermore, there is huge number of NAs.\n\n\n\n\n\n\nAfter using equal frequency binning method to our output variable, we can see that the data equally was distributed within three groups: Low, Medium, High.\n\nvg$total_sales &lt;- cut(vg$total_sales, \n                      breaks = quantile(vg$total_sales, probs = seq(0, 1, length.out = 4), na.rm = TRUE), \n                      include.lowest = TRUE,\n                      labels = c(\"Low\", \"Medium\", \"High\"))\ntable(vg$total_sales)\n\n\n   Low Medium   High \n  6339   6386   6197 \n\n\n\nstr(vg)\n\n'data.frame':   64016 obs. of  14 variables:\n $ img         : chr  \"/games/boxart/full_6510540AmericaFrontccc.jpg\" \"/games/boxart/full_5563178AmericaFrontccc.jpg\" \"/games/boxart/827563ccc.jpg\" \"/games/boxart/full_9218923AmericaFrontccc.jpg\" ...\n $ title       : chr  \"Grand Theft Auto V\" \"Grand Theft Auto V\" \"Grand Theft Auto: Vice City\" \"Grand Theft Auto V\" ...\n $ console     : chr  \"PS3\" \"PS4\" \"PS2\" \"X360\" ...\n $ genre       : chr  \"Action\" \"Action\" \"Action\" \"Action\" ...\n $ publisher   : chr  \"Rockstar Games\" \"Rockstar Games\" \"Rockstar Games\" \"Rockstar Games\" ...\n $ developer   : chr  \"Rockstar North\" \"Rockstar North\" \"Rockstar North\" \"Rockstar North\" ...\n $ critic_score: num  9.4 9.7 9.6 NA 8.1 8.7 8.8 9.8 8.4 8 ...\n $ total_sales : Factor w/ 3 levels \"Low\",\"Medium\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ na_sales    : num  6.37 6.06 8.41 9.06 6.18 9.07 9.76 5.26 8.27 4.99 ...\n $ jp_sales    : num  0.99 0.6 0.47 0.06 0.41 0.13 0.11 0.21 0.07 0.65 ...\n $ pal_sales   : num  9.85 9.71 5.49 5.33 6.05 4.29 3.73 6.21 4.32 5.88 ...\n $ other_sales : num  3.12 3.02 1.78 1.42 2.44 1.33 1.14 2.26 1.2 2.28 ...\n $ release_date: chr  \"2013-09-17\" \"2014-11-18\" \"2002-10-28\" \"2013-09-17\" ...\n $ last_update : chr  \"\" \"2018-01-03\" \"\" \"\" ...\n\n\n\nvg$img &lt;- as.factor(vg$img)\nvg$title &lt;- as.factor(vg$title)\nvg$console &lt;- as.factor(vg$console)\nvg$genre &lt;- as.factor(vg$genre)\nvg$publisher &lt;- as.factor(vg$publisher)\nvg$developer &lt;- as.factor(vg$developer)\nvg$release_date &lt;- as.factor(vg$release_date)\nvg$last_update &lt;- as.factor(vg$last_update)\n\nstr(vg)\n\n'data.frame':   64016 obs. of  14 variables:\n $ img         : Factor w/ 56177 levels \"/games/boxart/1000386ccc.jpg\",..: 31526 27433 6702 42887 25079 47407 47389 23589 12344 23571 ...\n $ title       : Factor w/ 39798 levels \"_summer ##\",\"- Arcane preRaise -\",..: 13767 13767 13779 13767 5281 5291 5276 27100 5283 5283 ...\n $ console     : Factor w/ 81 levels \"2600\",\"3DO\",\"3DS\",..: 57 58 56 76 58 76 76 58 76 57 ...\n $ genre       : Factor w/ 20 levels \"Action\",\"Action-Adventure\",..: 1 1 1 1 16 16 16 2 16 16 ...\n $ publisher   : Factor w/ 3383 levels \"][ Games\",\"@unepic_fran\",..: 2514 2514 2514 2514 84 84 84 2514 84 84 ...\n $ developer   : Factor w/ 8863 levels \"\",\".theprodukkt\",..: 6580 6580 6580 6580 8072 3787 8072 6576 8072 8072 ...\n $ critic_score: num  9.4 9.7 9.6 NA 8.1 8.7 8.8 9.8 8.4 8 ...\n $ total_sales : Factor w/ 3 levels \"Low\",\"Medium\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ na_sales    : num  6.37 6.06 8.41 9.06 6.18 9.07 9.76 5.26 8.27 4.99 ...\n $ jp_sales    : num  0.99 0.6 0.47 0.06 0.41 0.13 0.11 0.21 0.07 0.65 ...\n $ pal_sales   : num  9.85 9.71 5.49 5.33 6.05 4.29 3.73 6.21 4.32 5.88 ...\n $ other_sales : num  3.12 3.02 1.78 1.42 2.44 1.33 1.14 2.26 1.2 2.28 ...\n $ release_date: Factor w/ 7923 levels \"\",\"1971-12-03\",..: 6007 6357 2845 6007 6573 5506 5197 7244 5769 5769 ...\n $ last_update : Factor w/ 1546 levels \"\",\"2017-11-28\",..: 1 15 1 1 26 1 1 285 109 109 ...\n\n\nAfter converting all chr parameters to factor, our dataset contains only numbers or factors.\nNext we are going to do some “topN” filtering.\n\ntop_7_Console &lt;- vg %&gt;% \n  count(console, sort = TRUE) %&gt;% \n  slice(1:7)\ntop_7_Console\n\n  console     n\n1      PC 12617\n2     PS2  3565\n3      DS  3288\n4     PS4  2878\n5      PS  2707\n6      NS  2337\n7     XBL  2120\n\n\n7 popular console types: PC, PS2, DS, PS4, PS, NS, XBL. Also there is huge difference between PC and other remaining values, making PC much more popular. This is likely due to its open system, allowing users to install and download games freely without strict platform restrictions.\n\ntop_7_Genre &lt;- vg %&gt;% \n  count(genre, sort = TRUE) %&gt;% \n  slice(1:7)\ntop_7_Genre\n\n         genre    n\n1         Misc 9304\n2       Action 8557\n3    Adventure 6260\n4 Role-Playing 5721\n5       Sports 5586\n6      Shooter 5410\n7     Platform 4001\n\n\n7 most common genres: misc, action, adventure, role-playing, sports, shooter, platform. The most prevalent genre is Miscellaneous, likely due to games that don’t fit into traditional categories or are poorly documented. Following that, Action games are the second most popular, with Adventure games ranking third. The popularity of platform games (Mario, Zelda or Sonic) shows that traditional gameplay styles still have a strong place in market.\n\ntop_7_Publisher &lt;- vg %&gt;% \n  count(publisher, sort = TRUE) %&gt;% \n  slice(1:7) \ntop_7_Publisher\n\n        publisher    n\n1         Unknown 8842\n2            Sega 2207\n3         Ubisoft 1663\n4 Electronic Arts 1619\n5      Activision 1582\n6          Konami 1544\n7        Nintendo 1476\n\n\n7 most common publishers: unknown, sega, ubisoft, electronic arts, activision, konami, nintendo. Notably, 8,842 records lack publisher information, categorized under “Unknown.” Among named publishers, Sega remains a key player. Also, Ubisoft, EA, Activision have released most of the popular games over the last few years.\n\ntop_7_Developer &lt;- vg %&gt;% \n  count(developer, sort = TRUE) %&gt;% \n  slice(1:7)\ntop_7_Developer\n\n        developer    n\n1         Unknown 4435\n2          Konami  976\n3            Sega  915\n4          Capcom  870\n5           Namco  489\n6     Square Enix  425\n7 SNK Corporation  408\n\n\n7 most common developers: unknown, konami, sega, namco, square enix, capcom, snk corporation. Similarly, 4,435 records lack developer information. Several major developers, such as Sega and Konami, also appear among the top publishers, reinforcing their influence in the industry.\n\nvg &lt;- vg %&gt;% filter(console %in% top_7_Console$console & \n                      genre %in% top_7_Genre$genre &\n                      publisher %in% top_7_Publisher$publisher &\n                      developer %in% top_7_Developer$developer)\n\nnrow(vg)\n\n[1] 939\n\n\nAfter applying these filters, the dataset is reduced to 939 rows.\n\nvg &lt;- droplevels(vg)\nnrow(vg)\n\n[1] 939\n\n\nThe dataset contains 939 records.\n\nset.seed(79)\nvg.index &lt;- sample(c(1:nrow(vg)), nrow(vg)*0.6)\nvg_train &lt;- vg[vg.index, ]\nvg_valid &lt;- vg[-vg.index, ]\n\nNext we will use rpart.plot to display a classification tree that depicts our model.\n\nmodel &lt;- rpart(total_sales ~ console + genre + publisher + developer + critic_score, method = 'class', data = vg_train)\n\nrpart.plot(model)\n\n\n\n\n\n\n\n\nThe initial plot starts with default parameters, where the tree starts from console varaible at the root. Each node represents type of total_sales (Low, Medium, High) differentiated by color. Within each node, we see class probabilities and the percentage of observations classified at that node.\n\nrpart.plot(model, extra=2, fallen.leaves = FALSE)\n\n\n\n\n\n\n\n\nAfter adding extra=2, in the new plot the class rate is relative to the total number of observations in each class. For instance, in the second left node, 50 observations belong to Low sales category out of 102 total observations. And setting fallen.leaves = FALSE, moves the leaf nodes away from the bottom, changing the structure of trees.\n\nrpart.plot(model, type=5, extra = 2, fallen.leaves=FALSE)\n\n\n\n\n\n\n\n\nIn the third plot used type=5, which adds variable names for each split line, and class labels appear only at the leaves. This layout more intuitive, as it avoids overwhelming details such as numbers and labels.\nThe root node is console variable, and rule is Console = DS or PC. The root node is starting point of model, and have highest impact on outcome variable. In our model, the type of console is primary factor to decide total sales number. So, this approach can help strategize future plans for the game publisher, focusing more on high-performance consoles.\nWe see that from 5 input variables (console, genre, publisher, developer, critic score), only 2 (console, genre) appears in the tree model, as a useful parameters to predict price. Variable subset selection is automatic since it is part of the split selection.\n\nrpart.rules(model)\n\n total_sales   Low Medi High                                                                                                    \n         Low [ .56  .27  .16] when console is DS or NS or PC or PS or PS2 & genre is Action or Adventure or Misc or Role-Playing\n      Medium [ .19  .57  .23] when console is DS or NS or PC or PS or PS2 & genre is                                      Sports\n        High [ .11  .22  .67] when console is DS or NS or PC or PS or PS2 & genre is                         Platform or Shooter\n        High [ .00  .00 1.00] when console is                         PS4                                                       \n\n\nFrom the rules of our model, let’s describe second rule (index = 10): The video game is for PS console and Sports genre, by following tree nodes this game will be classified into Medium sales group.\n\ncomplex_model &lt;- rpart(total_sales ~ console + genre + publisher + developer + critic_score, method=\"class\", cp=0, minsplit=2, data = vg_train)\n\nrpart.plot(complex_model, extra=1, fallen.leaves=FALSE)\n\n\n\n\n\n\n\n\n\nfive_fold_cv &lt;- rpart(total_sales ~ console + genre + publisher + developer + critic_score, method=\"class\",\n                      cp=0.00001, minsplit=5, xval=5, data=vg_train)\n\na &lt;- printcp(five_fold_cv)\n\n\nClassification tree:\nrpart(formula = total_sales ~ console + genre + publisher + developer + \n    critic_score, data = vg_train, method = \"class\", cp = 1e-05, \n    minsplit = 5, xval = 5)\n\nVariables actually used in tree construction:\n[1] console      critic_score genre       \n\nRoot node error: 81/127 = 0.6378\n\nn=127 (436 observations deleted due to missingness)\n\n         CP nsplit rel error  xerror     xstd\n1 0.1666667      0   1.00000 1.08642 0.064178\n2 0.0925926      2   0.66667 0.85185 0.069303\n3 0.0493827      4   0.48148 0.85185 0.069303\n4 0.0370370      5   0.43210 0.77778 0.069562\n5 0.0246914      6   0.39506 0.76543 0.069545\n6 0.0123457      7   0.37037 0.77778 0.069562\n7 0.0061728     10   0.33333 0.77778 0.069562\n8 0.0000100     12   0.32099 0.80247 0.069545\n\na &lt;- data.frame(a)\na\n\n          CP nsplit rel.error    xerror       xstd\n1 0.16666667      0 1.0000000 1.0864198 0.06417809\n2 0.09259259      2 0.6666667 0.8518519 0.06930294\n3 0.04938272      4 0.4814815 0.8518519 0.06930294\n4 0.03703704      5 0.4320988 0.7777778 0.06956221\n5 0.02469136      6 0.3950617 0.7654321 0.06954496\n6 0.01234568      7 0.3703704 0.7777778 0.06956221\n7 0.00617284     10 0.3333333 0.7777778 0.06956221\n8 0.00001000     12 0.3209877 0.8024691 0.06954496\n\n\nFrom the complexity parameter table for eight trees, we see that xerror value decreased at some point and started to increase again. This minimum value of cross validation error (0.7037037) gives optimal cp value. In our case cp = 0.03703704\n\npruned.ct &lt;- prune(five_fold_cv,\n                   cp=five_fold_cv$cptable[which.min(five_fold_cv$cptable[,\"xerror\"]),\"CP\"])\n\nrpart.plot(pruned.ct,type=5, extra = 2, fallen.leaves=FALSE)\n\n\n\n\n\n\n\n\n\n# Huge tree results\ncomplex_model.pred &lt;- predict(complex_model, vg_train, type=\"class\")\nconfusionMatrix(complex_model.pred, vg_train$total_sales)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low     39     10    4\n    Medium   5     33    9\n    High     1      3   23\n\nOverall Statistics\n                                          \n               Accuracy : 0.748           \n                 95% CI : (0.6633, 0.8208)\n    No Information Rate : 0.3622          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.617           \n                                          \n Mcnemar's Test P-Value : 0.09099         \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity              0.8667        0.7174      0.6389\nSpecificity              0.8293        0.8272      0.9560\nPos Pred Value           0.7358        0.7021      0.8519\nNeg Pred Value           0.9189        0.8375      0.8700\nPrevalence               0.3543        0.3622      0.2835\nDetection Rate           0.3071        0.2598      0.1811\nDetection Prevalence     0.4173        0.3701      0.2126\nBalanced Accuracy        0.8480        0.7723      0.7975\n\ncomplex_model.pred2 &lt;- predict(complex_model, vg_valid, type=\"class\")\nconfusionMatrix(complex_model.pred2, vg_valid$total_sales)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low     18     15    3\n    Medium   4      8    5\n    High     5      2    7\n\nOverall Statistics\n                                          \n               Accuracy : 0.4925          \n                 95% CI : (0.3682, 0.6176)\n    No Information Rate : 0.403           \n    P-Value [Acc &gt; NIR] : 0.08620         \n                                          \n                  Kappa : 0.2096          \n                                          \n Mcnemar's Test P-Value : 0.04293         \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity              0.6667        0.3200      0.4667\nSpecificity              0.5500        0.7857      0.8654\nPos Pred Value           0.5000        0.4706      0.5000\nNeg Pred Value           0.7097        0.6600      0.8491\nPrevalence               0.4030        0.3731      0.2239\nDetection Rate           0.2687        0.1194      0.1045\nDetection Prevalence     0.5373        0.2537      0.2090\nBalanced Accuracy        0.6083        0.5529      0.6660\n\n\nThe fully grown tree model have a high accuracy for training data (74.8%), but its performance drops significantly on the validation data (49.25%). This suggests that the model is overfitting, meaning it has learned the noise and specific patterns of the training set that do not generalize well to new, unseen data.\n\n# Pruned tree results\npruned.ct.pred &lt;- predict(pruned.ct, vg_train, type=\"class\")\nconfusionMatrix(pruned.ct.pred, vg_train$total_sales)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low     32     10    4\n    Medium  12     34   17\n    High     1      2   15\n\nOverall Statistics\n                                          \n               Accuracy : 0.6378          \n                 95% CI : (0.5478, 0.7212)\n    No Information Rate : 0.3622          \n    P-Value [Acc &gt; NIR] : 2.688e-10       \n                                          \n                  Kappa : 0.4443          \n                                          \n Mcnemar's Test P-Value : 0.003155        \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity              0.7111        0.7391      0.4167\nSpecificity              0.8293        0.6420      0.9670\nPos Pred Value           0.6957        0.5397      0.8333\nNeg Pred Value           0.8395        0.8125      0.8073\nPrevalence               0.3543        0.3622      0.2835\nDetection Rate           0.2520        0.2677      0.1181\nDetection Prevalence     0.3622        0.4961      0.1417\nBalanced Accuracy        0.7702        0.6906      0.6918\n\npruned.ct.pred2 &lt;- predict(pruned.ct, vg_valid, type=\"class\")\nconfusionMatrix(pruned.ct.pred2, vg_valid$total_sales)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low     14     13    2\n    Medium   9     10   10\n    High     4      2    3\n\nOverall Statistics\n                                        \n               Accuracy : 0.403         \n                 95% CI : (0.2849, 0.53)\n    No Information Rate : 0.403         \n    P-Value [Acc &gt; NIR] : 0.54633       \n                                        \n                  Kappa : 0.0583        \n                                        \n Mcnemar's Test P-Value : 0.08112       \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity              0.5185        0.4000     0.20000\nSpecificity              0.6250        0.5476     0.88462\nPos Pred Value           0.4828        0.3448     0.33333\nNeg Pred Value           0.6579        0.6053     0.79310\nPrevalence               0.4030        0.3731     0.22388\nDetection Rate           0.2090        0.1493     0.04478\nDetection Prevalence     0.4328        0.4328     0.13433\nBalanced Accuracy        0.5718        0.4738     0.54231\n\n\nIn comparison the pruned tree shows lower accuracy on both the training set (61.42%) and the validation set (41.79%). While the overall accuracy is lower, the smaller performance gap between training and validation data indicates that the pruned tree generalizes better and is less prone to overfitting. Although the huge tree model appears to perform better in terms of accuracy. Therefore, the pruned model is more reliable for making predictions on new data, even if it comes at the cost of slightly lower accuracy. When working with the model that has more than two outcome parameters, the accuracy not always enough to evaluate performance. More variables, more chance the model predict wrong class, and baseline accuracy also drops down, making a high accuracy harder to achieve.\nWhen using a pruned tree, the difference between training and validation accuracy decreases because pruning reduces overfitting. By removing unnecessary splits, the model captures meaningful patterns rather than noise. As a result the training accuracy decreases, but validation accuracy remains more stable making the model more reliable to new data."
  },
  {
    "objectID": "projects/spotify/analysis.html",
    "href": "projects/spotify/analysis.html",
    "title": "K-Nearest Neighbors: You Like This Song…But Will George Like It?",
    "section": "",
    "text": "In this project, I’ll use k-NN clustering analysis to find out whether George—a fictional character—would vibe with my song or not.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(skimr)\nlibrary(caret)\nlibrary(FNN)\nlibrary(ggplot2)\n\nspotify_2023 &lt;- read.csv('spotify-2023.csv')\n\n\nstr(spotify_2023)\n\n'data.frame':   953 obs. of  24 variables:\n $ track_name          : chr  \"Seven (feat. Latto) (Explicit Ver.)\" \"LALA\" \"vampire\" \"Cruel Summer\" ...\n $ artist.s._name      : chr  \"Latto, Jung Kook\" \"Myke Towers\" \"Olivia Rodrigo\" \"Taylor Swift\" ...\n $ artist_count        : int  2 1 1 1 1 2 2 1 1 2 ...\n $ released_year       : int  2023 2023 2023 2019 2023 2023 2023 2023 2023 2023 ...\n $ released_month      : int  7 3 6 8 5 6 3 7 5 3 ...\n $ released_day        : int  14 23 30 23 18 1 16 7 15 17 ...\n $ in_spotify_playlists: int  553 1474 1397 7858 3133 2186 3090 714 1096 2953 ...\n $ in_spotify_charts   : int  147 48 113 100 50 91 50 43 83 44 ...\n $ streams             : chr  \"141381703\" \"133716286\" \"140003974\" \"800840817\" ...\n $ in_apple_playlists  : int  43 48 94 116 84 67 34 25 60 49 ...\n $ in_apple_charts     : int  263 126 207 207 133 213 222 89 210 110 ...\n $ in_deezer_playlists : chr  \"45\" \"58\" \"91\" \"125\" ...\n $ in_deezer_charts    : int  10 14 14 12 15 17 13 13 11 13 ...\n $ in_shazam_charts    : chr  \"826\" \"382\" \"949\" \"548\" ...\n $ bpm                 : int  125 92 138 170 144 141 148 100 130 170 ...\n $ key                 : chr  \"B\" \"C#\" \"F\" \"A\" ...\n $ mode                : chr  \"Major\" \"Major\" \"Major\" \"Major\" ...\n $ danceability_.      : int  80 71 51 55 65 92 67 67 85 81 ...\n $ valence_.           : int  89 61 32 58 23 66 83 26 22 56 ...\n $ energy_.            : int  83 74 53 72 80 58 76 71 62 48 ...\n $ acousticness_.      : int  31 7 17 11 14 19 48 37 12 21 ...\n $ instrumentalness_.  : int  0 0 0 0 63 0 0 0 0 0 ...\n $ liveness_.          : int  8 10 31 11 11 8 8 11 28 8 ...\n $ speechiness_.       : int  4 4 6 15 6 24 3 4 9 33 ...\n\n\nMy song is Money Trees - Kendrick Lamar, Jay Rock. I’m a big fan of Kendrick’s music, and especially songs from this album is one of the favorites of mine.\nHere is the values of this song from dataset:\ndanceability: 74\nenergy: 53\nspeechiness: 10\nacousticness: 7\nliveness: 21\nvalence: 37\nBPM: 144\n\nmy_song &lt;- spotify_2023 %&gt;% filter(track_name == 'Money Trees')\nmy_song\n\n\n\n\n\n\n\n\nspotify &lt;- read.csv('spotify.csv')\n\nstr(spotify)\n\n'data.frame':   2017 obs. of  17 variables:\n $ X               : int  0 1 2 3 4 5 6 7 8 9 ...\n $ acousticness    : num  0.0102 0.199 0.0344 0.604 0.18 0.00479 0.0145 0.0202 0.0481 0.00208 ...\n $ danceability    : num  0.833 0.743 0.838 0.494 0.678 0.804 0.739 0.266 0.603 0.836 ...\n $ duration_ms     : int  204600 326933 185707 199413 392893 251333 241400 349667 202853 226840 ...\n $ energy          : num  0.434 0.359 0.412 0.338 0.561 0.56 0.472 0.348 0.944 0.603 ...\n $ instrumentalness: num  2.19e-02 6.11e-03 2.34e-04 5.10e-01 5.12e-01 0.00 7.27e-06 6.64e-01 0.00 0.00 ...\n $ key             : int  2 1 2 5 5 8 1 10 11 7 ...\n $ liveness        : num  0.165 0.137 0.159 0.0922 0.439 0.164 0.207 0.16 0.342 0.571 ...\n $ loudness        : num  -8.79 -10.4 -7.15 -15.24 -11.65 ...\n $ mode            : int  1 1 1 1 0 1 1 0 0 1 ...\n $ speechiness     : num  0.431 0.0794 0.289 0.0261 0.0694 0.185 0.156 0.0371 0.347 0.237 ...\n $ tempo           : num  150.1 160.1 75 86.5 174 ...\n $ time_signature  : num  4 4 4 4 4 4 4 4 4 4 ...\n $ valence         : num  0.286 0.588 0.173 0.23 0.904 0.264 0.308 0.393 0.398 0.386 ...\n $ target          : int  1 1 1 1 1 1 1 1 1 1 ...\n $ song_title      : chr  \"Mask Off\" \"Redbone\" \"Xanny Family\" \"Master Of None\" ...\n $ artist          : chr  \"Future\" \"Childish Gambino\" \"Future\" \"Beach House\" ...\n\nspotify$target &lt;- factor(spotify$target)\nlevels(spotify$target)\n\n[1] \"0\" \"1\"\n\ntable(spotify$target)\n\n\n   0    1 \n 997 1020 \n\n\n\nData Exploration\nTarget variable is of type int, then I converted it to categorical variable (factor).\nThe target factor variable has 2 categories: 0 or 1. By counting total number of rows for each category, we get that George has 1020 favorite, and 997 disliked songs. Which is interesting, that number of disliked ones pretty close to liked. The music taste of George can be diverse, and Spotify’s recommendation system might be actively adjusting to his preferences. Actually, when you dislike one song in Spotify, the system tries to not suggest you similar songs, and try other different options. To state this opinion constantly we need to explore more about song preferences of George. Furthermore, there are could be temporal patterns in George’s preferences, for instance he prefer certain types of songs at different times of day, month or year.\n\ncolSums(is.na(spotify))\n\n               X     acousticness     danceability      duration_ms \n               0                0                0                0 \n          energy instrumentalness              key         liveness \n               0                0                0                0 \n        loudness             mode      speechiness            tempo \n               0                0                0                0 \n  time_signature          valence           target       song_title \n               0                0                0                0 \n          artist \n               0 \n\n\nThere is no NA values in this dataset.\n\nskim(spotify_2023)\n\n\nData summary\n\n\nName\nspotify_2023\n\n\nNumber of rows\n953\n\n\nNumber of columns\n24\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n7\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntrack_name\n0\n1\n2\n123\n0\n943\n0\n\n\nartist.s._name\n0\n1\n1\n117\n0\n645\n0\n\n\nstreams\n0\n1\n4\n102\n0\n949\n0\n\n\nin_deezer_playlists\n0\n1\n1\n6\n0\n348\n0\n\n\nin_shazam_charts\n0\n1\n0\n5\n50\n199\n0\n\n\nkey\n0\n1\n0\n2\n95\n12\n0\n\n\nmode\n0\n1\n5\n5\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nartist_count\n0\n1\n1.56\n0.89\n1\n1\n1\n2\n8\n▇▁▁▁▁\n\n\nreleased_year\n0\n1\n2018.24\n11.12\n1930\n2020\n2022\n2022\n2023\n▁▁▁▁▇\n\n\nreleased_month\n0\n1\n6.03\n3.57\n1\n3\n6\n9\n12\n▇▆▅▃▆\n\n\nreleased_day\n0\n1\n13.93\n9.20\n1\n6\n13\n22\n31\n▇▅▃▆▃\n\n\nin_spotify_playlists\n0\n1\n5200.12\n7897.61\n31\n875\n2224\n5542\n52898\n▇▁▁▁▁\n\n\nin_spotify_charts\n0\n1\n12.01\n19.58\n0\n0\n3\n16\n147\n▇▁▁▁▁\n\n\nin_apple_playlists\n0\n1\n67.81\n86.44\n0\n13\n34\n88\n672\n▇▁▁▁▁\n\n\nin_apple_charts\n0\n1\n51.91\n50.63\n0\n7\n38\n87\n275\n▇▃▂▁▁\n\n\nin_deezer_charts\n0\n1\n2.67\n6.04\n0\n0\n0\n2\n58\n▇▁▁▁▁\n\n\nbpm\n0\n1\n122.54\n28.06\n65\n100\n121\n140\n206\n▃▇▇▃▁\n\n\ndanceability_.\n0\n1\n66.97\n14.63\n23\n57\n69\n78\n96\n▁▃▆▇▃\n\n\nvalence_.\n0\n1\n51.43\n23.48\n4\n32\n51\n70\n97\n▅▇▇▇▅\n\n\nenergy_.\n0\n1\n64.28\n16.55\n9\n53\n66\n77\n97\n▁▂▅▇▃\n\n\nacousticness_.\n0\n1\n27.06\n26.00\n0\n6\n18\n43\n97\n▇▃▂▁▁\n\n\ninstrumentalness_.\n0\n1\n1.58\n8.41\n0\n0\n0\n0\n91\n▇▁▁▁▁\n\n\nliveness_.\n0\n1\n18.21\n13.71\n3\n10\n12\n24\n97\n▇▂▁▁▁\n\n\nspeechiness_.\n0\n1\n10.13\n9.91\n2\n4\n6\n11\n64\n▇▁▁▁▁\n\n\n\n\nspotify_2023$danceability_. &lt;- spotify_2023$danceability_./100\nspotify_2023$energy_. &lt;- spotify_2023$energy_./100\nspotify_2023$speechiness_. &lt;- spotify_2023$speechiness_./100\nspotify_2023$valence_. &lt;- spotify_2023$valence_./100\nspotify_2023$acousticness_. &lt;- spotify_2023$acousticness_./100\nspotify_2023$liveness_. &lt;- spotify_2023$liveness_./100\n\nspotify_2023 &lt;- spotify_2023 %&gt;% rename(danceability=danceability_., energy=energy_., speechiness=speechiness_., valence=valence_., acousticness=acousticness_., liveness=liveness_., tempo=bpm)\n\nmy_song &lt;- spotify_2023 %&gt;% filter(track_name == 'Money Trees')\n\nI converted the values in spotify_23 to decimal format. Also, applied the same changes to my_song by recreating it.\n\n\nData Partition\n\nset.seed(79)\nspotify.index &lt;- sample(c(1:nrow(spotify)), nrow(spotify)*0.6)\nspotify_train.df &lt;- spotify[spotify.index, ]\nspotify_valid.df &lt;- spotify[-spotify.index, ]\n\n\nliked &lt;- spotify_train.df %&gt;% filter(target==1)\ndisliked &lt;- spotify_train.df %&gt;% filter(target==0)\n\nt.test(liked$danceability, disliked$danceability)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$danceability and disliked$danceability\nt = 5.9297, df = 1198.7, p-value = 3.965e-09\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.03618685 0.07197390\nsample estimates:\nmean of x mean of y \n0.6451020 0.5910216 \n\nt.test(liked$tempo, disliked$tempo)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$tempo and disliked$tempo\nt = 0.32976, df = 1188, p-value = 0.7416\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.495354  3.503627\nsample estimates:\nmean of x mean of y \n 121.3866  120.8825 \n\nt.test(liked$energy, disliked$energy)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$energy and disliked$energy\nt = 0.90709, df = 1116.6, p-value = 0.3646\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.01228227  0.03340286\nsample estimates:\nmean of x mean of y \n0.6986168 0.6880565 \n\nt.test(liked$speechiness, disliked$speechiness)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$speechiness and disliked$speechiness\nt = 5.9565, df = 1102.5, p-value = 3.461e-09\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01988822 0.03942699\nsample estimates:\n mean of x  mean of y \n0.10598684 0.07632924 \n\nt.test(liked$valence, disliked$valence)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$valence and disliked$valence\nt = 3.4455, df = 1207.9, p-value = 0.0005895\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.02065933 0.07529932\nsample estimates:\nmean of x mean of y \n0.5254077 0.4774284 \n\nt.test(liked$acousticness, disliked$acousticness)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$acousticness and disliked$acousticness\nt = -4.0701, df = 1122.3, p-value = 5.028e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.08721203 -0.03047773\nsample estimates:\nmean of x mean of y \n0.1508101 0.2096550 \n\nt.test(liked$liveness, disliked$liveness)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$liveness and disliked$liveness\nt = 1.5056, df = 1192.1, p-value = 0.1324\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.004181182  0.031773731\nsample estimates:\nmean of x mean of y \n0.1991905 0.1853942 \n\n\nBased on the results above, here is the list of variables that show significant difference: Danceability(p_value = 3.965e-09), speechiness(p-value = 3.461e-09), valence(p-value = 0.0005895), acousticness(p-value = 5.028e-05). Very low p-value suggests that, there is significant difference on this values between liked and disliked songs, making them main parameters to identify George’s preferences in music. Other remaining variables have p-value more than typical threshold 0.05: tempo(p-value = 0.7416), energy(p-value = 0.3646), liveness(p-value = 0.1324).\n\nspotify_train.df &lt;- spotify_train.df %&gt;% select(-tempo, -energy, -liveness)\n\nk-NN method draws information from similarities between the variables by measuring distance between records. Variables with similar values across different outcome classes cannot provide useful information for distinguishing between groups. Including such variables can lead to overfitting, where the model performs well on training data but fails to generalize to new data. These insignificant variables affect the distance calculation, making it harder to distinguish between groups.\n\nhead(spotify_train.df)\n\n\n\n\n\n\n\n\n\nNormalization\nIn this step we are normalizing only those columns that will be used in k-NN model building.\n\nspotify_train_norm.df &lt;- spotify_train.df\nspotify_valid_norm.df &lt;- spotify_valid.df\nspotify_norm.df &lt;- spotify\nmy_song_norm &lt;- my_song\n\n\nnorm_values &lt;- preProcess(\n  spotify_train.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")], \n  method = c(\"center\", \"scale\"))\n\nspotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] &lt;- \n  predict(norm_values, spotify_train.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_train_norm.df)\n\nspotify_valid_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] &lt;- \n  predict(norm_values, spotify_valid.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_valid_norm.df)\n\nspotify_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] &lt;- \n  predict(norm_values, spotify[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_norm.df)\n\nmy_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] &lt;- \n  predict(norm_values, my_song[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n\n\nprint(my_song_norm)\n\n\n\n\n\n\n\n\n\nClustering\n\n# knn is all about numeric data, classification using numeric values\nnn &lt;- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = my_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],##what we are classifying: like or dislike\n           k=7)\nprint(nn)\n\n[1] 1\nattr(,\"nn.index\")\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]  823  966  277  929  653  984  436\nattr(,\"nn.dist\")\n          [,1]      [,2]      [,3]      [,4]      [,5]     [,6]      [,7]\n[1,] 0.0233238 0.3074891 0.3136877 0.3537573 0.4308537 0.471654 0.4811864\nLevels: 1\n\n\n\nnn_indexes &lt;- row.names(spotify_train.df)[attr(nn, \"nn.index\")]\nspotify_train.df[nn_indexes, ] %&gt;% select(song_title, artist, target, acousticness, danceability, speechiness, valence)\n\n\n\n\n\n\n\nBy running k-NN model as a result get “1”, which indicates that George will like my song. And by listing 7 nearest neighbors, I see that my song is also in this list and George already marked it as favorite. Within this songs, George marked only one song as disliked, which highlights not all similar songs are guaranteed to be liked. This disliked song has high valence value compared to others, but there is no difference in other variables. By running knn classification we get 7 nearest records with low distance value from our selected song. So if we just use numbers these songs look very similar to each other. But they are not. And the diversity of artists suggests George’s musical preferences are varied.\n\naccuracy.df &lt;- data.frame(k = seq(1,14,1), accuracy = rep(0,14))\n\nfor(i in 1:14) {\n  knn.pred &lt;- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = spotify_valid_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],\n           k=i)\n  \n  accuracy.df[i, 2] &lt;- confusionMatrix(knn.pred, spotify_valid_norm.df[ ,c(\"target\")])$overall['Accuracy']\n}\n\naccuracy.df[order(-accuracy.df$accuracy), ]\n\n    k  accuracy\n14 14 0.6183395\n12 12 0.6096654\n13 13 0.6096654\n10 10 0.6084263\n11 11 0.6034696\n5   5 0.6022305\n9   9 0.5997522\n4   4 0.5972739\n6   6 0.5960347\n8   8 0.5947955\n3   3 0.5885998\n7   7 0.5861214\n1   1 0.5774473\n2   2 0.5662949\n\n\nFrom the list above we can see accuracy for different k values between 1 and 14. We can see that the difference in accuracy between values is very small. k=14 has highest accuracy value 0.6183395, also k=5 provides very similar number 0.6022305.\n\nggplot(accuracy.df, aes(x=k, y=accuracy)) + \n  geom_point() +\n  geom_line() +\n  labs(title = \"Scatterplot of k values vs Accuracy\",\n       x = \"k values\",\n       y = \"Accuracy\") +\n  scale_x_continuous(breaks = seq(min(accuracy.df$k), max(accuracy.df$k), by = 3))\n\n\n\n\n\n\n\n\nThe graph clearly illustrates the differences in accuracy across various k-values. k = 10 has about 61% accuracy, similar to k = 12 and k = 13. Since they give the same result, k = 10 is a better choice to reduce noise. Additionally, the previously used k = 7 had one of the lowest accuracy scores at 59%. While k = 14 had the highest accuracy at 62%, k = 10 appears to be a more balanced choice. Selecting 10 nearest neighbors should provide a more reliable classification of my song.\n\nnn_10 &lt;- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = my_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],##what we are classifying: like or dislike\n           k=10)\nprint(nn_10)\n\n[1] 1\nattr(,\"nn.index\")\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]  823  966  277  929  653  984  436  476  136   127\nattr(,\"nn.dist\")\n          [,1]      [,2]      [,3]      [,4]      [,5]     [,6]      [,7]\n[1,] 0.0233238 0.3074891 0.3136877 0.3537573 0.4308537 0.471654 0.4811864\n         [,8]      [,9]     [,10]\n[1,] 0.501667 0.5050047 0.5092467\nLevels: 1\n\n\n\nnn_indexes_10 &lt;- row.names(spotify_train.df)[attr(nn_10, \"nn.index\")]\nspotify_train.df[nn_indexes_10, ] %&gt;% select(song_title, artist, target, acousticness, danceability, speechiness, valence)\n\n\n\n\n\n\n\nI chose k=10 as optimal with moderate accuracy value. The output of model didn’t change, it indicates George will like my song. But for now I got 10 nearest neighbors, and from this new list George disliked 3 songs. All these songs have high value of danceability around 70%, low speechiness and acousticness. All 3 disliked songs as for k=7, have higher value of valence compared to others. Higher valence indicates more positive, cheerful, or euphoric songs. It seems that George might prefer songs with lower valence, which are less positive, more neutral in mood or moodier over cheerful ones. Disliked songs have relatively low acousticness, this suggest that George prefer songs with slightly more acoustic elements. The danceability is quite similar for both groups, which implies this factor is not strong in determining preferences. The disliked songs have relatively low speechiness, and some liked songs have higher speechiness (‘Pacifier’ has 0.1240) indicating George prefer songs with more spoken lyrics or rap.\n\n\nLimitations of model\nI think main limitation here is that we are relying on numerical variables to predict whether someone will like this song or not. There are can be other factors such as good memories or associations with a song which can make them favorite. Also lyrics play main role in connecting with listeners on an emotional level. For instance, I tend to prefer songs with meaningful lyrics, while rap elements often give me an energy boost. Additionally, music preferences can vary based on context—what I listen to at the gym or while walking differs from what I play in the evening when I can’t sleep."
  },
  {
    "objectID": "projects/vancouver_311/vancouver_311.html",
    "href": "projects/vancouver_311/vancouver_311.html",
    "title": "City service requests made in Vancouver, British Columbia from 2022 to the present.",
    "section": "",
    "text": "This project focuses on exploring and visualizing data related to city service requests in Vancouver, British Columbia. The dataset is sourced from Vancouver’s Open Data Portal and contains information about service requests made from 2022 to the present.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nData Exploration\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(leaflet)\n\nvancouver_df &lt;- read.csv('./vancouver_311_requests.csv', sep = ';')\n\n\n\n\n\n\n\nstr() function shows structure of an object. From the result below we can see that, type of our dataset is data.frame which consists of 842862 rows and 13 columns. And also shows the type of each column.\n\nstr(vancouver_df)\n\n'data.frame':   842862 obs. of  13 variables:\n $ Department                    : chr  \"ENG - Parking Enforcement and Operations\" \"FSC - Property Tax\" \"DBL - Services Centre\" \"DBL - Services Centre\" ...\n $ Service.request.type          : chr  \"Abandoned or Uninsured Vehicle Case\" \"Property Tax Request Case\" \"Building and Development Inquiry Case\" \"Building and Development Inquiry Case\" ...\n $ Status                        : chr  \"Close\" \"Close\" \"Close\" \"Close\" ...\n $ Closure.reason                : chr  \"Insufficient info\" \"Alternate Service Required\" \"Service provided\" \"Service provided\" ...\n $ Service.request.open.timestamp: chr  \"2023-10-24T16:38:00-04:00\" \"2023-10-24T16:40:00-04:00\" \"2023-10-24T16:42:00-04:00\" \"2023-10-24T16:45:00-04:00\" ...\n $ Service.request.close.date    : chr  \"2023-10-24\" \"2023-10-25\" \"2023-10-27\" \"2023-10-27\" ...\n $ Last.modified.timestamp       : chr  \"2023-10-24T18:58:39-04:00\" \"2023-10-25T14:05:13-04:00\" \"2023-10-27T17:09:55-04:00\" \"2023-10-27T15:05:34-04:00\" ...\n $ Address                       : chr  \"\" \"\" \"\" \"\" ...\n $ Local.area                    : chr  \"\" \"Fairview\" \"Downtown\" \"Arbutus Ridge\" ...\n $ Channel                       : chr  \"Mobile App\" \"Phone\" \"WEB\" \"WEB\" ...\n $ Latitude                      : num  NA NA NA NA NA ...\n $ Longitude                     : num  NA NA NA NA NA ...\n $ geom                          : chr  \"\" \"\" \"\" \"\" ...\n\n\nIn our dataset 23 unique values of Local.area including empty value.\n\nunique(vancouver_df$Local.area)\n\n [1] \"\"                         \"Fairview\"                \n [3] \"Downtown\"                 \"Arbutus Ridge\"           \n [5] \"Strathcona\"               \"Mount Pleasant\"          \n [7] \"Shaughnessy\"              \"West Point Grey\"         \n [9] \"Kitsilano\"                \"West End\"                \n[11] \"Sunset\"                   \"South Cambie\"            \n[13] \"Marpole\"                  \"Kensington-Cedar Cottage\"\n[15] \"Grandview-Woodland\"       \"Oakridge\"                \n[17] \"Hastings-Sunrise\"         \"Renfrew-Collingwood\"     \n[19] \"Riley Park\"               \"Victoria-Fraserview\"     \n[21] \"Kerrisdale\"               \"Killarney\"               \n[23] \"Dunbar-Southlands\"       \n\n\n\nlength(unique(vancouver_df$Local.area))\n\n[1] 23\n\n\n\nsunset_df &lt;- filter(vancouver_df, Local.area == 'Sunset')\nnrow(sunset_df)\n\n[1] 33036\n\n\nNow I have 33036 records from my area Sunset.\n\nstr(sunset_df)\n\n'data.frame':   33036 obs. of  13 variables:\n $ Department                    : chr  \"DBL - Services Centre\" \"DBL - Animal Services\" \"DBL - Services Centre\" \"DBL - Property Use Inspections\" ...\n $ Service.request.type          : chr  \"Building and Development Inquiry Case\" \"Animal Concern Case\" \"Building and Development Inquiry Case\" \"Private Property Concern Case\" ...\n $ Status                        : chr  \"Close\" \"Close\" \"Close\" \"Close\" ...\n $ Closure.reason                : chr  \"Service provided\" \"Further action has been planned\" \"Service provided\" \"Assigned to inspector\" ...\n $ Service.request.open.timestamp: chr  \"2023-10-24T18:05:19-04:00\" \"2023-10-24T20:05:27-04:00\" \"2023-08-13T14:02:01-04:00\" \"2023-08-13T22:54:01-04:00\" ...\n $ Service.request.close.date    : chr  \"2023-10-25\" \"2023-10-24\" \"2023-08-16\" \"2023-08-16\" ...\n $ Last.modified.timestamp       : chr  \"2023-10-25T14:20:17-04:00\" \"2023-10-24T20:33:26-04:00\" \"2023-08-16T13:46:59-04:00\" \"2023-08-16T15:01:47-04:00\" ...\n $ Address                       : chr  \"\" \"\" \"\" \"\" ...\n $ Local.area                    : chr  \"Sunset\" \"Sunset\" \"Sunset\" \"Sunset\" ...\n $ Channel                       : chr  \"WEB\" \"Phone\" \"WEB\" \"WEB\" ...\n $ Latitude                      : num  NA NA NA NA NA ...\n $ Longitude                     : num  NA NA NA NA NA ...\n $ geom                          : chr  \"\" \"\" \"\" \"\" ...\n\n\nThe following columns have date-related information: Service.request.open.timestamp, Service.request.close.date, Last.modified.timestamp. Now R see them as character not date.\n\nsunset_df$Service.request.open.timestamp &lt;- as.Date(sunset_df$Service.request.open.timestamp)\nsunset_df$Service.request.close.date &lt;- as.Date(sunset_df$Service.request.close.date)\nsunset_df$Last.modified.timestamp &lt;- as.Date(sunset_df$Last.modified.timestamp)\n\nstr(sunset_df)\n\n'data.frame':   33036 obs. of  13 variables:\n $ Department                    : chr  \"DBL - Services Centre\" \"DBL - Animal Services\" \"DBL - Services Centre\" \"DBL - Property Use Inspections\" ...\n $ Service.request.type          : chr  \"Building and Development Inquiry Case\" \"Animal Concern Case\" \"Building and Development Inquiry Case\" \"Private Property Concern Case\" ...\n $ Status                        : chr  \"Close\" \"Close\" \"Close\" \"Close\" ...\n $ Closure.reason                : chr  \"Service provided\" \"Further action has been planned\" \"Service provided\" \"Assigned to inspector\" ...\n $ Service.request.open.timestamp: Date, format: \"2023-10-24\" \"2023-10-24\" ...\n $ Service.request.close.date    : Date, format: \"2023-10-25\" \"2023-10-24\" ...\n $ Last.modified.timestamp       : Date, format: \"2023-10-25\" \"2023-10-24\" ...\n $ Address                       : chr  \"\" \"\" \"\" \"\" ...\n $ Local.area                    : chr  \"Sunset\" \"Sunset\" \"Sunset\" \"Sunset\" ...\n $ Channel                       : chr  \"WEB\" \"Phone\" \"WEB\" \"WEB\" ...\n $ Latitude                      : num  NA NA NA NA NA ...\n $ Longitude                     : num  NA NA NA NA NA ...\n $ geom                          : chr  \"\" \"\" \"\" \"\" ...\n\n\nNow R sees these columns as Date.\n\nsunset_df &lt;- sunset_df %&gt;% mutate(duration = as.numeric(Service.request.close.date - Service.request.open.timestamp, units=\"days\"))\n\nTo extract numeric value of difference between dates, I used as.numeric() function and specified units as days.\n\nsum(is.na(sunset_df))\n\n[1] 41170\n\n\nIn our dataset 41170 total NA values.\n\ncolSums(is.na(sunset_df))\n\n                    Department           Service.request.type \n                             0                              0 \n                        Status                 Closure.reason \n                             0                              0 \nService.request.open.timestamp     Service.request.close.date \n                             0                            523 \n       Last.modified.timestamp                        Address \n                             0                              0 \n                    Local.area                        Channel \n                             0                              0 \n                      Latitude                      Longitude \n                         20062                          20062 \n                          geom                       duration \n                             0                            523 \n\n\nHere is the total # of NA values for each column. The columns Latitude and Longitude each has 20062 missing values, probably Address column is also contain empty values. The service close date didn’t recorded 523 times, which is affected duration column too.\n\nbirthday_reqs &lt;- sunset_df %&gt;% filter(month(Service.request.open.timestamp) == 11 & day(Service.request.open.timestamp) == 24)\nnrow(birthday_reqs)\n\n[1] 64\n\n\nMy birthday is in November 24th, and by using functions from lubridate package, we see that in my birthday occurred 64 requests.\n\nbirthday_reqs_channel &lt;- birthday_reqs %&gt;% group_by(Channel) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count))\nbirthday_reqs_channel\n\n# A tibble: 4 × 2\n  Channel    Count\n  &lt;chr&gt;      &lt;int&gt;\n1 WEB           27\n2 Phone         25\n3 Mobile App    11\n4 Chat           1\n\n\nOn this date the most of requests came from WEB, Phone channels.\n\nbirthday_reqs_types &lt;- birthday_reqs %&gt;% group_by(Service.request.type) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count))\nbirthday_reqs_types\n\n# A tibble: 31 × 2\n   Service.request.type                   Count\n   &lt;chr&gt;                                  &lt;int&gt;\n 1 Missed Green Bin Pickup Case               9\n 2 Green Bin Request Case                     7\n 3 Abandoned Non-Recyclables-Small Case       6\n 4 Business Licence Request Case              5\n 5 Abandoned or Uninsured Vehicle Case        4\n 6 Building and Development Inquiry Case      4\n 7 Abandoned Recyclables Case                 2\n 8 Sewer Drainage and Design Inquiry Case     2\n 9 Street Light Out Case                      2\n10 Street Light Pole Maintenance Case         2\n# ℹ 21 more rows\n\n\nThe top 5 requests inlcude cases related to Green bin (total 16), non-recyclables(total 6), business licence and abandoned vehicle.\n\nsunset_df %&gt;% group_by(Year = year(Service.request.open.timestamp)) %&gt;% summarise(Count = n())\n\n# A tibble: 4 × 2\n   Year Count\n  &lt;dbl&gt; &lt;int&gt;\n1  2022 10840\n2  2023 10192\n3  2024 11018\n4  2025   986\n\n\nThe dataset only contains city service requests going through January of 2025, so the 2025 annual total is not really comparable to the numbers from other years.\n\nsunset_df %&gt;% group_by(Channel) %&gt;% summarise(avg = mean(duration, na.rm = TRUE)) %&gt;% arrange(desc(avg))\n\n# A tibble: 7 × 2\n  Channel        avg\n  &lt;chr&gt;        &lt;dbl&gt;\n1 E-mail       10.5 \n2 Chat         10.1 \n3 Mobile App   10.1 \n4 Phone         9.85\n5 WEB           9.39\n6 Social Media  6.64\n7 Mail          3   \n\n\nFor the channels like E-mail, Chat, Mobile App the average duration to complete service request is more than 10 days. On the other hand, by using Mail channel they spent 3 days on average. Perhaps, since nowadays a lot of requests came from digital/web apps, and the older requests can left at the bottom of the queue which can lead to delays to finish them. Also, different types of requests can be sent through each type of channel, more complicated use E-mail, and small cases use Mail. Or some other factor can affect.\n\nopen_reqs &lt;- sunset_df %&gt;% filter(Status == \"Open\")\n# nrow(open_reqs)\nopen_reqs %&gt;% group_by(Month = month(Service.request.open.timestamp)) %&gt;% summarise(Count = n())\n\n# A tibble: 12 × 2\n   Month Count\n   &lt;dbl&gt; &lt;int&gt;\n 1     1   272\n 2     2    16\n 3     3    14\n 4     4    19\n 5     5    21\n 6     6    23\n 7     7    25\n 8     8    28\n 9     9    10\n10    10    29\n11    11    30\n12    12    36\n\n\n272 out of 523 total open requests are in January only. The dataset was retrieved in January 2025, and most of the yet-unresolved cases in it are recent ones – that’s what explains the January bump\n\nnames(sunset_df)\n\n [1] \"Department\"                     \"Service.request.type\"          \n [3] \"Status\"                         \"Closure.reason\"                \n [5] \"Service.request.open.timestamp\" \"Service.request.close.date\"    \n [7] \"Last.modified.timestamp\"        \"Address\"                       \n [9] \"Local.area\"                     \"Channel\"                       \n[11] \"Latitude\"                       \"Longitude\"                     \n[13] \"geom\"                           \"duration\"                      \n\nsunset_df &lt;- sunset_df %&gt;% rename(Service.request.open.date = Service.request.open.timestamp)\n\nnames(sunset_df)\n\n [1] \"Department\"                 \"Service.request.type\"      \n [3] \"Status\"                     \"Closure.reason\"            \n [5] \"Service.request.open.date\"  \"Service.request.close.date\"\n [7] \"Last.modified.timestamp\"    \"Address\"                   \n [9] \"Local.area\"                 \"Channel\"                   \n[11] \"Latitude\"                   \"Longitude\"                 \n[13] \"geom\"                       \"duration\"                  \n\n\nI renamed the column Service.request.open.timestamp to Service.request.open.date, because now it contains only dates without time.\n\nsunset_df$Address &lt;- NULL\n\nnames(sunset_df)\n\n [1] \"Department\"                 \"Service.request.type\"      \n [3] \"Status\"                     \"Closure.reason\"            \n [5] \"Service.request.open.date\"  \"Service.request.close.date\"\n [7] \"Last.modified.timestamp\"    \"Local.area\"                \n [9] \"Channel\"                    \"Latitude\"                  \n[11] \"Longitude\"                  \"geom\"                      \n[13] \"duration\"                  \n\n\nNow our dataset has 13 columns.\n\n\nData Visualization\n\ndata1 &lt;- sunset_df %&gt;% \n  group_by(DayOfWeek = wday(Service.request.open.date, label = TRUE)) %&gt;% \n  summarise(Count = n()) %&gt;% \n  mutate(DayOfWeek = factor(DayOfWeek, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"), ordered = TRUE)) %&gt;%\n  arrange(DayOfWeek)\n\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nggplot(data1, aes(x=DayOfWeek, y=Count)) +\n  geom_bar(stat = \"identity\", fill=\"thistle\", color=\"black\") +\n  labs(title = \"City Service requests by Day of Week\",\n       x = \"Day of Week\",\n       y = \"Number of Requests\") +\n  theme_clean()\n\n\n\n\n\n\n\n\nThe bar chart displays how many requests were made each day of week.Weekends have only about half the volume of requests, and the middle of the week is when the highest number of requests are occur. Maybe in the weekdays people tend to have more time or prefer to report rather than on weekends.\n\ntop7_req_types &lt;- sunset_df %&gt;% group_by(Service.request.type) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count)) %&gt;% slice_head(n=7)\ntop7_req_types$Service.request.type\n\n[1] \"Missed Green Bin Pickup Case\"         \n[2] \"Building and Development Inquiry Case\"\n[3] \"Missed Garbage Bin Pickup Case\"       \n[4] \"Garbage Bin Request Case\"             \n[5] \"City and Park Trees Maintenance Case\" \n[6] \"Green Bin Request Case\"               \n[7] \"Abandoned Non-Recyclables-Small Case\" \n\nsunset_df_top7 &lt;- sunset_df %&gt;% filter(Service.request.type %in% top7_req_types$Service.request.type)\nnrow(sunset_df_top7)\n\n[1] 16088\n\n\nNow there are only 16088 records with top 7 service request types.\n\ndata2 &lt;- sunset_df_top7 %&gt;% \n  group_by(DayOfWeek = wday(Service.request.open.date, label = TRUE), Department) %&gt;% \n  summarise(Count = n(), .groups = \"drop\") %&gt;% \n  mutate(DayOfWeek = factor(DayOfWeek, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"), ordered = TRUE)) %&gt;%\n  arrange(DayOfWeek)\n\n\nggplot(data2, aes(x=DayOfWeek, y=Count, fill = Department)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +\n  labs(title = \"City Service requests by Day of Week and Department\",\n       x = \"Day of Week\",\n       y = \"Number of Requests\") +\n  theme_foundation()\n\n\n\n\n\n\n\n\nThe main part of service requests from “ENG-Sanitation Services” no matter which day is it. The “PR-Urban Forestry” requests stays the same during the days of week, while “DBL-Services Centre” requests drops significantly on weekends. What if DBL Services Centre offices are closed on weekends, so citizens know this and wait until the week to make the reports? But if Urban Forestry is set up differently, that might explain why it doesn’t show such a big change. Or maybe Urban Forestry requests can be depend on weather conditions, and occur not so often like Sanitation services.\n\ndata3 &lt;- sunset_df_top7 %&gt;% group_by(Month = month(Service.request.open.date, label = TRUE), Service.request.type) %&gt;% \n  summarise(Count = n(), .groups = \"drop\")\n\nggplot(data3, aes(x=Month, y=Count, fill = Service.request.type)) + \n  geom_bar(stat = \"identity\") +\n  labs(title = \"City Service requests by Month and Service Request Type\",\n       x = \"Month\",\n       y = \"Number of Requests\") +\n  theme_calc()\n\n\n\n\n\n\n\n\nJanuary has highest number of requests, followed by November and December. Missed Green Bin Pickup Case occurred in these 3 months more than other months. In January post-holiday waste can put extra pressure to collection system. Also, November and December which are holiday season can lead to increase of waste too. November is often peak time for leaf fall, it can also impact collection system. It’s interesting that Building and Development inquires spike in January, and stays high during the year. January is like a month of new beginnings, when people tend to start new projects. Maybe it can be one of the reasons of large number of requests.\n\nggplot(sunset_df_top7, aes(x=duration)) + \n  geom_histogram(binwidth = 50, fill = \"skyblue\", color = \"black\", boundary = 0) +\n  labs(title = \"Distribution of Service Request Duration\",\n       x = \"Duration (in days)\",\n       y = \"Number of Requests\"\n       ) +\n  theme_bw()\n\nWarning: Removed 236 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThe distribution is left-skewed. It’s interesting that for some requests to be closed took around 800 days, maybe it’s result of some technical issues, or these cases delayed because of legal issues. And in most cases to complete request took between 0 to 50 days. By setting binwidth = 50, I say that each bin represents 50 days.\n\nggplot(sunset_df_top7, aes(x=duration)) + \n  geom_histogram(binwidth = 30, fill = \"skyblue\", color = \"black\", boundary = 0) +\n  facet_wrap(~Service.request.type) +\n  labs(title = \"Distribution of Service Request Duration\",\n       x = \"Duration (in days)\",\n       y = \"Number of Requests\"\n       ) +\n  theme_bw()\n\nWarning: Removed 236 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nMost of the city requests have a same pattern, which can represent that city requests processes in a quick turnaround time. In some cases it can take longer than 30 days, maybe because of legal issues which can occur for the City and Trees Maintenance case (permits to make changes from multiple departments)\n\nggplot(sunset_df_top7, aes(x=Channel, fill = Service.request.type)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Distribution of Service Request Type by Channel\",\n       y = \"Proportion of requests\",\n       fill = \"Service Request Type\")\n\n\n\n\n\n\n\n\nThis plot shows the distribution of service request types across channels. Interesting, that Abandoned Non-Recyclables mostly reported via mobile app, while City and Park Maintenance dominate in Social Media and E-mail. Also, Building and Development Inquiry mostly reported via WEB. Different requests seems like have preferred channels. For instance, garbage and green bin requests prefer chat or phone, that do not require any additional resources like images. For the City and Park Maintenance social media is popular, maybe because people prefer post about their awareness of city to public discussion.\nCreated map with Esri World Imagery tiles, and added popup text which will display department name for each service request.\n\nsunset_df_map &lt;- sunset_df %&gt;% filter(!is.na(Latitude) & !is.na(Longitude))\n\nleaflet(data = sunset_df_map) %&gt;%\n  addTiles() %&gt;%\n  addCircles(~Longitude, ~Latitude)\n\n\n\n\n\n\nleaflet(data = sunset_df_map) %&gt;%\n  addProviderTiles(\"Esri.WorldImagery\") %&gt;% \n  addCircles(~Longitude, ~Latitude, radius = 5, color = \"gold\", popup = ~Department)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dinara Zhorabek",
    "section": "",
    "text": "Dinara Zhorabek\n\n\nBusiness & Data Analyst | Full Stack Software Engineer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHi, I’m Dinara.\nAn aspiring business & data analyst and middle software engineer, avid traveler, passionate about continuous learning, problem-solving, and empowering others through data-driven solutions.\nI am pursuing a Master’s degree in Applied Business Analytics at Boston University and received a B.S. in Information and Communication Technology from Kazakh-British Technical University.\nI’m skilled in Python, R, SQL, Tableau and Power BI, and I am deeply committed to using data and technology to solve business challenges.\n\n\n\n\n\n\n\nPortfolio\n\n\nA glimpse of the projects I’ve been working on\n\n\n\n\n\n\n\n\n\n\nWords, Locations, and Prices\n\n\n\nAirbnb Analysis in Copenhagen.\n\n\n\n\n\n\n\n\n\n\nMET ABA Hackathon 2025\n\n\n\nVortex Sentiment Adaptive Volatility (VSAV) Strategy.\n\n\n\n \n\n\n\n\n\n\nYou Like This Song…But Will George Like It?\n\n\n\nUser Behavior Classification with Spotify Streaming Data.\n\n\n\n\n\n\n\n\n\n\nClustering Pokémon\n\n\n\nA Hierarchical Approach to Character Grouping.\n\n\n\n \n\n\n\n\n\n\nSay My Name\n\n\n\nUnpacking Emotion and Language in Breaking Bad Using R.\n\n\n\n \n\n\n\n\n\n\nComplaint Classifier\n\n\n\nUtilized the Naïve Bayes algorithm to classify and predict consumer disputes.\n\n\n\n \n\n\n\n\n\n\nMarket Basket Insights\n\n\n\nAssociation rule mining.\n\n\n\n \n\n\n\n\n\n\nSkating through Data\n\n\n\nForecasting hockey player salaries by utilizing statistical analysis and machine learning methods.\n\n\n\n \n\n\n\n\n\n\nConsole to Category\n\n\n\nPredicting Video Game Sales Using Classification Trees.\n\n\n\n \n\n\n\n\n\n\nVoices of the City\n\n\n\nAnalyzing Vancouver’s Service Request Patterns."
  },
  {
    "objectID": "projects/text_mining/breaking bad.html",
    "href": "projects/text_mining/breaking bad.html",
    "title": "Say my name: Unpacking Emotion and Language in Breaking Bad Using R",
    "section": "",
    "text": "Applied natural language processing techniques to a single episode of Breaking Bad using R. Conducted frequency analysis to identify top characters by line count, explored most common words and bigrams, and visualized key terms with a wordcloud. Performed sentiment analysis using Bing and AFINN lexicons to assess the emotional tone of the episode. Tools used include dplyr, tidytext, ggplot2, and wordcloud.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nText Mining\n\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\nlibrary(textdata)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\n\n\nbb &lt;- read.csv(\"BB_data.csv\")\nstr(bb)\n\n'data.frame':   5641 obs. of  5 variables:\n $ actor  : chr  \"Walter\" \"Skyler\" \"Walter\" \"Skyler\" ...\n $ text   : chr  \"My name is Walter Hartwell White. I live at 308 Negra Arroyo Lane Albuquerque, New Mexico, 87104. To all law en\"| __truncated__ \"Happy Birthday.\" \"Look at that.\" \"That is veggie bacon. Believe it or not. Zero cholesterol. You won't even taste the difference. What time do yo\"| __truncated__ ...\n $ season : int  1 1 1 1 1 1 1 1 1 1 ...\n $ episode: int  1 1 1 1 1 1 1 1 1 1 ...\n $ title  : chr  \"The Pilot\" \"The Pilot\" \"The Pilot\" \"The Pilot\" ...\n\nmy_data &lt;- bb %&gt;% filter(season==1 & episode==5)\nhead(my_data)\n\n    actor\n1   Jesse\n2 Manager\n3   Jesse\n4 Manager\n5   Jesse\n6 Manager\n                                                                                                                                                                                                                                                                                                            text\n1                                                                                                                                                               Here's my resume. I mean, technically it says \"curriculum vitae,\" which I think is more professional, but, you know, the same thing pretty much.\n2                                                                                                                                                                                                                                     Great. But this position truly is a no-experience-necessary kind of thing.\n3                                                                                                                                                                                           Well, and it doesn't really say it here, but I have a solid background in sales. I'm pretty much a self-starter, so.\n4                                                                                                                                                                                                                                                                                         That's excellent, but…\n5                                                                                                                                                                                                                                             I really feel I could be a major, major asset to your sales force.\n6 I'm thinking there's a little misunderstanding. This isn't actually a sales job. I mean, I'd be happy to consider you for a sales position, but our agents need to be licensed, have at least two years on-the-job experience, and usually a college degree. What you would be doing is more like advertising.\n  season episode       title\n1      1       5 Grey Matter\n2      1       5 Grey Matter\n3      1       5 Grey Matter\n4      1       5 Grey Matter\n5      1       5 Grey Matter\n6      1       5 Grey Matter\n\n\n\ntop_10 &lt;- my_data %&gt;% \n  count(actor, sort = TRUE) %&gt;% \n  slice(1:10)\n\nmy_data &lt;- my_data %&gt;% filter(actor %in% top_10$actor) %&gt;%\n  mutate(actor = factor(actor, levels = top_10$actor))\n\n\nggplot(my_data) + geom_bar(aes(y = actor))\n\n\n\n\n\n\n\n\nFrom the barplot above, we can see that Walter and Skyler have similar number of lines in Episode 5 Season 1, suggesting that they are the main characters here. They are followed by Jesse, Badger, and Elliot - most likely supporting characters who have shorter lines but appears alongside with main ones. Other remaining characters each have less than 20 lines, so they probably only showed up briefly or had minor roles in this episode.\n\nmy_data_txt &lt;- my_data %&gt;% select(text)\n\ntidy_const &lt;- my_data_txt %&gt;% unnest_tokens(word, text)\ntidy_const %&gt;% count(word, sort=TRUE) %&gt;% head(10)\n\n   word   n\n1   you 154\n2     i 116\n3    to  92\n4   the  86\n5    it  60\n6     a  58\n7  what  52\n8   and  43\n9  this  41\n10 that  39\n\n\nAfter extracting 10 most frequently used words, we mostly ended up with common pronouns like “you”, “I”, “and”, “these”, “that”. These are known as stop words—common terms that appear frequently across most texts. Since they don’t provide any insight into the context of the episode, they are not useful at all.\n\n# stop_words\ntidy_const &lt;- my_data_txt %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_const %&gt;% count(word, sort=TRUE) %&gt;% slice(1:10)\n\n        word  n\n1       walt 21\n2    elliott 11\n3       yeah 11\n4    talking 10\n5        hey  9\n6     skyler  8\n7         yo  8\n8      gonna  7\n9  treatment  7\n10       guy  6\n\n\nAfter removing stopwords, we extracted a different set of words. From the list we can see names of characters, like Walt, Elliiot, Skyler, which are can be main characters like we mentioned previously. Also, it contains less meaningful words like “yeah”, “yo”, “hey”.\n\ntidy_const2 &lt;- my_data_txt %&gt;% unnest_tokens(output = bigram, input=text, token=\"ngrams\", n=2)\ntidy_const2 %&gt;% count(bigram, sort=TRUE) %&gt;% slice(1:10)\n\n      bigram  n\n1   you know 19\n2       &lt;NA&gt; 16\n3    are you 12\n4     i mean 12\n5      to do 12\n6  thank you 11\n7    this is 11\n8    i don't 10\n9  all right  7\n10     and i  7\n\n\nBigrams is combination of two (bi) consecutive words, while unigrams are single words. Bigrams help to get more meaningful insights into context compared to unigrams. For example, the word “pillow” on its own just refers to an object, but the bigram “talking pillow” suggests something very different and more specific. By looking at pairs of words, we can better understand the meaning and flow of the text.\n\nbigrams_sep &lt;- tidy_const2 %&gt;% separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\nbigrams_filtered &lt;- bigrams_sep %&gt;% filter(!word1 %in% stop_words$word) %&gt;% \n  filter(!word2 %in% stop_words$word) %&gt;% \n  filter(!is.na(word1) | !is.na(word2)) %&gt;% \n  count(word1, word2, sort = TRUE)\n# bigrams_filtered\nbigrams_united &lt;- bigrams_filtered %&gt;% unite(col = bigram, word1, word2, sep=\" \")\nbigrams_united %&gt;% slice(1:10)\n\n                 bigram n\n1           snot trough 3\n2        talking pillow 3\n3               fat guy 2\n4           gray matter 2\n5        husband's life 2\n6             shit hand 2\n7             walter jr 2\n8              40 pills 1\n9             9th bases 1\n10 absolutely miserable 1\n\n\nAfter extracting bigrams, the initial results included common phrases like “are you” and “this is,” as well as some NA values. These aren’t very meaningful on their own. However, after applying filters we ended up with a more useful and insightful list of bigrams.\nBy reviewing the list of common words, we get a sense of the episode’s context. Walt, Elliot, and Skyler seem to be the main characters here. From the bigrams, “40 pills” could be related to drugs, possibly something illegal. The phrase “absolutely miserable” might indicate someone’s struggle with addiction, potentially to drugs. Additionally, the term “gray matter” is tied to chemicals, which gives us the impression that this episode could be about drug production or something related to the illegal drug trade.\n\nmy_data_txt %&gt;% \n  unnest_tokens(word, text) %&gt;% \n  inner_join(get_sentiments(\"bing\")) %&gt;% \n  count(word, sentiment, sort = TRUE) %&gt;% \n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %&gt;% \n  comparison.cloud(colors = c(\"red\", \"blue\"), max.words = 50, title.size = 1)\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\n\n\n\nI generated wordcloud using the bing sentiment analysis, which categorizes words into positive and negative groups. From the word cloud, we can see that the episode contains a number of negative words, but also some positive ones like “good,” “like,” and “beautiful.” But also we need to consider that these words are analyzed in isolation. While they are labeled as positive or negative by default, their actual meaning can change depending on the context in which they’re used.\n\nbing &lt;- my_data_txt %&gt;% unnest_tokens(word, text) %&gt;% \n  inner_join(get_sentiments(\"bing\")) %&gt;% \n  count(word, sentiment, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\nbing %&gt;% slice(1:10)\n\n     word sentiment  n\n1    like  positive 20\n2   right  positive 19\n3    well  positive 15\n4    good  positive 13\n5   thank  positive 11\n6    work  positive  7\n7    best  positive  5\n8  cancer  negative  5\n9    hell  negative  5\n10   love  positive  5\n\n\nFrom the list above, we can see top 10 words that made the biggest sentiment contributions. Among these, 2 words are negative (“cancer”, “hell”) while the rest are positive. By looking at the result, the overall tone of the episode can be more positive. However, the appeareance of word “cancer” can mean that probably one of the characters have illness, which introduces a more emotionally heavy or serious atmosphere. Similarly, the word “hell” often reflects anger, frustration, or a chaotic situation. These two words hint at deeper emotional layers while other words gives more positive emotions.\n\n# afinn &lt;- my_data_txt %&gt;% unnest_tokens(word, text) %&gt;% \n#   inner_join(get_sentiments(\"afinn\")) %&gt;% \n#   count(word, value, sort = TRUE) %&gt;% \n#   mutate(contribution = n*value)\n\n# afinn %&gt;% arrange(contribution) %&gt;% slice(1:3)\n\n# afinn %&gt;% arrange(desc(contribution)) %&gt;% slice(1:3)\n\nThree worst words are “no”, “hell”, “shit”.\nThree best words are “like”, “good”, “thank”.\n\n# sum(afinn$contribution)\n\nSum of values is positive 146.\nThe sum of sentiment values can help identify the overall emotional tone of an episode. In our case, since the total score is positive, it suggests the episode is generally positive. However, by analyzing in this way we are missing true meaning of words. For example, the word “hell” might be used in a positive or humorous way, while words considered positive—like “great”—could be used sarcastically, meaning the opposite. Additionally, simply counting the frequency of words overlooks deeper meaning. The tone and impact of a word can depend on who said it, how it was said, and in what scene. Without that context, the analysis can be incomplete or even misleading."
  },
  {
    "objectID": "projects/pokemon/pokemon.html",
    "href": "projects/pokemon/pokemon.html",
    "title": "Clustering Pokémon: A Hierarchical Approach to Character Grouping",
    "section": "",
    "text": "Performed hierarchical clustering on a sample of 20 Pokémon using R. Selected five numeric attributes (e.g., HP, Speed, Attack) and scaled them to build a dendrogram and identify character clusters. Explored both equal-weight and custom-weight variable clustering to observe differences in cluster composition. Used cutree() to assign clusters, visualized relationships with dendrograms, scatterplots, and boxplots, and analyzed cluster traits using summary statistics. Tools used include dplyr, ggplot2, and stats.\nThis work is part of an assignment for the AD699 Data Mining course."
  },
  {
    "objectID": "projects/text_mining/breaking_bad.html",
    "href": "projects/text_mining/breaking_bad.html",
    "title": "Say my name: Unpacking Emotion and Language in Breaking Bad Using R",
    "section": "",
    "text": "Applied natural language processing techniques to a single episode of Breaking Bad using R. Conducted frequency analysis to identify top characters by line count, explored most common words and bigrams, and visualized key terms with a wordcloud. Performed sentiment analysis using Bing and AFINN lexicons to assess the emotional tone of the episode. Tools used include dplyr, tidytext, ggplot2, and wordcloud.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nText Mining\n\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(textdata)\nlibrary(wordcloud)\nbb &lt;- read.csv(\"BB_data.csv\")\n\n\nstr(bb)\n\n'data.frame':   5641 obs. of  5 variables:\n $ actor  : chr  \"Walter\" \"Skyler\" \"Walter\" \"Skyler\" ...\n $ text   : chr  \"My name is Walter Hartwell White. I live at 308 Negra Arroyo Lane Albuquerque, New Mexico, 87104. To all law en\"| __truncated__ \"Happy Birthday.\" \"Look at that.\" \"That is veggie bacon. Believe it or not. Zero cholesterol. You won't even taste the difference. What time do yo\"| __truncated__ ...\n $ season : int  1 1 1 1 1 1 1 1 1 1 ...\n $ episode: int  1 1 1 1 1 1 1 1 1 1 ...\n $ title  : chr  \"The Pilot\" \"The Pilot\" \"The Pilot\" \"The Pilot\" ...\n\n\n\nmy_data &lt;- bb %&gt;% filter(season==1 & episode==5)\nhead(my_data)\n\n\n\n\n\n\n\n\ntop_10 &lt;- my_data %&gt;% \n  count(actor, sort = TRUE) %&gt;% \n  slice(1:10)\n\nmy_data &lt;- my_data %&gt;% filter(actor %in% top_10$actor) %&gt;%\n  mutate(actor = factor(actor, levels = top_10$actor))\n\n\nggplot(my_data) + geom_bar(aes(y = actor))\n\n\n\n\n\n\n\n\nFrom the barplot above, we can see that Walter and Skyler have similar number of lines in Episode 5 Season 1, suggesting that they are the main characters here. They are followed by Jesse, Badger, and Elliot - most likely supporting characters who have shorter lines but appears alongside with main ones. Other remaining characters each have less than 20 lines, so they probably only showed up briefly or had minor roles in this episode.\n\nmy_data_txt &lt;- my_data %&gt;% select(text)\n\ntidy_const &lt;- my_data_txt %&gt;% unnest_tokens(word, text)\ntidy_const %&gt;% count(word, sort=TRUE) %&gt;% head(10)\n\n   word   n\n1   you 154\n2     i 116\n3    to  92\n4   the  86\n5    it  60\n6     a  58\n7  what  52\n8   and  43\n9  this  41\n10 that  39\n\n\nAfter extracting 10 most frequently used words, we mostly ended up with common pronouns like “you”, “I”, “and”, “these”, “that”. These are known as stop words—common terms that appear frequently across most texts. Since they don’t provide any insight into the context of the episode, they are not useful at all.\n\n# stop_words\ntidy_const &lt;- my_data_txt %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words)\ntidy_const %&gt;% count(word, sort=TRUE) %&gt;% slice(1:10)\n\n        word  n\n1       walt 21\n2    elliott 11\n3       yeah 11\n4    talking 10\n5        hey  9\n6     skyler  8\n7         yo  8\n8      gonna  7\n9  treatment  7\n10       guy  6\n\n\nAfter removing stopwords, we extracted a different set of words. From the list we can see names of characters, like Walt, Elliiot, Skyler, which are can be main characters like we mentioned previously. Also, it contains less meaningful words like “yeah”, “yo”, “hey”.\n\ntidy_const2 &lt;- my_data_txt %&gt;% unnest_tokens(output = bigram, input=text, token=\"ngrams\", n=2)\ntidy_const2 %&gt;% count(bigram, sort=TRUE) %&gt;% slice(1:10)\n\n      bigram  n\n1   you know 19\n2       &lt;NA&gt; 16\n3    are you 12\n4     i mean 12\n5      to do 12\n6  thank you 11\n7    this is 11\n8    i don't 10\n9  all right  7\n10     and i  7\n\n\nBigrams is combination of two (bi) consecutive words, while unigrams are single words. Bigrams help to get more meaningful insights into context compared to unigrams. For example, the word “pillow” on its own just refers to an object, but the bigram “talking pillow” suggests something very different and more specific. By looking at pairs of words, we can better understand the meaning and flow of the text.\n\nbigrams_sep &lt;- tidy_const2 %&gt;% separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\nbigrams_filtered &lt;- bigrams_sep %&gt;% filter(!word1 %in% stop_words$word) %&gt;% \n  filter(!word2 %in% stop_words$word) %&gt;% \n  filter(!is.na(word1) | !is.na(word2)) %&gt;% \n  count(word1, word2, sort = TRUE)\n# bigrams_filtered\nbigrams_united &lt;- bigrams_filtered %&gt;% unite(col = bigram, word1, word2, sep=\" \")\nbigrams_united %&gt;% slice(1:10)\n\n                 bigram n\n1           snot trough 3\n2        talking pillow 3\n3               fat guy 2\n4           gray matter 2\n5        husband's life 2\n6             shit hand 2\n7             walter jr 2\n8              40 pills 1\n9             9th bases 1\n10 absolutely miserable 1\n\n\nAfter extracting bigrams, the initial results included common phrases like “are you” and “this is,” as well as some NA values. These aren’t very meaningful on their own. However, after applying filters we ended up with a more useful and insightful list of bigrams.\nBy reviewing the list of common words, we get a sense of the episode’s context. Walt, Elliot, and Skyler seem to be the main characters here. From the bigrams, “40 pills” could be related to drugs, possibly something illegal. The phrase “absolutely miserable” might indicate someone’s struggle with addiction, potentially to drugs. Additionally, the term “gray matter” is tied to chemicals, which gives us the impression that this episode could be about drug production or something related to the illegal drug trade.\n\nmy_data_txt %&gt;% \n  unnest_tokens(word, text) %&gt;% \n  inner_join(get_sentiments(\"bing\")) %&gt;% \n  count(word, sentiment, sort = TRUE) %&gt;% \n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %&gt;% \n  comparison.cloud(colors = c(\"red\", \"blue\"), max.words = 50, title.size = 1)\n\n\n\n\n\n\n\n\nI generated wordcloud using the bing sentiment analysis, which categorizes words into positive and negative groups. From the word cloud, we can see that the episode contains a number of negative words, but also some positive ones like “good,” “like,” and “beautiful.” But also we need to consider that these words are analyzed in isolation. While they are labeled as positive or negative by default, their actual meaning can change depending on the context in which they’re used.\n\nbing &lt;- my_data_txt %&gt;% unnest_tokens(word, text) %&gt;% \n  inner_join(get_sentiments(\"bing\")) %&gt;% \n  count(word, sentiment, sort = TRUE)\n\nbing %&gt;% slice(1:10)\n\n     word sentiment  n\n1    like  positive 20\n2   right  positive 19\n3    well  positive 15\n4    good  positive 13\n5   thank  positive 11\n6    work  positive  7\n7    best  positive  5\n8  cancer  negative  5\n9    hell  negative  5\n10   love  positive  5\n\n\nFrom the list above, we can see top 10 words that made the biggest sentiment contributions. Among these, 2 words are negative (“cancer”, “hell”) while the rest are positive. By looking at the result, the overall tone of the episode can be more positive. However, the appeareance of word “cancer” can mean that probably one of the characters have illness, which introduces a more emotionally heavy or serious atmosphere. Similarly, the word “hell” often reflects anger, frustration, or a chaotic situation. These two words hint at deeper emotional layers while other words gives more positive emotions.\n\n# afinn &lt;- my_data_txt %&gt;% unnest_tokens(word, text) %&gt;% \n#   inner_join(get_sentiments(\"afinn\")) %&gt;% \n#   count(word, value, sort = TRUE) %&gt;% \n#   mutate(contribution = n*value)\n\n# afinn %&gt;% arrange(contribution) %&gt;% slice(1:3)\n\n# afinn %&gt;% arrange(desc(contribution)) %&gt;% slice(1:3)\n\nThree worst words are “no”, “hell”, “shit”.\nThree best words are “like”, “good”, “thank”.\n\n# sum(afinn$contribution)\n\nSum of values is positive 146.\nThe sum of sentiment values can help identify the overall emotional tone of an episode. In our case, since the total score is positive, it suggests the episode is generally positive. However, by analyzing in this way we are missing true meaning of words. For example, the word “hell” might be used in a positive or humorous way, while words considered positive—like “great”—could be used sarcastically, meaning the opposite. Additionally, simply counting the frequency of words overlooks deeper meaning. The tone and impact of a word can depend on who said it, how it was said, and in what scene. Without that context, the analysis can be incomplete or even misleading."
  },
  {
    "objectID": "projects/airbnb/airbnb-copenhagen.html",
    "href": "projects/airbnb/airbnb-copenhagen.html",
    "title": "airbnb-copenhagen",
    "section": "",
    "text": "library(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(leaflet)\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(tidytext)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\ncopenhagen &lt;- read_csv('copenhagen_listings.csv')\n\nRows: 21707 Columns: 75\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (24): listing_url, source, name, description, neighborhood_overview, pi...\ndbl  (38): id, scrape_id, host_id, host_listings_count, host_total_listings_...\nlgl   (8): host_is_superhost, host_has_profile_pic, host_identity_verified, ...\ndate  (5): last_scraped, host_since, calendar_last_scraped, first_review, la...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe dataset contains information about Airbnb listings in Copenhagen, Denmark. Upon importing the data into R, we observed a significant number of missing values. To identify where these values occur, we used the colSums(is.na()) function, which enabled us to pinpoint missing data across columns and assess how best to handle them. By sorting these results in descending order, we prioritized columns with the most missing entries. We felt that values that are central to our exploratory and predictive analysis should have missing values removed, as imputing would potentially compromise these critical values. We chose to remove any rows that had NA values in price, review_scores_rating, and bedrooms. After this cleaning step, we retained 10,776 rows of information, plenty of data to run our analysis and a worthy compromise to ensure accuracy. For other columns with missing values, we applied targeted strategies. For example, we imputed missing values in the binary host_is_superhost column by assuming a value of FALSE for hosts without a recorded status. For the numeric variables bathrooms and beds, we replaced missing values with the median of each respective column. As for neighborhood_overview, we will simply exclude missing entries from any future analysis involving that variable. Remaining columns with missing values were deemed either nonessential for our analysis (e.g., license, host_neighbourhood, etc.), or had corresponding cleaned versions (e.g., neighbourhood and neighbourhood_cleansed). The has_availability field contained missing values. To impute these, we leveraged related variables: availability_30, availability_60, availability_90, and availability_365. If any of these showed more than 0 available days, we set has_availability to True. Furthermore, we found 2,630 records with no reviews, resulting in missing values for all review-related columns. We imputed review_scores_rating using the median value across the dataset. Overall, our cleaning approach allowed us to preserve a large portion of the dataset, while ensuring the data was complete for modeling purposes.\n\n\n\ncolSums(is.na(copenhagen)) %&gt;%\n  sort(decreasing = TRUE)\n\n                neighbourhood_group_cleansed \n                                       21707 \n                            calendar_updated \n                                       21707 \n                                     license \n                                       21707 \n                          host_neighbourhood \n                                       16461 \n                       neighborhood_overview \n                                       13154 \n                               neighbourhood \n                                       13154 \n                                  host_about \n                                       12660 \n                                       price \n                                        9236 \n                                   bathrooms \n                                        9227 \n                                        beds \n                                        9223 \n                               host_location \n                                        3478 \n                      review_scores_location \n                                        2633 \n                      review_scores_accuracy \n                                        2632 \n                   review_scores_cleanliness \n                                        2632 \n                       review_scores_checkin \n                                        2632 \n                 review_scores_communication \n                                        2632 \n                         review_scores_value \n                                        2632 \n                                first_review \n                                        2630 \n                                 last_review \n                                        2630 \n                        review_scores_rating \n                                        2630 \n                           reviews_per_month \n                                        2630 \n                                    bedrooms \n                                         712 \n                                 description \n                                         599 \n                            has_availability \n                                         430 \n                           host_is_superhost \n                                         195 \n                              bathrooms_text \n                                          10 \n                                   host_name \n                                           2 \n                                  host_since \n                                           2 \n                          host_response_time \n                                           2 \n                          host_response_rate \n                                           2 \n                        host_acceptance_rate \n                                           2 \n                          host_thumbnail_url \n                                           2 \n                            host_picture_url \n                                           2 \n                         host_listings_count \n                                           2 \n                   host_total_listings_count \n                                           2 \n                        host_has_profile_pic \n                                           2 \n                      host_identity_verified \n                                           2 \n                                          id \n                                           0 \n                                 listing_url \n                                           0 \n                                   scrape_id \n                                           0 \n                                last_scraped \n                                           0 \n                                      source \n                                           0 \n                                        name \n                                           0 \n                                 picture_url \n                                           0 \n                                     host_id \n                                           0 \n                                    host_url \n                                           0 \n                          host_verifications \n                                           0 \n                      neighbourhood_cleansed \n                                           0 \n                                    latitude \n                                           0 \n                                   longitude \n                                           0 \n                               property_type \n                                           0 \n                                   room_type \n                                           0 \n                                accommodates \n                                           0 \n                                   amenities \n                                           0 \n                              minimum_nights \n                                           0 \n                              maximum_nights \n                                           0 \n                      minimum_minimum_nights \n                                           0 \n                      maximum_minimum_nights \n                                           0 \n                      minimum_maximum_nights \n                                           0 \n                      maximum_maximum_nights \n                                           0 \n                      minimum_nights_avg_ntm \n                                           0 \n                      maximum_nights_avg_ntm \n                                           0 \n                             availability_30 \n                                           0 \n                             availability_60 \n                                           0 \n                             availability_90 \n                                           0 \n                            availability_365 \n                                           0 \n                       calendar_last_scraped \n                                           0 \n                           number_of_reviews \n                                           0 \n                       number_of_reviews_ltm \n                                           0 \n                      number_of_reviews_l30d \n                                           0 \n                            instant_bookable \n                                           0 \n              calculated_host_listings_count \n                                           0 \n calculated_host_listings_count_entire_homes \n                                           0 \ncalculated_host_listings_count_private_rooms \n                                           0 \n calculated_host_listings_count_shared_rooms \n                                           0 \n\ncopenhagen &lt;- copenhagen %&gt;%\n  filter(!is.na(price) & !is.na(review_scores_rating) & !is.na(bedrooms))\n\ncopenhagen &lt;- copenhagen %&gt;%\n  mutate(\n    host_is_superhost = ifelse(is.na(host_is_superhost), \"FALSE\", host_is_superhost)\n    )\n\ncopenhagen &lt;- copenhagen %&gt;%\n  group_by(room_type) %&gt;%\n  mutate(\n    bathrooms = ifelse(is.na(bathrooms), median(bathrooms, na.rm = TRUE), bathrooms),\n    bedrooms = ifelse(is.na(bedrooms), median(bedrooms, na.rm = TRUE), bedrooms),\n    beds = ifelse(is.na(beds), median(beds, na.rm = TRUE), beds),\n    price = ifelse(is.na(price), median(price, na.rm = TRUE), price)\n  ) %&gt;%\n  ungroup()\n\n# if property available in N days =&gt; has_availability is True\ncopenhagen$has_availability[is.na(copenhagen$has_availability)] &lt;- ifelse(\n  is.na(copenhagen$has_availability) &\n    rowSums(copenhagen[is.na(copenhagen$has_availability), c(\"availability_30\", \"availability_60\", \"availability_90\", \"availability_365\")] &gt; 0, na.rm = TRUE) &gt; 0, TRUE, FALSE)\n\nWarning in copenhagen$has_availability[is.na(copenhagen$has_availability)] &lt;-\nifelse(is.na(copenhagen$has_availability) & : number of items to replace is not\na multiple of replacement length\n\n# 2630 records with no_review\ncopenhagen$reviews_per_month[copenhagen$number_of_reviews == 0] &lt;- 0\n\n# set Median value for NAs in review related columns\ncopenhagen$review_scores_rating &lt;- replace_na(copenhagen$review_scores_rating, \n                                              median(copenhagen$review_scores_rating, na.rm = TRUE))\ncopenhagen$review_scores_location &lt;- replace_na(copenhagen$review_scores_location, \n                                                median(copenhagen$review_scores_location, na.rm = TRUE))\ncopenhagen$review_scores_value &lt;- replace_na(copenhagen$review_scores_value, \n                                             median(copenhagen$review_scores_value, na.rm = TRUE))\n\n\n\n\n\ntable(copenhagen$neighbourhood_cleansed)\n\n\n                Amager st               Amager Vest                Bispebjerg \n                      839                       989                       478 \n             Brnshj-Husum             Frederiksberg                  Indre By \n                      167                      1130                      1818 \n                  Nrrebro                   sterbro                     Valby \n                     1764                      1109                       381 \n                   Vanlse Vesterbro-Kongens Enghave \n                      242                      1859 \n\n\nWe first used table to see the different neighborhoods and how many listings were in each one. There are 11 neighborhoods and we chose to keep them all as these are the 10 municipal districts of Copenhagen plus Frederiksberg, which is its center. It felt like these would all be distinct and not too overwhelming to look at. We also knew we were going to want to look at price so made sure to convert it to a numeric and remove the dollar signs and commas.\n\ncopenhagen$price &lt;- as.numeric(gsub(\"[$,]\", \"\", copenhagen$price))\navgpricebyneighborhood &lt;- copenhagen %&gt;% group_by(neighbourhood_cleansed) %&gt;% \n  summarise(mean(price))\navgpricebyneighborhood\n\n# A tibble: 11 × 2\n   neighbourhood_cleansed    `mean(price)`\n   &lt;chr&gt;                             &lt;dbl&gt;\n 1 Amager Vest                       1332.\n 2 Amager st                         1220.\n 3 Bispebjerg                         937.\n 4 Brnshj-Husum                       959.\n 5 Frederiksberg                     1329.\n 6 Indre By                          1887.\n 7 Nrrebro                           1206.\n 8 Valby                             1021.\n 9 Vanlse                            1041.\n10 Vesterbro-Kongens Enghave         1365.\n11 sterbro                           1421.\n\n\nThe first statistic we chose to look at was the average price per night for a listing in each neighborhood. We see here that the average price for listings in Indre By are basically double those in Bispebjerg and Brnshj-Husum. There’s quite a big range across the neighborhoods, so our takeaway is that different neighborhoods are more expensive—we don’t know why yet though, it could be due to demand or popularity or any other factor.\n\navgaccomodatesbyneighborhood &lt;- copenhagen %&gt;% group_by(neighbourhood_cleansed) %&gt;% \n  summarise(mean(accommodates))\navgaccomodatesbyneighborhood\n\n# A tibble: 11 × 2\n   neighbourhood_cleansed    `mean(accommodates)`\n   &lt;chr&gt;                                    &lt;dbl&gt;\n 1 Amager Vest                               3.58\n 2 Amager st                                 3.48\n 3 Bispebjerg                                3.05\n 4 Brnshj-Husum                              3.96\n 5 Frederiksberg                             3.32\n 6 Indre By                                  3.76\n 7 Nrrebro                                   3.01\n 8 Valby                                     3.43\n 9 Vanlse                                    3.83\n10 Vesterbro-Kongens Enghave                 3.20\n11 sterbro                                   3.29\n\n\nThe next stat we chose to look at was the average number of people accommodated in each listing, or basically how many people can fit, by neighborhood. There is much less variety here with Nrrebro having a mean of approximately 3 and Brnshj-Husum having a mean of almost 4. Part of the reason for the smaller range is likely due to the fact that these are all still in the same city—it’s not likely for some neighborhoods to have properties accommodating 20 while some only accommodate 1.\n\npropertytypebyneighborhood &lt;- copenhagen %&gt;% count(neighbourhood_cleansed, property_type, \n                                                   sort = TRUE)\npropertytypebyneighborhood\n\n# A tibble: 197 × 3\n   neighbourhood_cleansed    property_type          n\n   &lt;chr&gt;                     &lt;chr&gt;              &lt;int&gt;\n 1 Indre By                  Entire rental unit  1108\n 2 Nrrebro                   Entire rental unit  1107\n 3 Vesterbro-Kongens Enghave Entire rental unit  1056\n 4 sterbro                   Entire rental unit   691\n 5 Frederiksberg             Entire rental unit   655\n 6 Vesterbro-Kongens Enghave Entire condo         573\n 7 Nrrebro                   Entire condo         517\n 8 Amager Vest               Entire rental unit   457\n 9 Amager st                 Entire rental unit   429\n10 Indre By                  Entire condo         411\n# ℹ 187 more rows\n\n\nNext, we looked at grouping by both property type and neighborhood. As we saw earlier by using table, Indre By has around 1800 listings. 1108 of them are entire rental units, this is the most common combination of neighborhood and property type. Entire rental units seem to make up a bulk of the listings in Copenhagen overall, accounting for the entire top 5 common combinations. By the time we even get to the 20th common combination, these are much rarer property types with less than 100 in each neighborhood.\n\nsuperhostbyneighborhood &lt;- copenhagen %&gt;% group_by(neighbourhood_cleansed) %&gt;% \n  summarise(total=n(), superhost = sum(host_is_superhost == TRUE),\n            percent = (superhost/total) * 100)\nsuperhostbyneighborhood\n\n# A tibble: 11 × 4\n   neighbourhood_cleansed    total superhost percent\n   &lt;chr&gt;                     &lt;int&gt;     &lt;int&gt;   &lt;dbl&gt;\n 1 Amager Vest                 989       180    18.2\n 2 Amager st                   839       153    18.2\n 3 Bispebjerg                  478        77    16.1\n 4 Brnshj-Husum                167        32    19.2\n 5 Frederiksberg              1130       184    16.3\n 6 Indre By                   1818       422    23.2\n 7 Nrrebro                    1764       257    14.6\n 8 Valby                       381        72    18.9\n 9 Vanlse                      242        46    19.0\n10 Vesterbro-Kongens Enghave  1859       360    19.4\n11 sterbro                    1109       164    14.8\n\n\nNext, we looked at the percentage of listings by superhosts per neighborhood. Indre By has the highest number of listings but it also has the highest superhost percentages with 23.21%. By contrast, Nrrerbro only has 14.57% even though it has a similar number of listings. Airbnb superhost status is something determined by Airbnb and is a status given to hosts that are top-rated and experienced. This suggests that either Indre By listings are reliable due to their superhosts, or it could also mean it’s a popular area letting the hosts get more experience. Knowing this is the most expensive average neighborhood, our takeaway is that it’s a bit of both. The price is likely due to popularity, and the hosts likely thus have competition so are more focused on making sure the guests have good experiences.\n\nratingbyneighborhood &lt;- copenhagen %&gt;% group_by(neighbourhood_cleansed) %&gt;% \n  summarise(mean(review_scores_rating))\nratingbyneighborhood\n\n# A tibble: 11 × 2\n   neighbourhood_cleansed    `mean(review_scores_rating)`\n   &lt;chr&gt;                                            &lt;dbl&gt;\n 1 Amager Vest                                       4.84\n 2 Amager st                                         4.81\n 3 Bispebjerg                                        4.80\n 4 Brnshj-Husum                                      4.80\n 5 Frederiksberg                                     4.85\n 6 Indre By                                          4.80\n 7 Nrrebro                                           4.83\n 8 Valby                                             4.82\n 9 Vanlse                                            4.79\n10 Vesterbro-Kongens Enghave                         4.85\n11 sterbro                                           4.84\n\n\nFinally, we looked at the average review_scores_rating which we interpreted as the overall rating score. Here, there is again not a huge range with the lowest score being 4.79 and the highest being 4.85. Our takeaway here is that most people are having positive experiences, but we do need to consider that perhaps the only people who are reviewing are those happy while those unhappy may be filing complaints with Airbnb instead. Alternatively, some people may feel bad leaving anything lower than a 4 knowing these aren’t really corporations.\n\n\n\nTo begin our visualization process, we earlier cleaned the Copenhagen dataset by removing punctuation from the price column and converting it to numeric data. Additionally, we filtered the data to exclude any listings priced over 25,000 DKK per night, as these extreme outliers would skew our visualizations and detract from more meaningful trends.\n\n#clean price column\ncopenhagen &lt;- copenhagen %&gt;% filter(price &lt; 25000)\n\n#boxplot of price in top 10\nggplot(copenhagen, aes(x = fct_reorder(neighbourhood_cleansed, price, .fun = median),\n                       y = price)) +\n  geom_boxplot(fill = \"cornflowerblue\") +\n  coord_flip() +\n  labs(title = \"Price Distribution by Neighborhood\",\n       x = \"Neighborhood\", y = \"Price (DKK)\")\n\n\n\n\n\n\n\n\nOur first visualization is a boxplot comparing listing price distributions across the eleven Copenhagen neighborhoods. Indre By (which translates to “Inner City”) exhibits both the highest median price and the widest price range, likely due to its central, high-demand location. Other neighborhoods with notable outliers above 10,000 DKK include Vesterbro/Kongens Enghave, Nørrebro (presented as “Nrrebro” in the data due to special character limitations), and Amager Øst (“Amager st” in the data). Each of these neighborhoods borders the water, suggesting that waterfront properties may be commanding a premium, due to location and aesthetic. In contrast, Bispebjerg and Brønshøj-Husum (“Brnshj-Husum”) show lower median prices and fewer extreme outliers, indicating more affordable options. These neighborhoods are farther from the harbor and city center, making them less desirable for tourists.\n\n#group by neighborhood and superhost\nsuperhost_breakdown &lt;- copenhagen %&gt;%\n  group_by(neighbourhood_cleansed, host_is_superhost) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  group_by(neighbourhood_cleansed) %&gt;%\n  mutate(percent = count / sum(count) * 100)\n\n\n#plot by number of listings and breakdown of super or not \nggplot(superhost_breakdown, aes(x = reorder(neighbourhood_cleansed, -count), y = count, fill = host_is_superhost)) +\n  geom_col() +\n  geom_text(\n    aes(label = paste0(round(percent), \"%\")),\n    position = position_stack(vjust = 0.5), \n    color = \"white\",                               \n    size = 2) +\n  coord_flip() +\n  labs(\n    title = \"Superhost Composition by Neighborhood\",\n    x = \"Neighborhood\",\n    y = \"Number of Listings\",\n    fill = \"Superhost\") +\n  scale_fill_manual(values = c(\"FALSE\" = \"cornflowerblue\", \"TRUE\" = \"salmon\"))\n\n\n\n\n\n\n\n\nOur second visualization is a stacked bar plot showing the percentage of Superhosts by neighborhood, with the total number of listings displayed along the x-axis. Superhost percentages generally fall within the 15–20% range, with Indre By again standing out at 23%. Indre By also has the second-highest number of listings, reinforcing its competitiveness and potentially explaining the higher concentration of Superhosts, the high number of listings also explains the wide range of pricing seen in the boxplot. Vesterbro/Kongens Enghave, Indre By, and Nørrebro also have significantly more listings than other neighborhoods, reflecting their popularity among tourists. On the other hand, Brønshøj-Husum and Vanløse have the lowest Superhost rates at 9%, aligning with their lower pricing and likely simpler accommodation offerings.\n\n# distribution of reviews with proportions and mean \nggplot(copenhagen, aes(x = review_scores_rating)) +\n  geom_histogram(\n    binwidth = 0.2, \n    fill = \"cornflowerblue\", \n    color = \"salmon\") +\n  stat_bin(\n    binwidth = 0.2,\n    geom = \"text\",\n    aes(label = scales::percent(after_stat(count / sum(count)), accuracy = .1)),\n    vjust = -0.5,\n    color = \"black\",\n    size = 2) +\n  geom_vline(\n    xintercept = mean(copenhagen$review_scores_rating, na.rm = TRUE),\n    color = \"orchid\",\n    linetype = \"dashed\",\n    size = 1) +\n  labs(\n    title = \"Distribution of Review Scores\",\n    x = \"Review Score\",\n    y = \"Number of Listings\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThe third visualization is a histogram showing the distribution of review scores across all listings. Review scores are heavily left-skewed, with almost all reviews clustered between 4.5 and 5.0. The mean review score, marked by a dashed orchid line on the plot, is 4.82. Each bar is labeled with the percentage of total listings it represents, helping highlight how concentrated the reviews are in the highest ranges. There is a small uptick (only around .3%) in reviews around 3.0 likely represents guests who had neutral experiences, possibly correlating with lower-cost, lower-amenity listings. While this diagram suggests almost exclusive positive reviews, this is possibly due to the subjective nature of voluntary reviewing. Guests who felt incredibly satisfied with their stays are more likely to share their happy feelings than those who simply felt neutral, or unhappy. It would be interesting to explore if Airbnb filters out negative reviews, as the near total absence of low scores seems somewhat unlikely.\n\nmean(copenhagen$review_scores_rating)\n\n[1] 4.828895\n\n#price vs review scatterplot\nggplot(copenhagen, aes(x = review_scores_rating, y = price)) +\n  geom_point(alpha = 0.3, color = \"orchid\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") +\n  labs(\n    title = \"Price vs. Review Score\",\n    x = \"Review Score\",\n    y = \"Price (DKK)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe fourth visualization is a scatterplot examining the relationship between price and review score, with a dashed line representing the line of best fit. While the trend line does have a slight positive incline, it is nearly flat suggesting that a higher price does not correlate to a higher review score. This indicates that other factors (such as location or amenities) likely correlate more to guest satisfaction than value. It is interesting to note that all the listings with a per night price of 10,000 DKK or more received a review score of 4.5 or greater. This indicates an incredibly premium price does tend to correlate with a satisfied guest.\n\n#density plot of price by room type\nggplot(copenhagen, aes(x = price, fill = room_type)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Price Distribution by Room Type\",\n    x = \"Price (DKK)\",\n    y = \"Density\",\n    fill = \"Room Type\") +\n  xlim(0, 3000) +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\nWarning: Removed 362 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nFinally, the fifth visualization is a density plot illustrating price distributions across different room types. As expected, entire homes/apartments have the highest price range, reflecting one of Airbnb’s main benefits compared to hotels. Hotel rooms show a spike in the mid-to-high range, with little variation at the higher end but a small second spike at the very low end. Perhaps these are hostel style accommodations that are being referred to as hotel rooms, as hotels are typically Airbnb’s main competitors. Private rooms and shared rooms are concentrated at lower price points, likely catering to students and budget-conscious travelers such as backpackers. Overall, room type and location are likely the strongest predictors of listing price.\n\n\n\n\nleaflet(data = copenhagen) %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(\n    ~longitude, ~latitude,\n    radius = .1,\n    color = \"#6A5ACD\",\n    fillOpacity = 0.2) %&gt;%\n  setView(lng = mean(copenhagen$longitude),\n          lat = mean(copenhagen$latitude),\n          zoom = 12)\n\n\n\n\n\nThe interactive map built with the Leaflet package shows the geographic distribution of Airbnb listings across Copenhagen. The map confirms what the neighborhood listings bar plot displayed in Part III, listings are highly concentrated in Vesterbro/Kongens Enghave, Indre By, and Nørrebro, as these areas are located in close proximity to both the city center and the harbor. As one moves farther from the city center, the density of listings decreases noticeably, with much sparser coverage in outlying areas that are likely residential suburbs. Additionally, there are very few listings directly along the harborfront, likely due to commercial zones for cargo and cruise ships. The map suggests that Airbnb hosts strategically locate their listings near Copenhagen’s main attractions, shopping areas, restaurants, historical sites, and public transportation. The density also drops off significantly in suburban residential areas, which are less accessible to tourists and are likely populated primarily by full-time local residents. Finally, several large patches of open land visible on the map correspond to Copenhagen’s protected parks and green spaces. The lack of Airbnb listings in these areas reflects a cultural phenomenon in the region. Copenhagen, and Denmark as a whole, are well known for their emphasis on sustainability and their strong commitment to preserving the environment. This is reflected in strict land use regulations that prevent commercial developments, including Airbnb listings, from encroaching on protected natural spaces.\n\n\n\n\n\n\ncopenhagen_text &lt;- copenhagen %&gt;%\n  filter(!is.na(neighborhood_overview)) %&gt;%\n  pull(neighborhood_overview) %&gt;%\n  iconv(from = \"UTF-8\", to = \"ASCII\", sub = \"\") %&gt;%\n  paste(collapse = \" \")\n\ncopenhagen_text_df &lt;- data.frame(text = copenhagen_text)\n\ncopenhagen_text_clean &lt;- copenhagen_text_df %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(!word %in% stopwords(\"english\")) %&gt;%\n  filter(!word %in% c(\"copenhagen\", \"area\", \"neighborhood\", \"location\", \"br\"))\n\ncopenhagen_freq &lt;- copenhagen_text_clean %&gt;%\n  count(word, sort = TRUE)\n\nwordcloud(\n  words = copenhagen_freq$word,\n  freq = copenhagen_freq$n,\n  max.words = 100,\n  colors = brewer.pal(8, \"Dark2\"),\n  random.order = TRUE)\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\ncenter could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\nfrederiksberg could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\ncafes could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\nshopping could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\nplaygrounds could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\nshops could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\nnrrebro could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\nstation could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\npark could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\nfind could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(words = copenhagen_freq$word, freq = copenhagen_freq$n, :\neverything could not be fit on page. It will not be plotted.\n\n\n\n\n\n\n\n\n\nThe wordcloud visualization is generated from the neighborhood overview descriptions from the Airbnb listings. The most common term is restaurants, followed by words like walk, located, close, minutes, shopping, and metro. These terms emphasize proximity and convenience, giving guests the impression of walkability and easy access to amenities. This again confirms the popularity of listings near Copenhagen’s city center, as guests tend to prioritize connection to attractions and transportation. Copenhagen is renowned for its public transportation system and overall walkability, making it strategic for hosts to highlight these traits in their descriptions.\nDescriptive words such as quiet, cozy, charming, and vibrant also appear prominently, reflecting the hosts’ efforts to market a welcoming and pleasant atmosphere. The Danish concept of hygge is centered around creating warm, cozy environments and enjoying simple pleasures, particularly in the winter months. Given that Copenhagen experiences very short daylight hours in winter (an average of just one hour of sunshine in December), emphasizing a cozy and comfortable environment is an effective way for hosts to appeal to guests seeking an authentic hygge experience. The hosts’ use of these descriptive words evokes a strong sense of hygge, making their listings even more attractive to travelers looking for comfort and community during the darker months.\n\n\n\n\nbigrams &lt;- copenhagen_text_df %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\nbigrams_separated &lt;- bigrams %&gt;%\n  separate(bigram, into = c(\"word1\", \"word2\"), sep = \" \")\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(!word1 %in% stopwords(\"english\"),\n         !word2 %in% stopwords(\"english\"),\n         !word1 %in% c(\"copenhagen\", \"br\"),\n         !word2 %in% c(\"copenhagen\", \"br\"))\n\nbigrams_united &lt;- bigrams_filtered %&gt;%\n  unite(bigram, word1, word2, sep = \" \")\nbigram_freq &lt;- bigrams_united %&gt;%\n  count(bigram, sort = TRUE)\n\nwordcloud(\n  words = bigram_freq$bigram,\n  freq = bigram_freq$n,\n  max.words = 100,\n  colors = brewer.pal(8, \"Dark2\"),\n  random.order = FALSE)\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : walking distance could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : public transportation could not be fit on page. It will not\nbe plotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : bars restaurants could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : cafs restaurants could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : shops restaurants could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : rosenborg castle could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : minutes walking could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : amager strandpark could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : good restaurants could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : grocery shopping could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : quiet residential could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : great restaurants could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : shopping mall could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : traditional danish could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(words = bigram_freq$bigram, freq = bigram_freq$n,\nmax.words = 100, : amager strand could not be fit on page. It will not be\nplotted.\n\n\n\n\n\n\n\n\n\nTo add further context, we also generated a second wordcloud based on bigrams from the neighborhood overview descriptions. While not explicitly required for the assignment, we felt the bigram visualization provided additional insight into features that hosts highlight in their listings. Common phrases such as city center, minute walk, metro station, and central station once again emphasize the value of proximity and accessibility. This reinforces the recurring theme that guests place a very high value on location and walkability. The more a host can market the convenience and centrality of their listing, the more it will likely appeal to potential guests. Additionally, the bigram cloud included Copenhagen-specific tourism features such as Islands Brygge, meatpacking district, and the Little Mermaid, suggesting that hosts emphasize proximity to famous landmarks when marketing their properties."
  },
  {
    "objectID": "projects/airbnb/airbnb-copenhagen.html#data-preparation-exploration",
    "href": "projects/airbnb/airbnb-copenhagen.html#data-preparation-exploration",
    "title": "Airbnb Analysis in Copenhagen",
    "section": "Data Preparation & Exploration",
    "text": "Data Preparation & Exploration\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(tm)\nlibrary(tidytext)\nlibrary(wordcloud)\n\ncopenhagen &lt;- read_csv('copenhagen_listings.csv')\n\nThe dataset contains information about Airbnb listings in Copenhagen, Denmark. Upon importing the data into R, we observed a significant number of missing values. To identify where these values occur, we used the colSums(is.na()) function, which enabled us to pinpoint missing data across columns and assess how best to handle them. By sorting these results in descending order, we prioritized columns with the most missing entries. We felt that values that are central to our exploratory and predictive analysis should have missing values removed, as imputing would potentially compromise these critical values. We chose to remove any rows that had NA values in price, review_scores_rating, and bedrooms. After this cleaning step, we retained 10,776 rows of information, plenty of data to run our analysis and a worthy compromise to ensure accuracy. For other columns with missing values, we applied targeted strategies. For example, we imputed missing values in the binary host_is_superhost column by assuming a value of FALSE for hosts without a recorded status. For the numeric variables bathrooms and beds, we replaced missing values with the median of each respective column. As for neighborhood_overview, we will simply exclude missing entries from any future analysis involving that variable. Remaining columns with missing values were deemed either nonessential for our analysis (e.g., license, host_neighbourhood, etc.), or had corresponding cleaned versions (e.g., neighbourhood and neighbourhood_cleansed). The has_availability field contained missing values. To impute these, we leveraged related variables: availability_30, availability_60, availability_90, and availability_365. If any of these showed more than 0 available days, we set has_availability to True. Furthermore, we found 2,630 records with no reviews, resulting in missing values for all review-related columns. We imputed review_scores_rating using the median value across the dataset. Overall, our cleaning approach allowed us to preserve a large portion of the dataset, while ensuring the data was complete for modeling purposes.\n\nMissing Values\n\nna_summary &lt;- colSums(is.na(copenhagen)) %&gt;%\n  sort(decreasing = TRUE) %&gt;%\n  as.data.frame() %&gt;%\n  tibble::rownames_to_column(var = \"Column\") %&gt;%\n  rename(Missing_Count = \".\") %&gt;%\n  mutate(Missing_Percent = round((Missing_Count / nrow(copenhagen)) * 100, 2))\n\nprint(na_summary)\n\n\n\n\n\n\n\n\ncopenhagen &lt;- copenhagen %&gt;%\n  filter(!is.na(price) & !is.na(review_scores_rating) & !is.na(bedrooms))\n\ncopenhagen &lt;- copenhagen %&gt;%\n  mutate(\n    host_is_superhost = ifelse(is.na(host_is_superhost), \"FALSE\", host_is_superhost)\n    )\n\ncopenhagen &lt;- copenhagen %&gt;%\n  group_by(room_type) %&gt;%\n  mutate(\n    bathrooms = ifelse(is.na(bathrooms), median(bathrooms, na.rm = TRUE), bathrooms),\n    bedrooms = ifelse(is.na(bedrooms), median(bedrooms, na.rm = TRUE), bedrooms),\n    beds = ifelse(is.na(beds), median(beds, na.rm = TRUE), beds),\n    price = ifelse(is.na(price), median(price, na.rm = TRUE), price)\n  ) %&gt;%\n  ungroup()\n\n# if property available in N days =&gt; has_availability is True\ncopenhagen$has_availability[is.na(copenhagen$has_availability)] &lt;- ifelse(\n  is.na(copenhagen$has_availability) &\n    rowSums(copenhagen[is.na(copenhagen$has_availability), c(\"availability_30\", \"availability_60\", \"availability_90\", \"availability_365\")] &gt; 0, na.rm = TRUE) &gt; 0, TRUE, FALSE)\n\n# 2630 records with no_review\ncopenhagen$reviews_per_month[copenhagen$number_of_reviews == 0] &lt;- 0\n\n# set Median value for NAs in review related columns\ncopenhagen$review_scores_rating &lt;- replace_na(copenhagen$review_scores_rating, \n                                              median(copenhagen$review_scores_rating, na.rm = TRUE))\ncopenhagen$review_scores_location &lt;- replace_na(copenhagen$review_scores_location, \n                                                median(copenhagen$review_scores_location, na.rm = TRUE))\ncopenhagen$review_scores_value &lt;- replace_na(copenhagen$review_scores_value, \n                                             median(copenhagen$review_scores_value, na.rm = TRUE))\n\n\n\nSummary Statistics\n\ntable(copenhagen$neighbourhood_cleansed)\n\n\n                Amager st               Amager Vest                Bispebjerg \n                      839                       989                       478 \n             Brnshj-Husum             Frederiksberg                  Indre By \n                      167                      1130                      1818 \n                  Nrrebro                   sterbro                     Valby \n                     1764                      1109                       381 \n                   Vanlse Vesterbro-Kongens Enghave \n                      242                      1859 \n\n\nWe first used table to see the different neighborhoods and how many listings were in each one. There are 11 neighborhoods and we chose to keep them all as these are the 10 municipal districts of Copenhagen plus Frederiksberg, which is its center. It felt like these would all be distinct and not too overwhelming to look at. We also knew we were going to want to look at price so made sure to convert it to a numeric and remove the dollar signs and commas.\n\ncopenhagen$price &lt;- as.numeric(gsub(\"[$,]\", \"\", copenhagen$price))\navgpricebyneighborhood &lt;- copenhagen %&gt;% group_by(neighbourhood_cleansed) %&gt;% \n  summarise(mean(price))\navgpricebyneighborhood\n\n# A tibble: 11 × 2\n   neighbourhood_cleansed    `mean(price)`\n   &lt;chr&gt;                             &lt;dbl&gt;\n 1 Amager Vest                       1332.\n 2 Amager st                         1220.\n 3 Bispebjerg                         937.\n 4 Brnshj-Husum                       959.\n 5 Frederiksberg                     1329.\n 6 Indre By                          1887.\n 7 Nrrebro                           1206.\n 8 Valby                             1021.\n 9 Vanlse                            1041.\n10 Vesterbro-Kongens Enghave         1365.\n11 sterbro                           1421.\n\n\nThe first statistic we chose to look at was the average price per night for a listing in each neighborhood. We see here that the average price for listings in Indre By are basically double those in Bispebjerg and Brnshj-Husum. There’s quite a big range across the neighborhoods, so our takeaway is that different neighborhoods are more expensive—we don’t know why yet though, it could be due to demand or popularity or any other factor.\n\navgaccomodatesbyneighborhood &lt;- copenhagen %&gt;% group_by(neighbourhood_cleansed) %&gt;% \n  summarise(mean(accommodates))\navgaccomodatesbyneighborhood\n\n# A tibble: 11 × 2\n   neighbourhood_cleansed    `mean(accommodates)`\n   &lt;chr&gt;                                    &lt;dbl&gt;\n 1 Amager Vest                               3.58\n 2 Amager st                                 3.48\n 3 Bispebjerg                                3.05\n 4 Brnshj-Husum                              3.96\n 5 Frederiksberg                             3.32\n 6 Indre By                                  3.76\n 7 Nrrebro                                   3.01\n 8 Valby                                     3.43\n 9 Vanlse                                    3.83\n10 Vesterbro-Kongens Enghave                 3.20\n11 sterbro                                   3.29\n\n\nThe next stat we chose to look at was the average number of people accommodated in each listing, or basically how many people can fit, by neighborhood. There is much less variety here with Nrrebro having a mean of approximately 3 and Brnshj-Husum having a mean of almost 4. Part of the reason for the smaller range is likely due to the fact that these are all still in the same city—it’s not likely for some neighborhoods to have properties accommodating 20 while some only accommodate 1.\n\npropertytypebyneighborhood &lt;- copenhagen %&gt;% count(neighbourhood_cleansed, property_type, \n                                                   sort = TRUE)\npropertytypebyneighborhood\n\n# A tibble: 197 × 3\n   neighbourhood_cleansed    property_type          n\n   &lt;chr&gt;                     &lt;chr&gt;              &lt;int&gt;\n 1 Indre By                  Entire rental unit  1108\n 2 Nrrebro                   Entire rental unit  1107\n 3 Vesterbro-Kongens Enghave Entire rental unit  1056\n 4 sterbro                   Entire rental unit   691\n 5 Frederiksberg             Entire rental unit   655\n 6 Vesterbro-Kongens Enghave Entire condo         573\n 7 Nrrebro                   Entire condo         517\n 8 Amager Vest               Entire rental unit   457\n 9 Amager st                 Entire rental unit   429\n10 Indre By                  Entire condo         411\n# ℹ 187 more rows\n\n\nNext, we looked at grouping by both property type and neighborhood. As we saw earlier by using table, Indre By has around 1800 listings. 1108 of them are entire rental units, this is the most common combination of neighborhood and property type. Entire rental units seem to make up a bulk of the listings in Copenhagen overall, accounting for the entire top 5 common combinations. By the time we even get to the 20th common combination, these are much rarer property types with less than 100 in each neighborhood.\n\nsuperhostbyneighborhood &lt;- copenhagen %&gt;% group_by(neighbourhood_cleansed) %&gt;% \n  summarise(total=n(), superhost = sum(host_is_superhost == TRUE),\n            percent = (superhost/total) * 100)\nprint(superhostbyneighborhood)\n\n# A tibble: 11 × 4\n   neighbourhood_cleansed    total superhost percent\n   &lt;chr&gt;                     &lt;int&gt;     &lt;int&gt;   &lt;dbl&gt;\n 1 Amager Vest                 989       180    18.2\n 2 Amager st                   839       153    18.2\n 3 Bispebjerg                  478        77    16.1\n 4 Brnshj-Husum                167        32    19.2\n 5 Frederiksberg              1130       184    16.3\n 6 Indre By                   1818       422    23.2\n 7 Nrrebro                    1764       257    14.6\n 8 Valby                       381        72    18.9\n 9 Vanlse                      242        46    19.0\n10 Vesterbro-Kongens Enghave  1859       360    19.4\n11 sterbro                    1109       164    14.8\n\n\nNext, we looked at the percentage of listings by superhosts per neighborhood. Indre By has the highest number of listings but it also has the highest superhost percentages with 23.21%. By contrast, Nrrerbro only has 14.57% even though it has a similar number of listings. Airbnb superhost status is something determined by Airbnb and is a status given to hosts that are top-rated and experienced. This suggests that either Indre By listings are reliable due to their superhosts, or it could also mean it’s a popular area letting the hosts get more experience. Knowing this is the most expensive average neighborhood, our takeaway is that it’s a bit of both. The price is likely due to popularity, and the hosts likely thus have competition so are more focused on making sure the guests have good experiences.\n\nratingbyneighborhood &lt;- copenhagen %&gt;% group_by(neighbourhood_cleansed) %&gt;% \n  summarise(mean(review_scores_rating))\nratingbyneighborhood\n\n# A tibble: 11 × 2\n   neighbourhood_cleansed    `mean(review_scores_rating)`\n   &lt;chr&gt;                                            &lt;dbl&gt;\n 1 Amager Vest                                       4.84\n 2 Amager st                                         4.81\n 3 Bispebjerg                                        4.80\n 4 Brnshj-Husum                                      4.80\n 5 Frederiksberg                                     4.85\n 6 Indre By                                          4.80\n 7 Nrrebro                                           4.83\n 8 Valby                                             4.82\n 9 Vanlse                                            4.79\n10 Vesterbro-Kongens Enghave                         4.85\n11 sterbro                                           4.84\n\n\nFinally, we looked at the average review_scores_rating which we interpreted as the overall rating score. Here, there is again not a huge range with the lowest score being 4.79 and the highest being 4.85. Our takeaway here is that most people are having positive experiences, but we do need to consider that perhaps the only people who are reviewing are those happy while those unhappy may be filing complaints with Airbnb instead. Alternatively, some people may feel bad leaving anything lower than a 4 knowing these aren’t really corporations.\n\n\nData Visualization\nTo begin our visualization process, we earlier cleaned the Copenhagen dataset by removing punctuation from the price column and converting it to numeric data. Additionally, we filtered the data to exclude any listings priced over 25,000 DKK per night, as these extreme outliers would skew our visualizations and detract from more meaningful trends.\n\n#clean price column\ncopenhagen &lt;- copenhagen %&gt;% filter(price &lt; 25000)\n\n#boxplot of price in top 10\nggplot(copenhagen, aes(x = fct_reorder(neighbourhood_cleansed, price, .fun = median),\n                       y = price)) +\n  geom_boxplot(fill = \"cornflowerblue\") +\n  coord_flip() +\n  labs(title = \"Price Distribution by Neighborhood\",\n       x = \"Neighborhood\", y = \"Price (DKK)\")\n\n\n\n\n\n\n\n\nOur first visualization is a boxplot comparing listing price distributions across the eleven Copenhagen neighborhoods. Indre By (which translates to “Inner City”) exhibits both the highest median price and the widest price range, likely due to its central, high-demand location. Other neighborhoods with notable outliers above 10,000 DKK include Vesterbro/Kongens Enghave, Nørrebro (presented as “Nrrebro” in the data due to special character limitations), and Amager Øst (“Amager st” in the data). Each of these neighborhoods borders the water, suggesting that waterfront properties may be commanding a premium, due to location and aesthetic. In contrast, Bispebjerg and Brønshøj-Husum (“Brnshj-Husum”) show lower median prices and fewer extreme outliers, indicating more affordable options. These neighborhoods are farther from the harbor and city center, making them less desirable for tourists.\n\n#group by neighborhood and superhost\nsuperhost_breakdown &lt;- copenhagen %&gt;%\n  group_by(neighbourhood_cleansed, host_is_superhost) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  group_by(neighbourhood_cleansed) %&gt;%\n  mutate(percent = count / sum(count) * 100)\n\n\n#plot by number of listings and breakdown of super or not \nggplot(superhost_breakdown, aes(x = reorder(neighbourhood_cleansed, -count), y = count, fill = host_is_superhost)) +\n  geom_col() +\n  geom_text(\n    aes(label = paste0(round(percent), \"%\")),\n    position = position_stack(vjust = 0.5), \n    color = \"white\",                               \n    size = 2) +\n  coord_flip() +\n  labs(\n    title = \"Superhost Composition by Neighborhood\",\n    x = \"Neighborhood\",\n    y = \"Number of Listings\",\n    fill = \"Superhost\") +\n  scale_fill_manual(values = c(\"FALSE\" = \"cornflowerblue\", \"TRUE\" = \"salmon\"))\n\n\n\n\n\n\n\n\nOur second visualization is a stacked bar plot showing the percentage of Superhosts by neighborhood, with the total number of listings displayed along the x-axis. Superhost percentages generally fall within the 15–20% range, with Indre By again standing out at 23%. Indre By also has the second-highest number of listings, reinforcing its competitiveness and potentially explaining the higher concentration of Superhosts, the high number of listings also explains the wide range of pricing seen in the boxplot. Vesterbro/Kongens Enghave, Indre By, and Nørrebro also have significantly more listings than other neighborhoods, reflecting their popularity among tourists. On the other hand, Brønshøj-Husum and Vanløse have the lowest Superhost rates at 9%, aligning with their lower pricing and likely simpler accommodation offerings.\n\n# distribution of reviews with proportions and mean \nggplot(copenhagen, aes(x = review_scores_rating)) +\n  geom_histogram(\n    binwidth = 0.2, \n    fill = \"cornflowerblue\", \n    color = \"salmon\") +\n  stat_bin(\n    binwidth = 0.2,\n    geom = \"text\",\n    aes(label = scales::percent(after_stat(count / sum(count)), accuracy = .1)),\n    vjust = -0.5,\n    color = \"black\",\n    size = 2) +\n  geom_vline(\n    xintercept = mean(copenhagen$review_scores_rating, na.rm = TRUE),\n    color = \"orchid\",\n    linetype = \"dashed\",\n    size = 1) +\n  labs(\n    title = \"Distribution of Review Scores\",\n    x = \"Review Score\",\n    y = \"Number of Listings\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe third visualization is a histogram showing the distribution of review scores across all listings. Review scores are heavily left-skewed, with almost all reviews clustered between 4.5 and 5.0. The mean review score, marked by a dashed orchid line on the plot, is 4.82. Each bar is labeled with the percentage of total listings it represents, helping highlight how concentrated the reviews are in the highest ranges. There is a small uptick (only around .3%) in reviews around 3.0 likely represents guests who had neutral experiences, possibly correlating with lower-cost, lower-amenity listings. While this diagram suggests almost exclusive positive reviews, this is possibly due to the subjective nature of voluntary reviewing. Guests who felt incredibly satisfied with their stays are more likely to share their happy feelings than those who simply felt neutral, or unhappy. It would be interesting to explore if Airbnb filters out negative reviews, as the near total absence of low scores seems somewhat unlikely.\n\nmean(copenhagen$review_scores_rating)\n\n[1] 4.828895\n\n#price vs review scatterplot\nggplot(copenhagen, aes(x = review_scores_rating, y = price)) +\n  geom_point(alpha = 0.3, color = \"orchid\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") +\n  labs(\n    title = \"Price vs. Review Score\",\n    x = \"Review Score\",\n    y = \"Price (DKK)\")\n\n\n\n\n\n\n\n\nThe fourth visualization is a scatterplot examining the relationship between price and review score, with a dashed line representing the line of best fit. While the trend line does have a slight positive incline, it is nearly flat suggesting that a higher price does not correlate to a higher review score. This indicates that other factors (such as location or amenities) likely correlate more to guest satisfaction than value. It is interesting to note that all the listings with a per night price of 10,000 DKK or more received a review score of 4.5 or greater. This indicates an incredibly premium price does tend to correlate with a satisfied guest.\n\n#density plot of price by room type\nggplot(copenhagen, aes(x = price, fill = room_type)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = \"Price Distribution by Room Type\",\n    x = \"Price (DKK)\",\n    y = \"Density\",\n    fill = \"Room Type\") +\n  xlim(0, 3000) +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nFinally, the fifth visualization is a density plot illustrating price distributions across different room types. As expected, entire homes/apartments have the highest price range, reflecting one of Airbnb’s main benefits compared to hotels. Hotel rooms show a spike in the mid-to-high range, with little variation at the higher end but a small second spike at the very low end. Perhaps these are hostel style accommodations that are being referred to as hotel rooms, as hotels are typically Airbnb’s main competitors. Private rooms and shared rooms are concentrated at lower price points, likely catering to students and budget-conscious travelers such as backpackers. Overall, room type and location are likely the strongest predictors of listing price.\n\n\nMapping\n\nleaflet(data = copenhagen) %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(\n    ~longitude, ~latitude,\n    radius = .1,\n    color = \"#6A5ACD\",\n    fillOpacity = 0.2) %&gt;%\n  setView(lng = mean(copenhagen$longitude),\n          lat = mean(copenhagen$latitude),\n          zoom = 12)\n\n\n\n\n\nThe interactive map built with the Leaflet package shows the geographic distribution of Airbnb listings across Copenhagen. The map confirms what the neighborhood listings bar plot displayed in Part III, listings are highly concentrated in Vesterbro/Kongens Enghave, Indre By, and Nørrebro, as these areas are located in close proximity to both the city center and the harbor. As one moves farther from the city center, the density of listings decreases noticeably, with much sparser coverage in outlying areas that are likely residential suburbs. Additionally, there are very few listings directly along the harborfront, likely due to commercial zones for cargo and cruise ships. The map suggests that Airbnb hosts strategically locate their listings near Copenhagen’s main attractions, shopping areas, restaurants, historical sites, and public transportation. The density also drops off significantly in suburban residential areas, which are less accessible to tourists and are likely populated primarily by full-time local residents. Finally, several large patches of open land visible on the map correspond to Copenhagen’s protected parks and green spaces. The lack of Airbnb listings in these areas reflects a cultural phenomenon in the region. Copenhagen, and Denmark as a whole, are well known for their emphasis on sustainability and their strong commitment to preserving the environment. This is reflected in strict land use regulations that prevent commercial developments, including Airbnb listings, from encroaching on protected natural spaces.\n\n\nWordcloud\n\nUnigram wordcloud\n\ncopenhagen_text &lt;- copenhagen %&gt;%\n  filter(!is.na(neighborhood_overview)) %&gt;%\n  pull(neighborhood_overview) %&gt;%\n  iconv(from = \"UTF-8\", to = \"ASCII\", sub = \"\") %&gt;%\n  paste(collapse = \" \")\n\ncopenhagen_text_df &lt;- data.frame(text = copenhagen_text)\n\ncopenhagen_text_clean &lt;- copenhagen_text_df %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(!word %in% stopwords(\"english\")) %&gt;%\n  filter(!word %in% c(\"copenhagen\", \"area\", \"neighborhood\", \"location\", \"br\"))\n\ncopenhagen_freq &lt;- copenhagen_text_clean %&gt;%\n  count(word, sort = TRUE)\n\nwordcloud(\n  words = copenhagen_freq$word,\n  freq = copenhagen_freq$n,\n  max.words = 100,\n  colors = brewer.pal(8, \"Dark2\"),\n  random.order = TRUE)\n\n\n\n\n\n\n\n\nThe wordcloud visualization is generated from the neighborhood overview descriptions from the Airbnb listings. The most common term is restaurants, followed by words like walk, located, close, minutes, shopping, and metro. These terms emphasize proximity and convenience, giving guests the impression of walkability and easy access to amenities. This again confirms the popularity of listings near Copenhagen’s city center, as guests tend to prioritize connection to attractions and transportation. Copenhagen is renowned for its public transportation system and overall walkability, making it strategic for hosts to highlight these traits in their descriptions.\nDescriptive words such as quiet, cozy, charming, and vibrant also appear prominently, reflecting the hosts’ efforts to market a welcoming and pleasant atmosphere. The Danish concept of hygge is centered around creating warm, cozy environments and enjoying simple pleasures, particularly in the winter months. Given that Copenhagen experiences very short daylight hours in winter (an average of just one hour of sunshine in December), emphasizing a cozy and comfortable environment is an effective way for hosts to appeal to guests seeking an authentic hygge experience. The hosts’ use of these descriptive words evokes a strong sense of hygge, making their listings even more attractive to travelers looking for comfort and community during the darker months.\n\n\nBigram cloud\n\nbigrams &lt;- copenhagen_text_df %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\nbigrams_separated &lt;- bigrams %&gt;%\n  separate(bigram, into = c(\"word1\", \"word2\"), sep = \" \")\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(!word1 %in% stopwords(\"english\"),\n         !word2 %in% stopwords(\"english\"),\n         !word1 %in% c(\"copenhagen\", \"br\"),\n         !word2 %in% c(\"copenhagen\", \"br\"))\n\nbigrams_united &lt;- bigrams_filtered %&gt;%\n  unite(bigram, word1, word2, sep = \" \")\nbigram_freq &lt;- bigrams_united %&gt;%\n  count(bigram, sort = TRUE)\n\nwordcloud(\n  words = bigram_freq$bigram,\n  freq = bigram_freq$n,\n  max.words = 100,\n  colors = brewer.pal(8, \"Dark2\"),\n  random.order = FALSE)\n\n\n\n\n\n\n\n\nTo add further context, we also generated a second wordcloud based on bigrams from the neighborhood overview descriptions. While not explicitly required for the assignment, we felt the bigram visualization provided additional insight into features that hosts highlight in their listings. Common phrases such as city center, minute walk, metro station, and central station once again emphasize the value of proximity and accessibility. This reinforces the recurring theme that guests place a very high value on location and walkability. The more a host can market the convenience and centrality of their listing, the more it will likely appeal to potential guests. Additionally, the bigram cloud included Copenhagen-specific tourism features such as Islands Brygge, meatpacking district, and the Little Mermaid, suggesting that hosts emphasize proximity to famous landmarks when marketing their properties."
  },
  {
    "objectID": "projects/airbnb/airbnb-copenhagen.html#prediction",
    "href": "projects/airbnb/airbnb-copenhagen.html#prediction",
    "title": "Airbnb Analysis in Copenhagen",
    "section": "Prediction",
    "text": "Prediction\n\nlibrary(reshape2)\n\n\nset.seed(699)\ntrain.indexpred &lt;- sample(c(1:nrow(copenhagen)), nrow(copenhagen)*0.6)\ntrain.df &lt;- copenhagen[train.indexpred,]\nvalid.df &lt;- copenhagen[-train.indexpred,]\n\ntrain.df = train.df %&gt;% select(19,29,33:36,38,39,41,52:55,57:59,62:68,70)\n\ntrain.df$host_is_superhost &lt;- as.factor(train.df$host_is_superhost)\ntrain.df$neighbourhood_cleansed &lt;- as.factor(train.df$neighbourhood_cleansed)\ntrain.df$property_type &lt;- as.factor(train.df$property_type)\ntrain.df$room_type &lt;- as.factor(train.df$room_type)\ntrain.df$instant_bookable &lt;- as.factor(train.df$instant_bookable)\n\nTo build our multiple linear regression model, we first began with dividing our data into a training and validation set. Then, we looked at the dataset and dataset description and selected the variables that we thought would be relevant, excluding things like url, host profile photo, etc… landing on the following variables:\n\ncolnames(train.df)\n\n [1] \"host_is_superhost\"           \"neighbourhood_cleansed\"     \n [3] \"property_type\"               \"room_type\"                  \n [5] \"accommodates\"                \"bathrooms\"                  \n [7] \"bedrooms\"                    \"beds\"                       \n [9] \"price\"                       \"availability_30\"            \n[11] \"availability_60\"             \"availability_90\"            \n[13] \"availability_365\"            \"number_of_reviews\"          \n[15] \"number_of_reviews_ltm\"       \"number_of_reviews_l30d\"     \n[17] \"review_scores_rating\"        \"review_scores_accuracy\"     \n[19] \"review_scores_cleanliness\"   \"review_scores_checkin\"      \n[21] \"review_scores_communication\" \"review_scores_location\"     \n[23] \"review_scores_value\"         \"instant_bookable\"           \n\n\n\n# Compute correlation matrix\ncordata = train.df %&gt;% select(5:8,10:23)\ncormat &lt;- cor(cordata, use = \"pairwise.complete.obs\")\n\n# Convert to long format\nmelted_cormat &lt;- melt(cormat)\n\n# Heatmap\nggplot(melted_cormat, aes(Var1, Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                       midpoint = 0, limit = c(-1,1), space = \"Lab\",\n                       name = \"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +\n  coord_fixed() +\n  labs(title = \"Correlation Heatmap\")\n\n\n\n\n\n\n\n\nWe then looked at correlation to try and avoid collinearity. We removed bedrooms, beds, availability_60, availability_90, number_of_reviews, and review_scores_value due to strong correlations with other variables.\n\ntrain.df = subset(train.df, select=-c(\n  bedrooms, beds, availability_60, availability_90, number_of_reviews_ltm,review_scores_value))\n\nFinally, we used backwards elimination to remove some more variables.\n\nmlrmodel &lt;-lm(price ~ .,\n              data = train.df)\n\nmlrmodel.step &lt;- step(mlrmodel, direction = \"backward\")\n\nStart:  AIC=84676.9\nprice ~ host_is_superhost + neighbourhood_cleansed + property_type + \n    room_type + accommodates + bathrooms + availability_30 + \n    availability_365 + number_of_reviews + number_of_reviews_l30d + \n    review_scores_rating + review_scores_accuracy + review_scores_cleanliness + \n    review_scores_checkin + review_scores_communication + review_scores_location + \n    instant_bookable\n\n                              Df Sum of Sq        RSS   AIC\n- room_type                    1    130507 3107648307 84675\n- review_scores_rating         1    453113 3107970912 84676\n- review_scores_checkin        1    625371 3108143171 84676\n- number_of_reviews            1    895824 3108413624 84677\n&lt;none&gt;                                     3107517800 84677\n- review_scores_location       1   1029241 3108547040 84677\n- host_is_superhost            1   2782925 3110300724 84681\n- review_scores_communication  1   2964072 3110481872 84681\n- instant_bookable             1   2993626 3110511426 84681\n- review_scores_accuracy       1   3539682 3111057481 84682\n- property_type               38  41170228 3148688028 84686\n- review_scores_cleanliness    1   8214675 3115732475 84692\n- availability_365             1  19980311 3127498110 84716\n- number_of_reviews_l30d       1  29940576 3137458376 84737\n- availability_30              1  39110394 3146628193 84756\n- bathrooms                    1 116410784 3223928583 84913\n- neighbourhood_cleansed      10 272114038 3379631838 85199\n- accommodates                 1 534452716 3641970515 85700\n\nStep:  AIC=84675.18\nprice ~ host_is_superhost + neighbourhood_cleansed + property_type + \n    accommodates + bathrooms + availability_30 + availability_365 + \n    number_of_reviews + number_of_reviews_l30d + review_scores_rating + \n    review_scores_accuracy + review_scores_cleanliness + review_scores_checkin + \n    review_scores_communication + review_scores_location + instant_bookable\n\n                              Df Sum of Sq        RSS   AIC\n- review_scores_rating         1    453500 3108101807 84674\n- review_scores_checkin        1    618935 3108267242 84674\n- number_of_reviews            1    893680 3108541987 84675\n&lt;none&gt;                                     3107648307 84675\n- review_scores_location       1   1044197 3108692504 84675\n- host_is_superhost            1   2778062 3110426369 84679\n- review_scores_communication  1   2956766 3110605073 84679\n- instant_bookable             1   3057900 3110706207 84680\n- review_scores_accuracy       1   3552931 3111201238 84681\n- review_scores_cleanliness    1   8221882 3115870189 84690\n- availability_365             1  19968925 3127617232 84715\n- number_of_reviews_l30d       1  30017066 3137665373 84735\n- availability_30              1  39142076 3146790383 84754\n- property_type               40 120583502 3228231809 84841\n- bathrooms                    1 116384097 3224032404 84911\n- neighbourhood_cleansed      10 272005324 3379653631 85197\n- accommodates                 1 534555892 3642204199 85699\n\nStep:  AIC=84674.12\nprice ~ host_is_superhost + neighbourhood_cleansed + property_type + \n    accommodates + bathrooms + availability_30 + availability_365 + \n    number_of_reviews + number_of_reviews_l30d + review_scores_accuracy + \n    review_scores_cleanliness + review_scores_checkin + review_scores_communication + \n    review_scores_location + instant_bookable\n\n                              Df Sum of Sq        RSS   AIC\n- review_scores_checkin        1    517584 3108619391 84673\n- number_of_reviews            1    826583 3108928390 84674\n- review_scores_location       1    862402 3108964209 84674\n&lt;none&gt;                                     3108101807 84674\n- host_is_superhost            1   2689241 3110791048 84678\n- review_scores_accuracy       1   3147032 3111248839 84679\n- instant_bookable             1   3237881 3111339688 84679\n- review_scores_communication  1   3891683 3111993490 84680\n- review_scores_cleanliness    1   8140360 3116242168 84689\n- availability_365             1  20002118 3128103925 84714\n- number_of_reviews_l30d       1  30155144 3138256951 84735\n- availability_30              1  39234663 3147336470 84753\n- property_type               40 120308115 3228409923 84840\n- bathrooms                    1 116336832 3224438639 84910\n- neighbourhood_cleansed      10 273403509 3381505316 85199\n- accommodates                 1 534913347 3643015154 85698\n\nStep:  AIC=84673.2\nprice ~ host_is_superhost + neighbourhood_cleansed + property_type + \n    accommodates + bathrooms + availability_30 + availability_365 + \n    number_of_reviews + number_of_reviews_l30d + review_scores_accuracy + \n    review_scores_cleanliness + review_scores_communication + \n    review_scores_location + instant_bookable\n\n                              Df Sum of Sq        RSS   AIC\n- number_of_reviews            1    790343 3109409734 84673\n&lt;none&gt;                                     3108619391 84673\n- review_scores_location       1   1094951 3109714342 84673\n- host_is_superhost            1   2721272 3111340662 84677\n- instant_bookable             1   3197921 3111817312 84678\n- review_scores_communication  1   3382674 3112002065 84678\n- review_scores_accuracy       1   3413615 3112033006 84678\n- review_scores_cleanliness    1   8518034 3117137425 84689\n- availability_365             1  20020743 3128640134 84713\n- number_of_reviews_l30d       1  30210300 3138829691 84734\n- availability_30              1  39201473 3147820864 84752\n- property_type               40 120098923 3228718314 84838\n- bathrooms                    1 116399996 3225019387 84909\n- neighbourhood_cleansed      10 272904291 3381523682 85197\n- accommodates                 1 535777278 3644396669 85699\n\nStep:  AIC=84672.84\nprice ~ host_is_superhost + neighbourhood_cleansed + property_type + \n    accommodates + bathrooms + availability_30 + availability_365 + \n    number_of_reviews_l30d + review_scores_accuracy + review_scores_cleanliness + \n    review_scores_communication + review_scores_location + instant_bookable\n\n                              Df Sum of Sq        RSS   AIC\n&lt;none&gt;                                     3109409734 84673\n- review_scores_location       1   1160816 3110570550 84673\n- host_is_superhost            1   2230932 3111640666 84675\n- instant_bookable             1   3122952 3112532686 84677\n- review_scores_communication  1   3394669 3112804403 84678\n- review_scores_accuracy       1   3403513 3112813247 84678\n- review_scores_cleanliness    1   8531773 3117941507 84689\n- availability_365             1  19743820 3129153554 84712\n- availability_30              1  39616788 3149026522 84753\n- number_of_reviews_l30d       1  44775227 3154184960 84763\n- property_type               40 123083939 3232493672 84844\n- bathrooms                    1 116222176 3225631909 84908\n- neighbourhood_cleansed      10 272201524 3381611258 85195\n- accommodates                 1 535031737 3644441471 85697\n\n\nThe regression equation that the model generated is extremely long because of the categorical variables that remain. It is the intercept (134.7556) + the product of each intercept times the value for each variable (ex:212.7862 * the number of people the listing can accommodate).\n\nsummary(mlrmodel.step)\n\n\nCall:\nlm(formula = price ~ host_is_superhost + neighbourhood_cleansed + \n    property_type + accommodates + bathrooms + availability_30 + \n    availability_365 + number_of_reviews_l30d + review_scores_accuracy + \n    review_scores_cleanliness + review_scores_communication + \n    review_scores_location + instant_bookable, data = train.df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1954.1  -312.0   -78.4   171.3 16721.0 \n\nCoefficients:\n                                                  Estimate Std. Error t value\n(Intercept)                                     -2.688e+02  4.749e+02  -0.566\nhost_is_superhostTRUE                            5.082e+01  2.372e+01   2.143\nneighbourhood_cleansedAmager Vest                6.346e+01  4.271e+01   1.486\nneighbourhood_cleansedBispebjerg                -1.495e+02  5.180e+01  -2.885\nneighbourhood_cleansedBrnshj-Husum              -3.287e+02  7.688e+01  -4.275\nneighbourhood_cleansedFrederiksberg              1.301e+02  4.146e+01   3.138\nneighbourhood_cleansedIndre By                   5.860e+02  3.895e+01  15.045\nneighbourhood_cleansedNrrebro                    7.991e+01  3.866e+01   2.067\nneighbourhood_cleansedsterbro                    1.275e+02  4.160e+01   3.065\nneighbourhood_cleansedValby                     -1.999e+02  5.557e+01  -3.597\nneighbourhood_cleansedVanlse                    -2.944e+02  6.791e+01  -4.335\nneighbourhood_cleansedVesterbro-Kongens Enghave  1.355e+02  3.827e+01   3.541\nproperty_typeEntire bungalow                    -1.421e+03  5.708e+02  -2.489\nproperty_typeEntire cabin                       -8.787e+02  5.718e+02  -1.537\nproperty_typeEntire condo                       -7.803e+02  4.045e+02  -1.929\nproperty_typeEntire cottage                     -1.205e+03  6.383e+02  -1.887\nproperty_typeEntire guest suite                 -1.059e+03  5.348e+02  -1.980\nproperty_typeEntire guesthouse                  -8.737e+02  4.952e+02  -1.764\nproperty_typeEntire home                        -9.046e+02  4.060e+02  -2.228\nproperty_typeEntire loft                         3.042e+01  4.208e+02   0.072\nproperty_typeEntire place                       -8.588e+02  5.709e+02  -1.504\nproperty_typeEntire rental unit                 -8.266e+02  4.044e+02  -2.044\nproperty_typeEntire serviced apartment          -1.025e+03  4.128e+02  -2.483\nproperty_typeEntire townhouse                   -7.308e+02  4.082e+02  -1.790\nproperty_typeEntire vacation home               -6.580e+02  6.377e+02  -1.032\nproperty_typeEntire villa                       -8.647e+02  4.150e+02  -2.084\nproperty_typeFarm stay                          -9.280e+02  6.381e+02  -1.454\nproperty_typeHouseboat                          -9.794e+02  4.943e+02  -1.981\nproperty_typePrivate room                       -1.078e+02  8.062e+02  -0.134\nproperty_typePrivate room in barn               -1.345e+03  8.063e+02  -1.669\nproperty_typePrivate room in bed and breakfast  -8.397e+02  4.957e+02  -1.694\nproperty_typePrivate room in boat               -1.330e+03  8.068e+02  -1.648\nproperty_typePrivate room in bungalow           -1.040e+03  8.096e+02  -1.285\nproperty_typePrivate room in cabin              -1.272e+03  8.075e+02  -1.575\nproperty_typePrivate room in casa particular    -1.469e+03  6.377e+02  -2.303\nproperty_typePrivate room in condo              -1.279e+03  4.087e+02  -3.130\nproperty_typePrivate room in guest suite        -1.291e+03  5.341e+02  -2.418\nproperty_typePrivate room in guesthouse         -9.407e+02  8.091e+02  -1.163\nproperty_typePrivate room in home               -1.216e+03  4.151e+02  -2.930\nproperty_typePrivate room in hostel             -1.260e+03  5.745e+02  -2.192\nproperty_typePrivate room in loft               -1.068e+03  6.394e+02  -1.670\nproperty_typePrivate room in rental unit        -1.216e+03  4.060e+02  -2.996\nproperty_typePrivate room in shipping container -1.502e+03  5.719e+02  -2.627\nproperty_typePrivate room in townhouse          -1.248e+03  4.607e+02  -2.709\nproperty_typePrivate room in villa              -1.189e+03  4.564e+02  -2.604\nproperty_typeRoom in aparthotel                 -1.271e+03  4.674e+02  -2.720\nproperty_typeRoom in boutique hotel             -1.176e+03  8.065e+02  -1.459\nproperty_typeRoom in hostel                     -1.847e+03  5.349e+02  -3.452\nproperty_typeRoom in hotel                      -1.215e+03  4.549e+02  -2.670\nproperty_typeShared room in condo               -1.586e+03  8.066e+02  -1.967\nproperty_typeTiny home                          -9.525e+02  5.348e+02  -1.781\nproperty_typeTower                              -1.067e+03  8.060e+02  -1.324\naccommodates                                     2.067e+02  6.229e+00  33.185\nbathrooms                                        5.080e+02  3.284e+01  15.467\navailability_30                                  8.265e+00  9.153e-01   9.030\navailability_365                                 5.532e-01  8.678e-02   6.375\nnumber_of_reviews_l30d                          -7.778e+01  8.102e+00  -9.600\nreview_scores_accuracy                           1.335e+02  5.044e+01   2.647\nreview_scores_cleanliness                        1.321e+02  3.152e+01   4.191\nreview_scores_communication                     -1.403e+02  5.308e+01  -2.643\nreview_scores_location                           7.214e+01  4.667e+01   1.546\ninstant_bookableTRUE                             8.477e+01  3.344e+01   2.535\n                                                Pr(&gt;|t|)    \n(Intercept)                                     0.571495    \nhost_is_superhostTRUE                           0.032162 *  \nneighbourhood_cleansedAmager Vest               0.137393    \nneighbourhood_cleansedBispebjerg                0.003923 ** \nneighbourhood_cleansedBrnshj-Husum              1.94e-05 ***\nneighbourhood_cleansedFrederiksberg             0.001706 ** \nneighbourhood_cleansedIndre By                   &lt; 2e-16 ***\nneighbourhood_cleansedNrrebro                   0.038751 *  \nneighbourhood_cleansedsterbro                   0.002184 ** \nneighbourhood_cleansedValby                     0.000324 ***\nneighbourhood_cleansedVanlse                    1.48e-05 ***\nneighbourhood_cleansedVesterbro-Kongens Enghave 0.000401 ***\nproperty_typeEntire bungalow                    0.012819 *  \nproperty_typeEntire cabin                       0.124374    \nproperty_typeEntire condo                       0.053795 .  \nproperty_typeEntire cottage                     0.059169 .  \nproperty_typeEntire guest suite                 0.047783 *  \nproperty_typeEntire guesthouse                  0.077700 .  \nproperty_typeEntire home                        0.025905 *  \nproperty_typeEntire loft                        0.942377    \nproperty_typeEntire place                       0.132566    \nproperty_typeEntire rental unit                 0.040979 *  \nproperty_typeEntire serviced apartment          0.013048 *  \nproperty_typeEntire townhouse                   0.073466 .  \nproperty_typeEntire vacation home               0.302196    \nproperty_typeEntire villa                       0.037223 *  \nproperty_typeFarm stay                          0.145885    \nproperty_typeHouseboat                          0.047594 *  \nproperty_typePrivate room                       0.893659    \nproperty_typePrivate room in barn               0.095262 .  \nproperty_typePrivate room in bed and breakfast  0.090340 .  \nproperty_typePrivate room in boat               0.099401 .  \nproperty_typePrivate room in bungalow           0.198782    \nproperty_typePrivate room in cabin              0.115211    \nproperty_typePrivate room in casa particular    0.021302 *  \nproperty_typePrivate room in condo              0.001758 ** \nproperty_typePrivate room in guest suite        0.015622 *  \nproperty_typePrivate room in guesthouse         0.244965    \nproperty_typePrivate room in home               0.003405 ** \nproperty_typePrivate room in hostel             0.028384 *  \nproperty_typePrivate room in loft               0.095008 .  \nproperty_typePrivate room in rental unit        0.002745 ** \nproperty_typePrivate room in shipping container 0.008641 ** \nproperty_typePrivate room in townhouse          0.006765 ** \nproperty_typePrivate room in villa              0.009226 ** \nproperty_typeRoom in aparthotel                 0.006548 ** \nproperty_typeRoom in boutique hotel             0.144704    \nproperty_typeRoom in hostel                     0.000559 ***\nproperty_typeRoom in hotel                      0.007593 ** \nproperty_typeShared room in condo               0.049280 *  \nproperty_typeTiny home                          0.074926 .  \nproperty_typeTower                              0.185452    \naccommodates                                     &lt; 2e-16 ***\nbathrooms                                        &lt; 2e-16 ***\navailability_30                                  &lt; 2e-16 ***\navailability_365                                1.96e-10 ***\nnumber_of_reviews_l30d                           &lt; 2e-16 ***\nreview_scores_accuracy                          0.008147 ** \nreview_scores_cleanliness                       2.82e-05 ***\nreview_scores_communication                     0.008230 ** \nreview_scores_location                          0.122220    \ninstant_bookableTRUE                            0.011258 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 697 on 6400 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.4054,    Adjusted R-squared:  0.3997 \nF-statistic: 71.53 on 61 and 6400 DF,  p-value: &lt; 2.2e-16\n\n\nTo evaluate the model, we first looked at its r-squared. The r-squared is 0.4019 which is not a super strong r-squared value or sign of a super great model, but this was better than other iterations of the model we tried (ex: without property type). Because the model has so many predictors, we also looked at the adjusted r-squared which is 0.3966 and also not too good. There is also a residual standard error of 704.3. However, the f-statistic is 75.52 so quite large and combined with a pretty small p-value, which would suggest that the model is statistically significant. In other words, the model doesn’t explain a lot of the variation within price, but the predictors are predictors that matter.\nThere are many possible reasons for why the model is not performing as well as we would have liked. For instance, while we did include all the numerical variables initially, we removed some after checking for multicollinearity. Perhaps we removed some that are actually key to predicting price. Additionally, one  big reason is that Airbnb pricing is very nuanced based on human perception. There are things like decor, vibe, or even the host’s personal financial situation that just can’t be captured in a model."
  },
  {
    "objectID": "projects/airbnb/airbnb-copenhagen.html#classification",
    "href": "projects/airbnb/airbnb-copenhagen.html#classification",
    "title": "Airbnb Analysis in Copenhagen",
    "section": "Classification",
    "text": "Classification\n\nK-Nearest Neighbors\nIn this session, we want to explore whether or not rentals in Copenhagen have a combination of amenities: dining tables and wine glasses. This is interesting to us because dining tables and wine glasses can add a sense of ‘home’ in the rental and we want to know what kinds of rentals are more likely to provide a combination of these 2 amenities. To do this, we hand-picked several numeric variables to conduct a K-nearest neighbors analysis.\n\nlibrary(caret)\nlibrary(FNN)\n\n\ncolumn_names = data.frame(colnames(copenhagen))\n\nnumeric_df = copenhagen %&gt;% select_if(is.numeric)\ncharacter_df = copenhagen %&gt;% select_if(is.character)\n\nWe first selected necessary numeric columns and created relevant data frames:\n\n# Use variables price, bedrooms, accommodates, review_scores_value, review_scores_rating, review_scores_cleanliness, reviews_per_month to predict whether the room has both dining table and wine glasses\ndata_knn_1 = copenhagen %&gt;%\n  select(price, \n         bedrooms, \n         accommodates, \n         review_scores_value,\n         review_scores_rating,\n         review_scores_cleanliness,\n         reviews_per_month,\n         amenities)\n\n# 2. Omit all the NA values\ndata_knn_1 = na.omit(data_knn_1)\n\n# 3. Create target variable: have both dining table and wine glasses\ndata_knn_1 = data_knn_1 %&gt;%\n  mutate(Dine_Wine = grepl(\"Dining table\", amenities, ignore.case = TRUE) \n         & grepl(\"Wine glasses\", amenities, ignore.case = TRUE))\ntable(data_knn_1$Dine_Wine)\n\n\nFALSE  TRUE \n 6162  4610 \n\ndata_knn_1$Dine_Wine = as.factor(data_knn_1$Dine_Wine)\n\nWe then partitioned the data and performed t-tests to test whether there are significant differences among groups.\n\n# 4. Data Partition\nset.seed(5)\ntrain.index = sample(c(1:nrow(data_knn_1)), nrow(data_knn_1)*0.6)\ntrain.set = data_knn_1[train.index,]\nvalid.set = data_knn_1[-train.index,]\n\n# 5. Doing t-tests to test the significance of input variables\ntrain.true = train.set %&gt;%\n  filter(Dine_Wine == 'TRUE')\n\ntrain.false = train.set %&gt;%\n  filter(Dine_Wine == 'FALSE')\n\nprice_test = t.test(train.true$price, \n                    train.false$price)\nprice_test\n\n\n    Welch Two Sample t-test\n\ndata:  train.true$price and train.false$price\nt = 7.4873, df = 5480.9, p-value = 8.148e-14\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 118.7054 202.9153\nsample estimates:\nmean of x mean of y \n 1446.102  1285.292 \n\nbedrooms_test = t.test(train.true$bedrooms, \n                       train.false$bedrooms)\nbedrooms_test\n\n\n    Welch Two Sample t-test\n\ndata:  train.true$bedrooms and train.false$bedrooms\nt = 4.8844, df = 5644, p-value = 1.066e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.06715592 0.15720351\nsample estimates:\nmean of x mean of y \n 1.668986  1.556806 \n\naccommadates_test = t.test(train.true$accommodates, \n                           train.false$accommodates)\naccommadates_test\n\n\n    Welch Two Sample t-test\n\ndata:  train.true$accommodates and train.false$accommodates\nt = 8.8907, df = 5521.9, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.3007682 0.4709262\nsample estimates:\nmean of x mean of y \n 3.595386  3.209539 \n\nreview_value_test = t.test(train.true$review_scores_value, \n                           train.false$review_scores_value)\nreview_value_test\n\n\n    Welch Two Sample t-test\n\ndata:  train.true$review_scores_value and train.false$review_scores_value\nt = 5.9718, df = 6407.7, p-value = 2.473e-09\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.03065759 0.06062144\nsample estimates:\nmean of x mean of y \n 4.743603  4.697964 \n\nreview_rate_test = t.test(train.true$review_scores_rating, \n                          train.false$review_scores_rating)\nreview_rate_test\n\n\n    Welch Two Sample t-test\n\ndata:  train.true$review_scores_rating and train.false$review_scores_rating\nt = 7.243, df = 6448, p-value = 4.901e-13\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.03447898 0.06006856\nsample estimates:\nmean of x mean of y \n 4.858078  4.810804 \n\nreview_clean_test = t.test(train.true$review_scores_cleanliness, \n                           train.false$review_scores_cleanliness)\nreview_clean_test\n\n\n    Welch Two Sample t-test\n\ndata:  train.true$review_scores_cleanliness and train.false$review_scores_cleanliness\nt = 7.8811, df = 6393.9, p-value = 3.787e-15\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.05305877 0.08819355\nsample estimates:\nmean of x mean of y \n 4.754083  4.683457 \n\nreview_per_test = t.test(train.true$reviews_per_month, \n                         train.false$reviews_per_month)\nreview_per_test\n\n\n    Welch Two Sample t-test\n\ndata:  train.true$reviews_per_month and train.false$reviews_per_month\nt = 3.5581, df = 3636, p-value = 0.0003783\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.07436031 0.25688435\nsample estimates:\nmean of x mean of y \n 1.032160  0.866538 \n\n\nAll variables passed the t-test and show significant difference # between true and false groups. We then create a KNN model and try to predict a hypothetical case:\n\n# 6. Normalize the data\nnorm.values = preProcess(train.set[, 1:ncol(train.set)],\n                         method = c(\"center\", \"scale\"))\ntrain.norm.set = predict(norm.values, train.set[, 1:ncol(train.set)])\nvalid.norm.set = predict(norm.values, valid.set[, 1:ncol(valid.set)])\nknn.norm.raw = predict(norm.values, data_knn_1[, 1:ncol(data_knn_1)])\n\n# Build a KNN model to predict a new record (k=7)\nnew_rental = data.frame(price = 2800,\n                        bedrooms = 4,\n                        accommodates = 8,\n                        review_scores_value = 4.85,\n                        review_scores_rating = 4.90,\n                        review_scores_cleanliness = 4.80,\n                        reviews_per_month = 0.50)\nnew_norm_rental = predict(norm.values, new_rental)\n\nnn = knn(train = train.norm.set[, 1:7],\n         test = new_norm_rental,\n         cl = train.norm.set[[9]],\n         k = 7)\nnn\n\n[1] FALSE\nattr(,\"nn.index\")\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,] 1696 1560 1249 5855 5112 3773 2516\nattr(,\"nn.dist\")\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,] 0.3578159 0.4349341 0.7493476 0.7678441 0.7806102 0.8262365 0.8519238\nLevels: FALSE\n\n# 7. Try to find the optimal k-value\naccuracy.df = data.frame(k = seq(1,20,1), accuracy = rep(0,20))\n\nfor (i in 1:20) {\n  knn.pred = knn(train.norm.set[, 1:7],\n                 valid.norm.set[, 1:7],\n                 cl = train.norm.set[[9]],\n                 k = i)\n  accuracy.df[i,2] = confusionMatrix(knn.pred, valid.norm.set[[9]])$overall[1]\n}\n\naccuracy.df\n\n    k  accuracy\n1   1 0.5265723\n2   2 0.5497795\n3   3 0.5465305\n4   4 0.5613832\n5   5 0.5551172\n6   6 0.5685774\n7   7 0.5592945\n8   8 0.5695057\n9   9 0.5676491\n10 10 0.5801810\n11 11 0.5727547\n12 12 0.5818055\n13 13 0.5797169\n14 14 0.5794848\n15 15 0.5834300\n16 16 0.5783244\n17 17 0.5808772\n18 18 0.5841262\n19 19 0.5848225\n20 20 0.5815735\n\naccuracy.df %&gt;%\n  filter(accuracy == max(accuracy, na.rm = TRUE))\n\n   k  accuracy\n1 19 0.5848225\n\n# 8. It seems that k=16 gives the best result, so run knn again\nnn_2 = knn(train = train.norm.set[, 1:7],\n           test = new_norm_rental,\n           cl = train.norm.set[[9]],\n           k = 16)\nnn_2\n\n[1] FALSE\nattr(,\"nn.index\")\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n[1,] 1696 1560 1249 5855 5112 3773 2516 5570  195  2010  6449  4066  3044  2635\n     [,15] [,16]\n[1,]  4398  4491\nattr(,\"nn.dist\")\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,] 0.3578159 0.4349341 0.7493476 0.7678441 0.7806102 0.8262365 0.8519238\n          [,8]      [,9]     [,10]     [,11]    [,12]    [,13]    [,14]\n[1,] 0.8555788 0.8748228 0.8776053 0.9320734 1.004684 1.005562 1.042803\n        [,15]    [,16]\n[1,] 1.057603 1.057974\nLevels: FALSE\n\n\nWe then evaluated the model’s performance compared to the naive benchmark:\n\n# 9. Compare again the naive benchmark\nknn_pred = knn(\n  train = train.norm.set[, 1:7],\n  test = valid.norm.set[, 1:7],\n  cl = train.norm.set[[9]],\n  k = 16\n)\nmean(knn_pred == valid.norm.set[[9]])\n\n[1] 0.5783244\n\nmost_common = names(which.max(table(train.norm.set[[9]])))\nnaive_pred = rep(most_common, nrow(valid.norm.set))\nmean(naive_pred == valid.norm.set[[9]])\n\n[1] 0.5639359\n\n\nAs a summary, here is a recap of the KNN model building process: 1. Amenities and Predictors Choices: as illustrated in the beginning, our team wants to predict whether rentals in Copenhagen have a combination of dining tables and wine glasses in the rentals or not. Seven predictors were chosen: price, bedrooms, accommodates, review_scores_value, review_scores_rating, review_scores_cleanliness, reviews_per_month. Those predictors were chosen because we predict that higher prices, more bedrooms, and more accommodation capacity can indicate that the rentals are more high-end and therefore should have both dining tables and wine glasses. It is also inferred that the better review scores are, the more likely the rentals will have the combination, as better reviews can indicate good amenity supply and great environments. The hypothetical rental (with price = 2800, 4 bedrooms, 8 accommodation capacity, etc.) is classified by the model as a “TRUE” case, meaning that it is likely to have the combination of dining tables and wine glasses.\nFor model assessment: we used an iteration method to try to find the optimal k-value by comparing the performance of using different k-values on the validation data. We chose a range of 1 to 20 for the k-value and tested the accuracy of those values on the validation set. The iteration process showed us that when k = 16, the accuracy of the model is maximized (0.5812065, or 58.12%), so the k = 16 value is decided. We then compared the accuracy of our model to the naive method (i.e. simply predict if the new rental belongs to the most class in our dataset), and found that our model’s accuracy is 0.84% point better (or 1.47% better) than the naive method. Although this may not seem to be a huge improvement, when rental data gets large, our model can predict more accurately  than the naive method.\n\n\nNaive Bayes\nIn this part our team was tasked with predicting the binned version of the review_scores_value variable. We selected four categorical variables as the input variables:\n\nlibrary(e1071)\n\n\n# 1. Selecting input categorical variables\nnaive_raw = copenhagen %&gt;%\n  select(neighbourhood_cleansed,\n         property_type,\n         room_type,\n         bathrooms_text,\n         review_scores_value)\n\n# 2. Binning the response variable review_score_value\nnaive_raw &lt;- naive_raw %&gt;%\n  mutate(\n    value_bin = ntile(review_scores_value, 3),\n    value_label = case_when(\n      value_bin == 1 ~ \"not good\",\n      value_bin == 2 ~ \"medium\",\n      value_bin == 3 ~ \"good\"\n    )\n  )\n\n# 3. Look at the data and do necessary adjustments\nstr(naive_raw)\n\ntibble [10,774 × 7] (S3: tbl_df/tbl/data.frame)\n $ neighbourhood_cleansed: chr [1:10774] \"Indre By\" \"Amager Vest\" \"Nrrebro\" \"Indre By\" ...\n $ property_type         : chr [1:10774] \"Entire condo\" \"Entire condo\" \"Private room in condo\" \"Entire condo\" ...\n $ room_type             : chr [1:10774] \"Entire home/apt\" \"Entire home/apt\" \"Private room\" \"Entire home/apt\" ...\n $ bathrooms_text        : chr [1:10774] \"1 bath\" \"2 baths\" \"1 shared bath\" \"1.5 baths\" ...\n $ review_scores_value   : num [1:10774] 4.89 4.66 4.6 5 4.76 4.9 4.68 4.79 4.89 4.69 ...\n $ value_bin             : int [1:10774] 3 1 1 3 2 3 2 2 3 2 ...\n $ value_label           : chr [1:10774] \"good\" \"not good\" \"not good\" \"good\" ...\n\nsapply(naive_raw, n_distinct)\n\nneighbourhood_cleansed          property_type              room_type \n                    11                     45                      4 \n        bathrooms_text    review_scores_value              value_bin \n                    20                    130                      3 \n           value_label \n                     3 \n\nnaive_raw %&gt;%\n  count(property_type, sort = TRUE)\n\n# A tibble: 45 × 2\n   property_type                   n\n   &lt;chr&gt;                       &lt;int&gt;\n 1 Entire rental unit           6055\n 2 Entire condo                 2681\n 3 Private room in rental unit   580\n 4 Entire home                   481\n 5 Entire townhouse              213\n 6 Private room in condo         205\n 7 Entire serviced apartment     124\n 8 Entire villa                   89\n 9 Private room in home           83\n10 Entire loft                    66\n# ℹ 35 more rows\n\nnaive_raw %&gt;%\n  count(bathrooms_text, sort = TRUE)\n\n# A tibble: 20 × 2\n   bathrooms_text        n\n   &lt;chr&gt;             &lt;int&gt;\n 1 1 bath             8563\n 2 1.5 baths           598\n 3 2 baths             597\n 4 1 shared bath       583\n 5 1 private bath      141\n 6 Half-bath            62\n 7 2.5 baths            50\n 8 1.5 shared baths     38\n 9 2 shared baths       38\n10 0 baths              31\n11 3 baths              29\n12 Shared half-bath     13\n13 0 shared baths       12\n14 Private half-bath     7\n15 3.5 baths             4\n16 &lt;NA&gt;                  3\n17 5 baths               2\n18 2.5 shared baths      1\n19 3 shared baths        1\n20 4 baths               1\n\nnaive_raw %&gt;%\n  count(neighbourhood_cleansed, sort = TRUE)\n\n# A tibble: 11 × 2\n   neighbourhood_cleansed        n\n   &lt;chr&gt;                     &lt;int&gt;\n 1 Vesterbro-Kongens Enghave  1858\n 2 Indre By                   1818\n 3 Nrrebro                    1764\n 4 Frederiksberg              1130\n 5 sterbro                    1108\n 6 Amager Vest                 989\n 7 Amager st                   839\n 8 Bispebjerg                  478\n 9 Valby                       381\n10 Vanlse                      242\n11 Brnshj-Husum                167\n\n# To keep top 10 values for property_type and bathrooms_text\ntop10_col1 = naive_raw %&gt;%\n  count(property_type, sort = TRUE) %&gt;%\n  slice_head(n = 10)\n\ntop10_col2 = naive_raw %&gt;%\n  count(bathrooms_text, sort = TRUE) %&gt;%\n  slice_head(n = 10)\n\nnaive_clean = naive_raw %&gt;%\n  filter(\n    property_type %in% top10_col1$property_type,\n    bathrooms_text %in% top10_col2$bathrooms_text\n  )\n\nsapply(naive_clean, n_distinct)\n\nneighbourhood_cleansed          property_type              room_type \n                    11                     10                      2 \n        bathrooms_text    review_scores_value              value_bin \n                    10                    128                      3 \n           value_label \n                     3 \n\n# delete NA value in value_label and delete the original value columns\nnaive_clean = naive_clean %&gt;%\n  filter(!is.na(value_label))\nsapply(naive_clean, n_distinct)\n\nneighbourhood_cleansed          property_type              room_type \n                    11                     10                      2 \n        bathrooms_text    review_scores_value              value_bin \n                    10                    128                      3 \n           value_label \n                     3 \n\nnaive_clean = naive_clean %&gt;%\n  select(\n    -review_scores_value,\n    -value_bin\n  )\n\nnaive_clean$value_label = as.factor(naive_clean$value_label)\nclass(naive_clean$value_label)\n\n[1] \"factor\"\n\n\nWe then made several graphs to get a basic sense of how the input variables impact the response binned variable:\n\n# 4. Visualize value_label by different input variables\npro_chart_function = function(i) {\n  df = naive_clean %&gt;%\n    count(!!sym(i), value_label) %&gt;%\n    group_by(!!sym(i)) %&gt;%\n    mutate(proportion = n / sum(n)) %&gt;%\n    ungroup()\n  \n  chart = ggplot(df, aes(x = !!sym(i), y = proportion, fill = value_label)) +\n    geom_bar(stat = \"identity\", position = \"fill\") +\n    labs(y = \"Proportion\", x = paste(i), title = paste(\"Proportional Bar Plot for Consumer Value Perception by\", i)) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  return(chart)\n}\n\npro_chart_function('neighbourhood_cleansed')\n\n\n\n\n\n\n\npro_chart_function('property_type')\n\n\n\n\n\n\n\npro_chart_function('room_type')\n\n\n\n\n\n\n\npro_chart_function('bathrooms_text')\n\n\n\n\n\n\n\n\nIt seems that all of our input variables have some impact on the response variable (by showing results differences among different categories which are within the same input variables). For example, we can see that the Entire Serviced Apartment property type seems to be prone to generating customer reviews which are “not so good”. To assess the results in a more sophisticated way, we then build a naive bayes model:\n\n# 5. Data Partitioning\nset.seed(5)\ntrain.index_naive = sample(c(1:nrow(naive_clean)), \n                           nrow(naive_clean)*0.6)\ntrain.set_naive = naive_clean[train.index_naive, ]\nvalid.set_naive = naive_clean[-train.index_naive, ]\n\n# 6. Build the Naive Bayes Model\nvalue.nb = naiveBayes(value_label ~ ., \n                      data = train.set_naive,\n                      laplace = 1) # Laplace (add-one) smoothing\nvalue.nb\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n     good    medium  not good \n0.3377179 0.3304279 0.3318542 \n\nConditional probabilities:\n          neighbourhood_cleansed\nY           Amager st Amager Vest Bispebjerg Brnshj-Husum Frederiksberg\n  good     0.07320507  0.09948381 0.04880338   0.01736274    0.11168466\n  medium   0.07146283  0.09448441 0.03741007   0.01438849    0.11175060\n  not good 0.08404967  0.07784145 0.05253104   0.01432665    0.10171920\n          neighbourhood_cleansed\nY            Indre By    Nrrebro    sterbro      Valby     Vanlse\n  good     0.13561708 0.16518067 0.10980760 0.03472548 0.02205537\n  medium   0.17985612 0.17314149 0.09928058 0.03069544 0.01678657\n  not good 0.18863419 0.17430755 0.10506208 0.03581662 0.02196753\n          neighbourhood_cleansed\nY          Vesterbro-Kongens Enghave\n  good                    0.18723604\n  medium                  0.17601918\n  not good                0.14899713\n\n          property_type\nY          Entire condo Entire home Entire loft Entire rental unit\n  good      0.243078367 0.054434538 0.002815580        0.580947912\n  medium    0.292086331 0.037889688 0.011031175        0.534772182\n  not good  0.232091691 0.037726839 0.007163324        0.603629417\n          property_type\nY          Entire serviced apartment Entire townhouse Entire villa\n  good                   0.003284843      0.024401689  0.010793055\n  medium                 0.011510791      0.020143885  0.008153477\n  not good               0.025310411      0.020057307  0.008595989\n          property_type\nY          Private room in condo Private room in home\n  good               0.017832004          0.008916002\n  medium             0.025419664          0.008153477\n  not good           0.013371538          0.009551098\n          property_type\nY          Private room in rental unit\n  good                     0.058188644\n  medium                   0.055635492\n  not good                 0.047277937\n\n          room_type\nY          Entire home/apt Private room\n  good          0.91694040   0.08399812\n  medium        0.91270983   0.08824940\n  not good      0.93170965   0.06924546\n\n          bathrooms_text\nY              0 baths      1 bath 1 private bath 1 shared bath   1.5 baths\n  good     0.004692633 0.794931957    0.011262318   0.051149695 0.058657907\n  medium   0.002398082 0.799520384    0.014868106   0.060431655 0.058513189\n  not good 0.002865330 0.833333333    0.008118434   0.041547278 0.049188157\n          bathrooms_text\nY          1.5 shared baths     2 baths 2 shared baths   2.5 baths   Half-bath\n  good          0.003284843 0.066635382    0.002815580 0.005631159 0.005631159\n  medium        0.004796163 0.050359712    0.002398082 0.006235012 0.005275779\n  not good      0.001910220 0.049665712    0.004775549 0.005253104 0.008118434\n\n\nA fictional rental scenario we made up is located in Valby, is the entire home, and has 1.5 shared baths. Our model shows that the A-priori probabilities for the 3 target bins are 0.3374610 (good), 0.3334521 (medium) and 0.3290869 (not good), respectively. The model predicts that our fictional rental (which is in the neighborhood Valby, is the Entire Home property type, is the Entire home/apt room type, and has 1.5 shared baths) will be likely to receive “Good” reviews from customers.\n\n# 7. Make a prediction based on a fictional rental scenario\nfiction_rental = data.frame(\n  neighbourhood_cleansed = 'Valby',\n  property_type = 'Entire home',\n  room_type = 'Entire home/apt',\n  bathrooms_text = '1.5 shared baths'\n)\n\nfic.pred = predict(value.nb, fiction_rental)\nfic.pred\n\n[1] good\nLevels: good medium not good\n\n\nNext, we assessed our model using the validation set:\n\n# 8. Assessing the model\ntrain_pred = predict(value.nb, train.set_naive)\ntrain_matrix = confusionMatrix(train_pred, train.set_naive$value_label)\ntrain_accuracy = train_matrix$overall['Accuracy']\ntrain_accuracy\n\n Accuracy \n0.3904913 \n\ntrain_matrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction good medium not good\n  good      791    603      612\n  medium    608    750      559\n  not good  732    732      923\n\nOverall Statistics\n                                          \n               Accuracy : 0.3905          \n                 95% CI : (0.3784, 0.4027)\n    No Information Rate : 0.3377          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.0858          \n                                          \n Mcnemar's Test P-Value : 2.062e-07       \n\nStatistics by Class:\n\n                     Class: good Class: medium Class: not good\nSensitivity               0.3712        0.3597          0.4408\nSpecificity               0.7093        0.7238          0.6528\nPos Pred Value            0.3943        0.3912          0.3867\nNeg Pred Value            0.6887        0.6961          0.7015\nPrevalence                0.3377        0.3304          0.3319\nDetection Rate            0.1254        0.1189          0.1463\nDetection Prevalence      0.3179        0.3038          0.3783\nBalanced Accuracy         0.5402        0.5417          0.5468\n\nvalid_pred = predict(value.nb, valid.set_naive)\nvalid_matrix = confusionMatrix(valid_pred, valid.set_naive$value_label)\nvalid_accuracy = valid_matrix$overall['Accuracy']\nvalid_accuracy\n\n Accuracy \n0.3769011 \n\nvalid_matrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction good medium not good\n  good      490    466      416\n  medium    407    484      355\n  not good  498    480      612\n\nOverall Statistics\n                                          \n               Accuracy : 0.3769          \n                 95% CI : (0.3622, 0.3917)\n    No Information Rate : 0.3398          \n    P-Value [Acc &gt; NIR] : 2.564e-07       \n                                          \n                  Kappa : 0.066           \n                                          \n Mcnemar's Test P-Value : 1.343e-06       \n\nStatistics by Class:\n\n                     Class: good Class: medium Class: not good\nSensitivity               0.3513        0.3385          0.4425\nSpecificity               0.6865        0.7257          0.6538\nPos Pred Value            0.3571        0.3884          0.3849\nNeg Pred Value            0.6809        0.6806          0.7055\nPrevalence                0.3315        0.3398          0.3287\nDetection Rate            0.1164        0.1150          0.1454\nDetection Prevalence      0.3260        0.2961          0.3779\nBalanced Accuracy         0.5189        0.5321          0.5482\n\n\nThe confusion matrices for the training set and validation set show that the respective accuracy scores are 0.3846 (train set) and 0.3749 (valid set). The similar accuracy scores demonstrate that there is no overfitting concern in our model. As for the naive method, as we have 3 classes which are binned by the equal frequency method, the naive rates for all 3 classes will be around 33.33% (100%/3). In other words, our model will have a 12.48% improvement (using the 37.49% accuracy rate of the validation set) compared to the naive method, proving the value of our naive bayes model.\nIn summary, to predict the review_scores_value which shows how much value a customer sees in the rental experience, our team used an equal frequency binning method which splits the target variable into 3 bins – good, medium, and not good. We built a naive bayes model which uses the neighbourhood_cleansed, property_type, room_type, bathrooms_text variables (all categorical) as input variables, because we infer that these rental features are most likely to make a difference in determining a customer’s perceived value gained. Note that for both the property_type and bathrooms_text variables, only the 10 most frequent values are kept to simplify our analysis. Our model predicts that a fictional rental which is in the neighborhood Valby, is the Entire Home property type, is the Entire home/apt room type, and has 1.5 shared baths will probably receive “Good” reviews from customers.\nWhen assessing our model against both the training data and validation data, it has been found that the model’s accuracy is more than 37% in both cases. This means that we can get a 12.48% improvement compared to the naive method which predicts the probabilities to be 33.33% for all 3 bins. In other words, when taking values from the rental’s neighborhood, property, room and bathroom features, we are more confident than the naive method that we’ll predict customers’ value review types correctly.\n\n\nClassification Tree\nTo predict whether a rental is short-term or long-term, we used a classification tree model with the following features: host_is_superhost, neighbourhood_cleansed, host_identity_verified, property_type, room_type, accommodates, bathrooms, bedrooms, price, review_scores_rating, has_availability.\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\n\ntree_data &lt;- copenhagen %&gt;% \n  select(host_is_superhost,\n         neighbourhood_cleansed,\n         host_identity_verified, \n         property_type,\n         room_type, \n         accommodates,\n         bathrooms,\n         bedrooms,\n         # beds,\n         price, \n         review_scores_rating, \n         has_availability, \n         minimum_nights)\n\nsum(is.na(tree_data))\n\n[1] 0\n\n\nSince property_type had too many unique values, we restricted it to the top 10 most frequent property types.\n\ntop_10_property &lt;- tree_data %&gt;% \n  count(property_type, sort = TRUE) %&gt;% \n  slice(1:10)\n\ntree_data = tree_data %&gt;%\n  filter(property_type %in% top_10_property$property_type)\n\ntree_data$host_identity_verified &lt;- factor(tree_data$host_identity_verified)\ntree_data$property_type &lt;- factor(tree_data$property_type)\ntree_data$neighbourhood_cleansed &lt;- factor(tree_data$neighbourhood_cleansed)\ntree_data$host_is_superhost &lt;- factor(tree_data$host_is_superhost)\ntree_data$has_availability &lt;- factor(tree_data$has_availability)\n\nAfter cleaning the data, we binned minimum_nights into 3 categories: short(&lt;2 nights), medium(2-5 nights), and long (&gt;5 nights). What’s interesting, at the beginning we tried to bin minimum_nights into 2 categories, but observed a large number of extremely long stays (e.g., &gt;100 nights). Thus, using three categories provided a better balance between classes and more meaningful distinctions in rental behavior.\n\n# Bin the minimum nights into 3 bins\ntree_data$rental_type &lt;- cut(tree_data$minimum_nights,\n                             breaks = c(0, 2, 5, Inf),\n                             labels = c(\"Short\", \"Medium\", \"Long\"))\ntable(tree_data$rental_type)\n\n\n Short Medium   Long \n  4661   4915   1001 \n\ntree_data &lt;- tree_data %&gt;% select(-minimum_nights)\nsummary(tree_data)\n\n host_is_superhost               neighbourhood_cleansed host_identity_verified\n FALSE:8690        Vesterbro-Kongens Enghave:1839       FALSE:1013            \n TRUE :1887        Indre By                 :1775       TRUE :9564            \n                   Nrrebro                  :1753                             \n                   Frederiksberg            :1122                             \n                   sterbro                  :1102                             \n                   Amager Vest              : 948                             \n                   (Other)                  :2038                             \n                     property_type   room_type          accommodates   \n Entire rental unit         :6055   Length:10577       Min.   : 1.000  \n Entire condo               :2681   Class :character   1st Qu.: 2.000  \n Private room in rental unit: 580   Mode  :character   Median : 3.000  \n Entire home                : 481                      Mean   : 3.379  \n Entire townhouse           : 213                      3rd Qu.: 4.000  \n Private room in condo      : 205                      Max.   :16.000  \n (Other)                    : 362                                      \n   bathrooms        bedrooms         price       review_scores_rating\n Min.   :0.000   Min.   :0.000   Min.   :   72   Min.   :0.000       \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:  900   1st Qu.:4.750       \n Median :1.000   Median :1.000   Median : 1160   Median :4.910       \n Mean   :1.096   Mean   :1.604   Mean   : 1365   Mean   :4.831       \n 3rd Qu.:1.000   3rd Qu.:2.000   3rd Qu.: 1600   3rd Qu.:5.000       \n Max.   :5.000   Max.   :8.000   Max.   :20000   Max.   :5.000       \n                                                                     \n has_availability rental_type  \n FALSE:    1      Short :4661  \n TRUE :10576      Medium:4915  \n                  Long  :1001  \n                               \n                               \n                               \n                               \n\n# data partitioning\nset.seed(79)\ntree_data.index &lt;- sample(c(1:nrow(tree_data)), nrow(tree_data)*0.6)\ntrain_set &lt;- tree_data[tree_data.index, ]\nvalid_set &lt;- tree_data[-tree_data.index, ]\n\ntree_model &lt;- rpart(rental_type ~ .,\n                    method = 'class', \n                    data = train_set)\n\nrpart.plot(tree_model)\n\n\n\n\n\n\n\n\nOur initial tree was very shallow (only one level) and did not perform well. We applied cross-validation to select optimal cp value that minimizes prediction error. After pruning the tree using optimal cp, the final tree performed better: the training set’s accuracy is 56.35%, while the validation set’s is 54.15%. While the overall accuracy is lower, the smaller performance gap between training and validation data indicates that the pruned tree generalizes better and is less prone to overfitting.\n\n#cross validation\nfive_fold_cv &lt;- \n  rpart(rental_type ~ .,method = 'class', cp=0.00001, minsplit=5, xval=5,  data = train_set)\na &lt;- printcp(five_fold_cv)\n\n\nClassification tree:\nrpart(formula = rental_type ~ ., data = train_set, method = \"class\", \n    cp = 1e-05, minsplit = 5, xval = 5)\n\nVariables actually used in tree construction:\n[1] accommodates           bathrooms              bedrooms              \n[4] host_identity_verified host_is_superhost      neighbourhood_cleansed\n[7] price                  property_type          review_scores_rating  \n\nRoot node error: 3381/6346 = 0.53278\n\nn= 6346 \n\n           CP nsplit rel error  xerror     xstd\n1  8.3999e-02      0   1.00000 1.00000 0.011755\n2  2.6028e-02      1   0.91600 0.91600 0.011777\n3  4.4366e-03      3   0.86395 0.87578 0.011754\n4  3.8450e-03      4   0.85951 0.88287 0.011760\n5  2.3662e-03      6   0.85182 0.88169 0.011759\n6  2.2183e-03     12   0.83614 0.87814 0.011756\n7  2.0704e-03     14   0.83171 0.87844 0.011757\n8  1.7746e-03     16   0.82757 0.89086 0.011766\n9  1.4789e-03     19   0.82224 0.89796 0.011770\n10 1.4131e-03     22   0.81781 0.89707 0.011769\n11 1.3310e-03     31   0.80509 0.89737 0.011769\n12 1.2817e-03     35   0.79976 0.89737 0.011769\n13 1.1831e-03     40   0.79326 0.89825 0.011770\n14 8.8731e-04     45   0.78734 0.89737 0.011769\n15 8.0281e-04     86   0.73499 0.89737 0.011769\n16 7.8872e-04     93   0.72937 0.90121 0.011772\n17 7.6900e-04     97   0.72612 0.90121 0.011772\n18 7.3943e-04    108   0.71251 0.90121 0.011772\n19 6.9013e-04    118   0.70453 0.90565 0.011774\n20 5.9154e-04    128   0.69713 0.91423 0.011777\n21 5.3239e-04    186   0.66075 0.92754 0.011780\n22 5.1760e-04    195   0.65513 0.92931 0.011780\n23 5.0281e-04    216   0.64360 0.92990 0.011780\n24 4.9295e-04    232   0.63472 0.92990 0.011780\n25 4.7323e-04    256   0.62082 0.92931 0.011780\n26 4.4366e-04    290   0.60367 0.92961 0.011780\n27 3.9436e-04    340   0.58089 0.93286 0.011781\n28 3.6971e-04    398   0.55546 0.93375 0.011781\n29 3.5492e-04    408   0.55161 0.95327 0.011779\n30 3.3274e-04    416   0.54865 0.95327 0.011779\n31 2.9577e-04    426   0.54481 0.95652 0.011779\n32 2.5880e-04    820   0.40994 0.96510 0.011776\n33 2.4648e-04    840   0.40402 0.96510 0.011776\n34 2.3662e-04    850   0.40106 0.96717 0.011775\n35 2.3004e-04    877   0.39456 0.96717 0.011775\n36 2.2183e-04    886   0.39249 0.96747 0.011775\n37 1.9718e-04    925   0.38332 0.97338 0.011773\n38 1.8486e-04   1008   0.36616 0.97338 0.011773\n39 1.7746e-04   1018   0.36409 0.98344 0.011767\n40 1.6901e-04   1028   0.36202 0.98344 0.011767\n41 1.4789e-04   1048   0.35759 0.98492 0.011766\n42 1.1831e-04   1138   0.34369 0.98521 0.011766\n43 9.8590e-05   1145   0.34280 0.98906 0.011764\n44 7.3943e-05   1172   0.34014 0.99024 0.011763\n45 6.5727e-05   1176   0.33984 0.98994 0.011763\n46 5.9154e-05   1185   0.33925 0.98994 0.011763\n47 1.0000e-05   1195   0.33866 0.98994 0.011763\n\n\n\n# Re-build model with optimal CP value\npruned.ct &lt;- prune(five_fold_cv,\n                   cp=five_fold_cv$cptable[which.min(five_fold_cv$cptable[,\"xerror\"]),\"CP\"])\nrpart.plot(pruned.ct,type=5, extra = 2, fallen.leaves=FALSE)\n\n\n\n\n\n\n\nhead(rpart.rules(pruned.ct))\n\n rental_type  Sho Med Lon                                                                                                                                                               \n       Short [.53 .39 .07] when property_type is                                                                                        Entire rental unit & review_scores_rating &lt;  4.8\n       Short [.69 .20 .11] when property_type is Entire serviced apartment or Private room in condo or Private room in home or Private room in rental unit                              \n      Medium [.41 .49 .10] when property_type is                                                                                        Entire rental unit & review_scores_rating &gt;= 4.8\n      Medium [.33 .56 .10] when property_type is                            Entire condo or Entire home or Entire loft or Entire townhouse or Entire villa                              \n\n#assessing model\n# Training Set\ntrain.pred &lt;- predict(pruned.ct, train_set, type=\"class\")\nconfusionMatrix(train.pred, train_set$rental_type)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Short Medium Long\n    Short   1071    611  161\n    Medium  1693   2354  456\n    Long       0      0    0\n\nOverall Statistics\n                                         \n               Accuracy : 0.5397         \n                 95% CI : (0.5274, 0.552)\n    No Information Rate : 0.4672         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.1507         \n                                         \n Mcnemar's Test P-Value : &lt; 2.2e-16      \n\nStatistics by Class:\n\n                     Class: Short Class: Medium Class: Long\nSensitivity                0.3875        0.7939     0.00000\nSpecificity                0.7845        0.3644     1.00000\nPos Pred Value             0.5811        0.5228         NaN\nNeg Pred Value             0.6240        0.6685     0.90277\nPrevalence                 0.4355        0.4672     0.09723\nDetection Rate             0.1688        0.3709     0.00000\nDetection Prevalence       0.2904        0.7096     0.00000\nBalanced Accuracy          0.5860        0.5792     0.50000\n\n# Validation Set\nvalid.pred &lt;- predict(pruned.ct, valid_set, type=\"class\")\nconfusionMatrix(valid.pred, valid_set$rental_type)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Short Medium Long\n    Short    704    416   91\n    Medium  1193   1534  293\n    Long       0      0    0\n\nOverall Statistics\n                                          \n               Accuracy : 0.529           \n                 95% CI : (0.5138, 0.5441)\n    No Information Rate : 0.4609          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.132           \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n\nStatistics by Class:\n\n                     Class: Short Class: Medium Class: Long\nSensitivity                0.3711        0.7867     0.00000\nSpecificity                0.7828        0.3485     1.00000\nPos Pred Value             0.5813        0.5079         NaN\nNeg Pred Value             0.6050        0.6565     0.90924\nPrevalence                 0.4484        0.4609     0.09076\nDetection Rate             0.1664        0.3626     0.00000\nDetection Prevalence       0.2862        0.7138     0.00000\nBalanced Accuracy          0.5769        0.5676     0.50000\n\n\nProperty type, price, and number of bedrooms are among the most influential variables in predicting whether a rental is short-, medium-, or long-term. However, since the overall model performance is not so high, it suggests that rental duration is hard to predict based on listing features alone. Other factors — such as individual traveler preferences, trip purpose, or external events — likely play a major role in determining how long guests stay, but these factors are not captured in the dataset."
  },
  {
    "objectID": "projects/airbnb/airbnb-copenhagen.html#clustering",
    "href": "projects/airbnb/airbnb-copenhagen.html#clustering",
    "title": "Airbnb Analysis in Copenhagen",
    "section": "Clustering",
    "text": "Clustering\nTo cluster properties we used 7 features and created new variable “price per bedroom”. The neighborhood_cleansed and room_type columns were listed as categorical variables, so we converted them to factors and assigned them dummy values, since k-means clustering requires all input features to be numeric.\n\n# Select features\nfeatures &lt;- copenhagen %&gt;%\n  select(price, number_of_reviews, review_scores_rating, minimum_nights, bedrooms,\n         neighbourhood_cleansed, room_type)\n\n#  Check NAs\ncolSums(is.na(features))\n\n                 price      number_of_reviews   review_scores_rating \n                     0                      0                      0 \n        minimum_nights               bedrooms neighbourhood_cleansed \n                     0                      0                      0 \n             room_type \n                     0 \n\n#  Create new feature: price_per_bedroom\nfeatures &lt;- features %&gt;%\n  mutate(price_per_bedroom = price / bedrooms)\n\nfeatures$price_per_bedroom[is.infinite(features$price_per_bedroom)] &lt;- NA\nfeatures &lt;- features %&gt;% drop_na()\n\n# Convert categorical columns to factors\nfeatures$neighbourhood_cleansed &lt;- as.factor(features$neighbourhood_cleansed)\nfeatures$room_type &lt;- as.factor(features$room_type)\n\n# Create dummy variables\ndummies &lt;- dummyVars(~ neighbourhood_cleansed + room_type, data = features)\ncategorical_data &lt;- as.data.frame(predict(dummies, newdata = features))\n\n# Select numeric variables \nnumeric_data &lt;- features %&gt;%\n  select(price_per_bedroom, number_of_reviews, review_scores_rating, minimum_nights)\n\nThen, we combined our data together and scaled final data to ensure even weighting.\n\n#  Combine data\nfinal_data &lt;- bind_cols(numeric_data, categorical_data)\n\n# Scale the final dataset\nscaled_data &lt;- scale(final_data)\n\ncolSums(is.na(features))\n\n                 price      number_of_reviews   review_scores_rating \n                     0                      0                      0 \n        minimum_nights               bedrooms neighbourhood_cleansed \n                     0                      0                      0 \n             room_type      price_per_bedroom \n                     0                      0 \n\n\nWe then created an Elbow plot based on the scaled data to determine the ideal number of clusters. While the elbow plot is not a perfect way to determine the number of clusters (as there really is no perfect number) it can still give insight. We chose to use 3 clusters based on the elbow plot, and found three clusters resulted in distinguishable characteristics.\n\nset.seed(9) \nwss &lt;- vector()\n\nfor (k in 1:10) {\n  kmeans_model &lt;- kmeans(scaled_data, centers = k, nstart = 25)\n  wss[k] &lt;- kmeans_model$tot.withinss\n}\n\n# Elbow plot\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Number of Clusters (k)\",\n     ylab = \"Total Within-Cluster Sum of Squares\",\n     main = \"Elbow Method for Choosing k\")\n\n\n\n\n\n\n\nset.seed(9)\nkmeans_model &lt;- kmeans(scaled_data, centers = 3, nstart = 25)\nfeatures$cluster &lt;- as.factor(kmeans_model$cluster)\n\nAs a result we have 3 Clusters:\nCluster 1: Apples - basic, common, and affordable;  not many people’s favorite fruit but still solid. Lowest mean price per bedroom, highest mean number of reviews, smallest mean number of bedrooms, and slightly lower review scores. This indicates cheap, small properties that are popular but not necessarily top-rated.\nCluster 2: Mango - typically more expensive/seen as exotic, not everyone has tried it (medium # of reviews). Highest mean price per bedroom, medium number of reviews, and medium size. This may represent higher-end or luxury properties, potentially located in desirable neighborhoods but reviewed slightly less frequently.\nCluster 3: Strawberries - not as expensive as mangos but more expensive than apples, come in bunches like the bedrooms in these properties, and are many people’s favorite fruit. Medium mean price per bedroom, highest number of bedrooms, and highest review score. This suggests larger properties , offering excellent guest experiences at a moderate price.\n\n# features\nfeatures %&gt;%\n  group_by(cluster) %&gt;%\n  summarise(across(everything(), list(mean = mean), .names = \"mean_{col}\"))\n\n# A tibble: 3 × 9\n  cluster mean_price mean_number_of_reviews mean_review_scores_rating\n  &lt;fct&gt;        &lt;dbl&gt;                  &lt;dbl&gt;                     &lt;dbl&gt;\n1 1             632.                   56.8                      4.78\n2 2            1701.                   30.1                      4.83\n3 3            1295.                   17.5                      4.84\n# ℹ 5 more variables: mean_minimum_nights &lt;dbl&gt;, mean_bedrooms &lt;dbl&gt;,\n#   mean_neighbourhood_cleansed &lt;dbl&gt;, mean_room_type &lt;dbl&gt;,\n#   mean_price_per_bedroom &lt;dbl&gt;\n\nggplot(features, aes(x = cluster)) +\n  geom_bar(fill = \"skyblue\") +\n  geom_text(stat = \"count\", aes(label = ..count..), vjust = -0.5, size = 3) +\n  labs(title = \"Number of Listings per Cluster\",\n       x = \"Cluster\",\n       y = \"Count of Listings\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom first plot we see that, cluster 1 has 1890 values, cluster 2 has 6442, cluster 3 has 13214. Cluster 3 has a much higher amount of listings in it, suggesting the majority of listings are medium price, well reviewed, and have a greater number of bedrooms.\n\nggplot(features %&gt;% filter(price &lt; 10000), \n       aes(x=room_type, y=price, fill=cluster)) + \n  geom_bar(position=\"dodge\", stat=\"identity\") +\n  labs(title = \"Distribution of price within different room types per Cluster (price &lt; 10000)\",\n       x = \"Room Type\",\n       y = \"Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot shows the relationship between room type and price across clusters. We observe that Cluster 1 has the lowest prices, so it includes all kinds of room types . In contrast, Cluster 2 and Cluster 3 have much higher prices, and mainly offer entire homes/apartments. This suggests that higher-priced properties are usually entire listings, while lower-priced properties offer more diverse room options.\n\nggplot(features %&gt;% filter(price &lt; 5000), \n       aes(x = price, y = review_scores_rating, color = cluster)) +\n  geom_point(alpha = 0.7, size = 1.5) +\n  scale_x_continuous(breaks = seq(0, 5000, by = 500)) +\n  scale_y_continuous(breaks = seq(0, 5, by = 1)) +\n  labs(\n    title = \"K-Means Clustering: Price per Bedroom vs Rating (price &lt; 5000)\",\n    x = \"Price per Bedroom\",\n    y = \"Review Score Rating\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot shows that higher price per bedroom is not strongly associated with higher review scores. This suggests that customer satisfaction depends more on experience and value for money than on the property’s price. Thus, offering better service and cost performance is more important for achieving high ratings than simply charging more."
  },
  {
    "objectID": "projects/airbnb/airbnb-copenhagen.html#conclusions",
    "href": "projects/airbnb/airbnb-copenhagen.html#conclusions",
    "title": "Airbnb Analysis in Copenhagen",
    "section": "Conclusions",
    "text": "Conclusions\nThis project gave our team valuable hands-on experience in both technical and interpretive data mining. It was fascinating to learn about Copenhagen, a city which we had limited familiarity with, through the lens of the Airbnb rental market. Beyond the data itself, we gained insight into the geography, culture, and the tourism dynamics of Copenhagen. One of the biggest challenges we encountered was realizing that real-world data is rarely clean or complete. Throughout the project, we encountered missing values, inconsistent formats, and variables that did not always behave as expected. Through trial and error, data cleaning, and model adjustments, we were able to reach results we feel are meaningful and reflective of the Copenhagen rental market.\nSome of the outcomes surprised us, while others were disappointing. The MLR, classification tree, regression, and naive bayes models reminded us that not every outcome is predictable in the real world, especially when human behavior is involved. This project proves that using more complicated methods does not necessarily lead to better predictions, and that in the real world any increase in accuracy, no matter how seemingly small, can be incredibly valuable. Real world data can also be shocking and questionable, listings priced over 60,000 DKK per night or minimum stays of over 200 nights could either represent exceptional cases or possible errors.  What was ultimately most interesting about our work was seeing how different visualizations and modeling approaches revealed different parts of the same story. Rental markets are volatile, but certain themes consistently emerged across our analysis, like location and size importance to guests. This project reinforced the importance of patience and critical thinking when interpreting data. We must often look at the bigger picture and use domain knowledge to help understand the complexities of the real world.\nThe data mining findings from this project could be beneficial to a wide range of audiences, both within Copenhagen and beyond. In Copenhagen itself, the most obvious beneficiaries are Airbnb hosts themselves! These hosts could use the insights from our neighborhood trends, regression pricing model, and clustering analysis to better position their listings thus optimizing both demand and revenue. Understanding what characteristics drive guests to book a stay, leave a review, and pay a premium is incredibly valuable. Our cluster analysis allows hosts to better identify their direct competition and adjust their pricing and marketing strategies accordingly. Investors could use neighborhood analysis to identify more affordable neighborhoods for property acquisition or target higher value neighborhoods to increase profits. Similarly, travelers visiting Copenhagen on a specific budget can use the findings to select rental types and locations that best align with their needs and desires. Competitors such as hotels or Verbo hosts could also use the data to adapt their pricing models or service offerings based on Airbnb’s successes and shortcomings. Additionally, the Copenhagen tourism department could leverage the geographic patterns we uncovered to promote less-saturated neighborhoods and better understand the features of Copenhagen that are naturally attracting tourists. Existing Airbnb hosts could use our superhost analysis to strategically decide if pursuing superhost status would benefit their listings. Although our regression model indicated superhost is not a significant driver of pricing, neighborhood patterns showed where superhosts are concentrated. Achieving superhost status could be a way for hosts to differentiate in underrepresented neighborhoods and potentially boost bookings.\nOutside of Copenhagen, city planners and analysts in other major urban cities could gain insights into what amenities, price points, and property types travelers prefer. Our modeling techniques, particularly the regression model and clustering analysis, could be replicated using other cities’ datasets to better understand their own local rental markets. Identifying which property features drive price premiums or rental satisfactions could help shape tourism strategies and investment decisions in cities with similar rental markets. Finally, marketers across the travel, real estate, and hospitality sectors would find significant value in this data mining work. Airbnb hosts, hotel brands, real estate agents, and tourism boards could all use our insights to better tailor their marketing strategies. In particular, our word cloud and clustering could help marketers craft more effective messaging by emulating key neighborhood descriptors and property features that are successful with Copenhagens’ guests. Popular property types, frequently mentioned listing characteristics, and pricing strategies identified in our analysis provide guidance for designing compelling marketing and promotional campaigns.\nOur insight into which factors most influence pricing, listing successes, and guest satisfaction, our findings offer actionable strategies for a wide variety of audiences, both within Copenhagen and across the broader travel and hospitality industries."
  },
  {
    "objectID": "projects/pokemon/pokemon.html#data-exploration",
    "href": "projects/pokemon/pokemon.html#data-exploration",
    "title": "Clustering Pokémon: A Hierarchical Approach to Character Grouping",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(ggplot2)\nlibrary(ggdendro)\nlibrary(skimr)\npokemon &lt;- read.csv('all_pokemon_data.csv')\n\n\n\n\n\n\n\n\ndim(pokemon)\n\n[1] 1184   24\n\n\nDataset consists of 1184 rows and 24 columns.\n\nset.seed(79)\nrandom_sample &lt;- pokemon[sample(nrow(pokemon), 20), ]\n\nsum(is.na(random_sample))\n\n[1] 0\n\n\nThere is no NA values in random chosen sample.\n\n\n\n\n\n\n\nskim(random_sample)\n\n\nData summary\n\n\nName\nrandom_sample\n\n\nNumber of rows\n20\n\n\nNumber of columns\n24\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n9\n\n\nnumeric\n15\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1\n5\n14\n0\n20\n0\n\n\nPrimary.Typing\n0\n1\n3\n8\n0\n10\n0\n\n\nSecondary.Typing\n0\n1\n0\n6\n9\n7\n0\n\n\nSecondary.Typing.Flag\n0\n1\n4\n5\n0\n2\n0\n\n\nGeneration\n0\n1\n12\n15\n0\n8\n0\n\n\nLegendary.Status\n0\n1\n4\n5\n0\n2\n0\n\n\nForm\n0\n1\n4\n6\n0\n3\n0\n\n\nAlt.Form.Flag\n0\n1\n4\n5\n0\n2\n0\n\n\nColor.ID\n0\n1\n3\n5\n0\n7\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nNational.Dex..\n0\n1\n564.10\n331.92\n40\n271.25\n572.0\n886.25\n1001\n▅▅▃▂▇\n\n\nEvolution.Stage\n0\n1\n1.70\n0.80\n1\n1.00\n1.5\n2.00\n3\n▇▁▅▁▃\n\n\nNumber.of.Evolution\n0\n1\n1.95\n0.89\n1\n1.00\n2.0\n3.00\n3\n▇▁▅▁▇\n\n\nCatch.Rate\n0\n1\n55.00\n49.28\n3\n41.25\n47.5\n75.00\n235\n▇▇▁▁▁\n\n\nHeight..dm.\n0\n1\n13.15\n6.18\n3\n10.00\n12.0\n18.25\n25\n▃▇▇▅▂\n\n\nWeight..hg.\n0\n1\n1216.95\n2004.27\n3\n74.25\n470.0\n1381.00\n8000\n▇▂▁▁▁\n\n\nHeight..in.\n0\n1\n51.70\n24.35\n12\n39.00\n47.0\n72.00\n98\n▃▇▇▅▂\n\n\nWeight..lbs.\n0\n1\n268.35\n441.84\n1\n16.50\n103.5\n304.25\n1764\n▇▂▁▁▁\n\n\nBase.Stat.Total\n0\n1\n476.70\n84.69\n198\n449.25\n492.5\n512.50\n580\n▁▁▂▇▆\n\n\nHealth\n0\n1\n82.05\n25.90\n28\n68.75\n82.5\n100.00\n140\n▂▇▇▇▁\n\n\nAttack\n0\n1\n92.50\n30.02\n25\n70.00\n95.0\n107.50\n145\n▁▇▅▇▅\n\n\nDefense\n0\n1\n79.50\n30.49\n25\n60.00\n72.5\n100.00\n135\n▅▇▇▅▆\n\n\nSpecial.Attack\n0\n1\n71.60\n22.26\n20\n58.75\n72.5\n86.25\n112\n▁▅▇▅▅\n\n\nSpecial.Defense\n0\n1\n78.05\n32.10\n20\n63.75\n72.5\n90.00\n154\n▃▇▇▂▂\n\n\nSpeed\n0\n1\n73.00\n25.52\n30\n52.50\n72.5\n91.25\n120\n▇▂▇▇▂\n\n\n\n\n\nCategorical: National.Dex, Evolution.Stage\nNumerical: Number.of.Evolution, Catch Rate, Height..dm., Weight..hg., Height..in., Weight..lbs., Base.Stat.Total, Health, Attack, Defense, Special Attack, Special Defense, Speed\n\nrand_sp_numeric &lt;- random_sample %&gt;%\n  select (Name, Catch.Rate, Special.Defense, Special.Attack, Speed, Health)\nrownames(rand_sp_numeric) &lt;- rand_sp_numeric$Name\n\nrand_sp_numeric\n\n\n\n\n\n\n\n\nrand_sp_numeric &lt;- rand_sp_numeric %&gt;% select(-Name)\n\nPerformance indicators can be good options for clustering. Since Base Stat Total already covers other stats, keeping it can cause multicolinerity between variables. Therefore, I excluded Base Stat Total and instead selected a focused set of variables: Speed, Catch Rate, Health, Special Attack, Special Defense. I want to cluster pokemons based on magic-based powerhouses, because of that I chose Special variables. Speed, health, and catch rate help show how fast, tough, or easy to catch a Pokémon is—things that also reflect their overall performance.\n\n\n\nData summary\n\n\nName\nrand_sp_numeric\n\n\nNumber of rows\n20\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCatch.Rate\n0\n1\n55.00\n49.28\n3\n41.25\n47.5\n75.00\n235\n▇▇▁▁▁\n\n\nSpecial.Defense\n0\n1\n78.05\n32.10\n20\n63.75\n72.5\n90.00\n154\n▃▇▇▂▂\n\n\nSpecial.Attack\n0\n1\n71.60\n22.26\n20\n58.75\n72.5\n86.25\n112\n▁▅▇▅▅\n\n\nSpeed\n0\n1\n73.00\n25.52\n30\n52.50\n72.5\n91.25\n120\n▇▂▇▇▂\n\n\nHealth\n0\n1\n82.05\n25.90\n28\n68.75\n82.5\n100.00\n140\n▂▇▇▇▁\n\n\n\n\n\n\nhead(rand_sp_numeric)\n\n\n\n\n\n\n\nThe data needs to be normalized because each column has a different range of values. This makes sure that all variables have equal weight in clustering. Otherwise, the column with higher magnitude - Catch Rate will have higher influence.\n\nnorm &lt;- preProcess(rand_sp_numeric, method = c(\"center\", \"scale\"))\nrandom_sample_norm &lt;- predict(norm, rand_sp_numeric)\nrandom_sample_norm"
  },
  {
    "objectID": "projects/pokemon/pokemon.html#hierarchical-clustering",
    "href": "projects/pokemon/pokemon.html#hierarchical-clustering",
    "title": "Clustering Pokémon: A Hierarchical Approach to Character Grouping",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\nd.norm &lt;- dist(random_sample_norm, method=\"euclidean\")\n# d.norm\n\nhc &lt;- hclust(d.norm, method=\"average\")\nggdendrogram(hc) \n\n\n\n\n\n\n\nggdendrogram(hc) + \n  geom_hline(yintercept = 3, color = \"red\", linetype = \"dashed\")\n\n\n\n\n\n\n\n\nThe number of clusters can vary depending on how we interpret the data. If we look for very distinct groups, we can see that the leftmost cluster is clearly separate from the others. Let’s draw a line with cut point=3, and group characters based on that. As a result we get 5 groups: {ralts}, {stonjourner}, {minior-violet and wugtrio}, {wigglytuff}, {others}.\n\nclusters &lt;- cutree(hc, k = 5)\nclusters\n\n      sawsbuck       wo-chien        calyrex        suicune  minior-violet \n             1              1              1              1              2 \n    conkeldurr     annihilape      glastrier       ursaring          yanma \n             1              1              1              1              1 \n    masquerain          ralts castform-sunny    stonjourner        wugtrio \n             1              3              1              4              2 \n       crustle        florges     wigglytuff       primeape      arctovish \n             1              1              5              1              1 \n\n\n\nrand_sp_numeric$cluster &lt;- clusters\n\ncluster_mean &lt;- rand_sp_numeric %&gt;% group_by(cluster) %&gt;% \n  summarise_all(mean)\ncluster_mean\n\n# A tibble: 5 × 6\n  cluster Catch.Rate Special.Defense Special.Attack Speed Health\n    &lt;int&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1       1         45            88.4           75.5    71   85.2\n2       2         40            65             75     120   47.5\n3       3        235            35             45      40   28  \n4       4         60            20             20      70  100  \n5       5         50            50             85      45  140  \n\n\nAfter calculating mean values of each cluster, we see that:\n\nCluster 1: high special defense; moderate speed, special attack and health; low catch rate. This cluster seems to group defensive Pokemon that are less focused on catching.\nCluster 2: highest speed, lowest catch rate, remaining variables are moderate. This cluster groups fastest Pokemon that are harder to catch, possibly indicating Pokemon with high mobility.\nCluster 3: highest catch rate, while remaining values are low. Seems like this cluster have only one Pokemon, since catch rate is extremely high, which indicates Pokemon that’s easy to catch but weak in battle.\nCluster 4: lowest special defense and attack, health parameter is relatively high and others are moderate. This cluster group Pokemon with high survivability but with low offensive capabilities.\nCluster 5: highest special attack and health values, and others moderate. This cluster groups powerful special attackers with strong survival level."
  },
  {
    "objectID": "projects/pokemon/pokemon.html#data-visualization",
    "href": "projects/pokemon/pokemon.html#data-visualization",
    "title": "Clustering Pokémon: A Hierarchical Approach to Character Grouping",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nrand_sp_numeric$cluster &lt;- factor(rand_sp_numeric$cluster)\n\nggplot(rand_sp_numeric, aes(x=Speed, y=Catch.Rate, color=cluster)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe scatter plot above shows relationship between Speed and Catch Rate across clusters. We can see one Pokemon in Cluster 3 with the highest Catch rate. Although there are two members in CLuster 2 with the highest Speed values.\n\nggplot(rand_sp_numeric, aes(x=Special.Attack, y=Health, color=cluster)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe scatter plot shows the relationship between Health and Special Attack across clusters. We can see that Cluster 1 members have average health and higher special attack, Cluster 4 have one member with lowest special attack and moderate health value. CLuster 5 groups Pokemon with high health and special attack, and Cluster 3 contains Pokemon with lowest health and moderate attack, likely focusing more on offense. Although, Cluster 2 consists of two Pokemon: one with lowest health and moderate attack, another with high attack and moderate health.\n\nggplot(rand_sp_numeric, aes(x=cluster, y=Speed)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe boxplot above shows distribution of Speed values across clusters. Cluster 1 has a large number of Pokemon, with a broader range of Speed values, while other clusters contains only 1 or 2 Pokemon. When comparing the median Speed values, Cluster 2 has the highest median Speed, indicating that these Pokemon are the fastest. Clusters 3 and 4 have lower median Speed values, meaning that these are slowest ones.\n\nrand_sp_numeric[5,]\n\n              Catch.Rate Special.Defense Special.Attack Speed Health cluster\nminior-violet         30              60            100   120     60       2\n\nrand_sp_numeric %&gt;% filter(cluster==2)\n\n              Catch.Rate Special.Defense Special.Attack Speed Health cluster\nminior-violet         30              60            100   120     60       2\nwugtrio               50              70             50   120     35       2\n\n\nThe Pokemon Minior-violet falls into Cluster 2, group of fastest Pokemon. This cluster contains two Pokemon, both having the similar highest Speed value, making it the key feature in Cluster 2. They also have relatively close special defense values. Although there are large differences in Special Attack and Health, these seem to have less impact compared to their shared high Speed."
  },
  {
    "objectID": "projects/pokemon/pokemon.html#custom-weighting",
    "href": "projects/pokemon/pokemon.html#custom-weighting",
    "title": "Clustering Pokémon: A Hierarchical Approach to Character Grouping",
    "section": "Custom Weighting",
    "text": "Custom Weighting\nEqual weighting can be problematic if some parameters are more important than others. If we want the clustering to depend more on certain variables and less on others, giving all features the same weight might not reflect what we’re really trying to group. For example, in this case, I’m more interested in clustering based on special or magical attributes, so those variables should carry more influence.\n\nrandom_sample.weighted &lt;- random_sample_norm\n\nrandom_sample.weighted$Special.Attack &lt;- random_sample.weighted$Special.Attack*30\nrandom_sample.weighted$Special.Defense &lt;- random_sample.weighted$Special.Defense*25\nrandom_sample.weighted$Speed &lt;- random_sample.weighted$Speed*20\nrandom_sample.weighted$Health &lt;- random_sample.weighted$Health*10\nrandom_sample.weighted$Catch.Rate &lt;- random_sample.weighted$Catch.Rate*5\n\nSince I want to cluster based on special, magic-based parameters, I assigned weights to the variables accordingly: Special attack is the most important, so it gets a weight of 30. Special defense supports magical resistance, and gets a weight of 25. Speed is useful for overall performance, so I gave it a weight of 20. Health is still important, just less critical here, with a weight of 10. Catch rate has the least impact on magical strength, so it gets a weight of 5.\n\nd.norm2 &lt;- dist(random_sample.weighted, method=\"euclidean\")\n\nhc2 &lt;- hclust(d.norm2, method=\"average\")\nggdendrogram(hc2)\n\n\n\n\n\n\n\n\nIn new dendrogram, the y-axis now ranges from 0 to 75, compared to the previous range of 0 to 4. This shows a wider range of dissimilarity within clusters. Also, there are no singletons in this version — all items are connected to small groups. The way the items cluster together, and the distances at which these groupings happen, are also noticeably different compared to the unweighted version. For instance, before applying weight Pokemon Ralts didn’t belong to any clear group — it was directly connected to all the remaining ones. After applying the weights, Ralts is now grouped with Stonjourner, and together they are then connected to the rest.\n\nclusters2 &lt;- cutree(hc2, k = 4)\nclusters2\n\n      sawsbuck       wo-chien        calyrex        suicune  minior-violet \n             1              2              1              2              3 \n    conkeldurr     annihilape      glastrier       ursaring          yanma \n             1              1              1              1              1 \n    masquerain          ralts castform-sunny    stonjourner        wugtrio \n             3              4              1              4              1 \n       crustle        florges     wigglytuff       primeape      arctovish \n             1              2              1              1              1 \n\n\n\nrand_sp_numeric$cluster &lt;- clusters2\n\nrand_sp_numeric %&gt;% group_by(cluster) %&gt;% \n  summarise_all(mean)\n\n# A tibble: 4 × 6\n  cluster Catch.Rate Special.Defense Special.Attack Speed Health\n    &lt;int&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1       1       49.7            73.8           66.9  70.8   86.2\n2       2       18             135.            99    76.7   87.7\n3       3       52.5            71            100   100     65  \n4       4      148.             27.5           32.5  55     64  \n\n\nAfter grouping Pokemon into 4 groups, some patterns emerge:\n\nCluster 1: high Health, with other parameters moderate. This group includes Pokemon that are built to last longer in battle, but with less focus on offensive power.\nCluster 2: highest special defense, attack, and health, with the lowest catch rate. These are powerful, magic-based Pokemon that are hard to catch — fitting for their strong battle stats.\nCluster 3: highest special attack and speed, but low health value. These are fast, offensive Pokemon — strong in attack but fragile in battle.\nCluster 4: highest catch rate, with other values low. These are easiest to catch, and tend to be less competitive in battle.\n\n\nrand_sp_numeric[5,]\n\n              Catch.Rate Special.Defense Special.Attack Speed Health cluster\nminior-violet         30              60            100   120     60       3\n\nrand_sp_numeric %&gt;% filter(cluster==3)\n\n              Catch.Rate Special.Defense Special.Attack Speed Health cluster\nminior-violet         30              60            100   120     60       3\nmasquerain            75              82            100    80     70       3\n\n\nAfter weighting, Minior-violet was moved to Cluster 3, which also contains two Pokemon. The other member of this cluster has the same Special Attack value, similar Health, and both share a high Speed value. Before weighting, Minior-violet was placed in Cluster 2, where its highest Speed value was the key feature driving the grouping. After weighting, special attack and defense were given more importance. This made Minior-violet more similar to Masquerain, which shares those characteristics of high Special Attack and Speed, even though both Pokémon have relatively low Health."
  }
]