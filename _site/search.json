[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "During the final year of my bachelor’s studies, I began working full-time as a Full-Stack Software Engineer in the banking industry at VTB Bank Kazakhstan. In this role, I applied my academic knowledge in real-world scenarios and gained significant experience. In this role, I developed web applications from the ground up, enhanced existing ones, and managed databases. My work involved third-party API integrations, performance tuning, and seamless deployment into production environments. I led frontend and backend development using technologies such as Java, Node.js, React, and Oracle APEX, and worked extensively with PostgreSQL and Oracle databases. Additionally, I utilized tools like DBMS Profiler and Query Analyzer to optimize query performance and ensure system reliability.\nThis experience led me to discover a fascination with data management and analytics — the way data surrounds us and can yield powerful insights into trends, behaviors, and decision-making. This interest inspired me to dive deeper into data analytics through online courses, and ultimately, to pursue a graduate degree in Applied Business Analytics at Boston University. So now, I’m a graduate student:)"
  },
  {
    "objectID": "projects/nhl_players/nhl_players_analysis.html",
    "href": "projects/nhl_players/nhl_players_analysis.html",
    "title": "Forecasting the salaries of hockey players",
    "section": "",
    "text": "This project focuses on forecasting hockey player salaries by utilizing statistical analysis and machine learning methods. By analyzing player performance metrics, historical salary information, and other relevant factors, the project aims to deliver precise salary predictions.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nSimple Linear Regression\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\n\nmain_data &lt;- read.csv('nhl_players.csv')\nglimpse(main_data)\n\nRows: 568\nColumns: 18\n$ name          &lt;chr&gt; \"Zemgus Girgensons\", \"Zack Smith\", \"Zack Kassian\", \"Zach…\n$ Team_y        &lt;chr&gt; \"BUF\", \"CHI\", \"EDM\", \"MIN\", \"TOR\", \"BUF, T.B\", \"BUF, T.B…\n$ Position_y    &lt;chr&gt; \"C\", \"C\", \"W\", \"W\", \"W\", \"D\", \"D\", \"C\", \"C\", \"W\", \"C\", \"…\n$ HANDED        &lt;chr&gt; \"Left\", \"Left\", \"Right\", \"Left\", \"Right\", \"Right\", \"Righ…\n$ GP            &lt;int&gt; 68, 50, 59, 69, 51, 27, 27, 57, 70, 68, 63, 70, 55, 68, …\n$ G             &lt;int&gt; 12, 4, 15, 25, 21, 1, 1, 6, 10, 31, 15, 7, 4, 8, 17, 1, …\n$ A             &lt;int&gt; 7, 7, 19, 21, 16, 6, 6, 7, 20, 28, 31, 12, 17, 17, 14, 7…\n$ P             &lt;int&gt; 19, 11, 34, 46, 37, 7, 7, 13, 30, 59, 46, 19, 21, 25, 31…\n$ Sh            &lt;int&gt; 85, 43, 99, 155, 106, 29, 29, 98, 110, 197, 138, 99, 62,…\n$ Sh_perc       &lt;dbl&gt; 0.14, 0.09, 0.15, 0.16, 0.20, 0.03, 0.03, 0.06, 0.09, 0.…\n$ SALARY        &lt;chr&gt; \"$1,600,000\", \"$3,250,000\", \"$2,000,000\", \"$9,000,000\", …\n$ PIM           &lt;int&gt; 10, 29, 69, 8, 23, 22, 22, 28, 49, 12, 16, 39, 6, 66, 45…\n$ Giveaways     &lt;int&gt; 11, 14, 45, 22, 16, 11, 11, 21, 14, 41, 37, 8, 27, 18, 2…\n$ Takeaways     &lt;int&gt; 13, 21, 26, 21, 32, 4, 4, 20, 48, 42, 54, 22, 10, 27, 25…\n$ Hits          &lt;int&gt; 110, 112, 157, 27, 52, 30, 30, 136, 78, 9, 40, 213, 31, …\n$ Hits.Taken    &lt;int&gt; 71, 71, 54, 60, 101, 21, 21, 99, 114, 66, 94, 129, 54, 6…\n$ blocked_shots &lt;int&gt; 20, 18, 8, 38, 23, 27, 27, 30, 20, 14, 38, 11, 75, 29, 4…\n$ PlusMinus     &lt;int&gt; -1, 2, 0, -11, 13, 0, 0, 6, -5, -2, 11, 0, -8, -21, -5, …\n\n\nBy looking at dataset, we can see that all variables that are represented by number are numeric. Even the salary column is represented as character because of the dollar sign, it is also numeric. And all other remaining variables are categorical.\nCategorical: name, Team_y, Position_y, Handed\nNumeric: GP, G, A, P, Sh, Sh_perc, Salary, PIM, Giveaways, Takeaways, Hits, Hits.Taken, blocked_shots, PlusMinus\nDiscrete numeric: GP, G, A, P, Sh, Giveaways, Takeaways, Hits, Hits.Taken, blocked_shots, PlusMinus Continuous numeric: Sh_perc, Salary, PIM\n\ncolSums(is.na(main_data))\n\n         name        Team_y    Position_y        HANDED            GP \n            0             0             0             0             0 \n            G             A             P            Sh       Sh_perc \n            0             0             0             0             0 \n       SALARY           PIM     Giveaways     Takeaways          Hits \n            0             0             0             0             0 \n   Hits.Taken blocked_shots     PlusMinus \n            0             0             0 \n\n\nOur dataset doesn’t have any NA values.\n\nmain_data &lt;- main_data %&gt;% rename(Team = Team_y, Position = Position_y)\nnames(main_data)\n\n [1] \"name\"          \"Team\"          \"Position\"      \"HANDED\"       \n [5] \"GP\"            \"G\"             \"A\"             \"P\"            \n [9] \"Sh\"            \"Sh_perc\"       \"SALARY\"        \"PIM\"          \n[13] \"Giveaways\"     \"Takeaways\"     \"Hits\"          \"Hits.Taken\"   \n[17] \"blocked_shots\" \"PlusMinus\"    \n\n\nI renamed the columns ‘Team_y’, ‘Position_y’ by removing last two characters, since they are not useful at all.\n\nmain_data$name[duplicated(main_data$name)]\n\n[1] \"Zach Bogosian\"     \"Valeri Nichushkin\" \"Troy Brouwer\"     \n[4] \"Paul Byron\"        \"Kevin Shattenkirk\" \"Corey Perry\"      \n[7] \"Andrej Sekera\"    \n\nclean_data &lt;- main_data %&gt;% distinct(name, .keep_all = TRUE)\nnrow(clean_data)\n\n[1] 561\n\n\nThere are 7 duplicated name values found, after removing them we had 561 values in dataset.\n\nclean_data$SALARY &lt;- as.numeric(gsub(\"[$,]\", \"\", clean_data$SALARY))\nstr(clean_data)\n\n'data.frame':   561 obs. of  18 variables:\n $ name         : chr  \"Zemgus Girgensons\" \"Zack Smith\" \"Zack Kassian\" \"Zach Parise\" ...\n $ Team         : chr  \"BUF\" \"CHI\" \"EDM\" \"MIN\" ...\n $ Position     : chr  \"C\" \"C\" \"W\" \"W\" ...\n $ HANDED       : chr  \"Left\" \"Left\" \"Right\" \"Left\" ...\n $ GP           : int  68 50 59 69 51 27 57 70 68 63 ...\n $ G            : int  12 4 15 25 21 1 6 10 31 15 ...\n $ A            : int  7 7 19 21 16 6 7 20 28 31 ...\n $ P            : int  19 11 34 46 37 7 13 30 59 46 ...\n $ Sh           : int  85 43 99 155 106 29 98 110 197 138 ...\n $ Sh_perc      : num  0.14 0.09 0.15 0.16 0.2 0.03 0.06 0.09 0.16 0.11 ...\n $ SALARY       : num  1600000 3250000 2000000 9000000 2500000 6000000 1000000 6300000 9000000 5900000 ...\n $ PIM          : int  10 29 69 8 23 22 28 49 12 16 ...\n $ Giveaways    : int  11 14 45 22 16 11 21 14 41 37 ...\n $ Takeaways    : int  13 21 26 21 32 4 20 48 42 54 ...\n $ Hits         : int  110 112 157 27 52 30 136 78 9 40 ...\n $ Hits.Taken   : int  71 71 54 60 101 21 99 114 66 94 ...\n $ blocked_shots: int  20 18 8 38 23 27 30 20 14 38 ...\n $ PlusMinus    : int  -1 2 0 -11 13 0 6 -5 -2 11 ...\n\n\nAfter clearing up the Salary variable from specific characters, I converted it to numeric type.\n\nset.seed(79)\nnhl.index &lt;- sample(c(1:nrow(clean_data)), nrow(clean_data)*0.6)\nnhl_train.df &lt;- clean_data[nhl.index, ]\nnhl_valid.df &lt;- clean_data[-nhl.index, ]\n\nData partitioning helps prevent biased decisions by ensuring that insights from training dataset also applicable to validation set. If we analyze first, we may unintentionally use insights that we think are applicable for every scenario while it can lead to overfitting. By partitioning first, we can ensure that our tests on training and validation sets provide independent performance measures.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFrom the plot above we can see that most of players with small salary also have small number of points, and by increase of total points salary also going up. However despite the total points, it seems like other parameters also affect the salary. Because in some cases even the player has not so high points, the salary is extremely large number. For instance, let’s look at X=60, where we can see that there is one player with very high salary around 16000000, while majority’s salary below 10 million. Maybe other factors such as budget of the team, position type, total number of games played have more impact to the salary. A player with more games may be valued higher due to greater experience, and their impact on team performance could also be considered in the evaluation.\n\ncor(nhl_train.df$SALARY, nhl_train.df$P)\n\n[1] 0.6699033\n\ncor.test(nhl_train.df$SALARY, nhl_train.df$P)\n\n\n    Pearson's product-moment correlation\n\ndata:  nhl_train.df$SALARY and nhl_train.df$P\nt = 16.49, df = 334, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6063712 0.7249371\nsample estimates:\n      cor \n0.6699033 \n\n\nCorrelation is the linear relationship between two continuous variables. Pearson’s correlation measures strength of that relationship. Correlation value here is 0.67, which is not strong(&lt;0.7) but around that value. High t-value and very low p-value suggests correlation is significant, meaning we can reject null hypothesis that there is no correlation between Price and Salary.\n\nmodel &lt;- lm(SALARY ~ P, data = nhl_train.df)\nsummary(model)\n\n\nCall:\nlm(formula = SALARY ~ P, data = nhl_train.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-4686187 -1402916  -578619  1201397  9209631 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1311280     186918   7.015 1.28e-11 ***\nP              99477       6033  16.490  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2220000 on 334 degrees of freedom\nMultiple R-squared:  0.4488,    Adjusted R-squared:  0.4471 \nF-statistic: 271.9 on 1 and 334 DF,  p-value: &lt; 2.2e-16\n\n\n\nmax(residuals(model))\n\n[1] 9209631\n\nmax_res_index &lt;- which.max(residuals(model))\nactual_data_max_res &lt;- nhl_train.df[max_res_index, ]\nactual_data_max_res$SALARY\n\n[1] 14500000\n\npredict(model, newdata = actual_data_max_res)\n\n    371 \n5290369 \n\n\nFor highest residual value in the model: Actual salary:14,500,000 Predicted salary:5,290,369 Residual is difference between actual and predicted values, which is 9,209,631 for this player.\n\nmin(residuals(model))\n\n[1] -4686187\n\nmin_res_index &lt;- which.min(residuals(model))\nactual_data_min_res &lt;- nhl_train.df[min_res_index, ]\nactual_data_min_res$SALARY\n\n[1] 1400000\n\npredict(model, newdata = actual_data_min_res)\n\n     33 \n6086187 \n\n\nFor lowest residual value in the model: Actual Salary: 1,400,000 Predicted Salary: 6,086,187 From this record we can determine which value was subtracted from another, so residual = actual - predicted = 1400000 - 6086187 = -4686187\nBesides Points the number of games played, shot percentage, penalties in minutes can also impact salary. More games played more reliable player looks like, higher shot percentage shows higher efficiency of scoring, more penalties can negatively impact team. The player’s performance, and defensive skills could have more impact. Even if a player just joined the team, his strong impact on team performance and outstanding gameplay can boost their popularity. The increased demand may attract interest from other team managers which definitely influence the player’s value.\n\nsummary(model)\n\n\nCall:\nlm(formula = SALARY ~ P, data = nhl_train.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-4686187 -1402916  -578619  1201397  9209631 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1311280     186918   7.015 1.28e-11 ***\nP              99477       6033  16.490  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2220000 on 334 degrees of freedom\nMultiple R-squared:  0.4488,    Adjusted R-squared:  0.4471 \nF-statistic: 271.9 on 1 and 334 DF,  p-value: &lt; 2.2e-16\n\n\nThe regression equation is 1311280 + 99477*P From the equation, we see that even if the player doesn’t have any points he will start with 1,311,280 salary. And each earned point will increase that minimum salary by 99,477. Let’s assume P=10 –&gt; Salary=2,306,050.\n\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\ntrain &lt;- predict(model, nhl_train.df)\nvalid &lt;- predict(model, nhl_valid.df)\n\n# Training Set\naccuracy(train, nhl_train.df$SALARY)\n\n                       ME    RMSE     MAE       MPE     MAPE\nTest set 0.00000000443383 2213352 1688599 -51.21713 77.33822\n\n# Validation Set\naccuracy(valid, nhl_valid.df$SALARY)\n\n                ME    RMSE     MAE       MPE     MAPE\nTest set -30878.14 2126314 1659595 -47.01189 71.70397\n\n\nSince we are using our model to predict value, we need to be sure that we are not overfitting our data. Overfitting would make the model ineffective, as it would perform well on training data but fail to new, unseen data. The values above show overall measures of predictive accuracy. RMSE value for validation data (2126314) is smaller than for the training data, which is 2213352. However both values are close, which is indicates that model is not overfitting. Mean absolute error for holdout set (1659595) also smaller than the value for training set (1688599). Thus, we actually see less error on validation data.\n\nsd(nhl_train.df$SALARY)\n\n[1] 2985599\n\n2213352/sd(nhl_train.df$SALARY)\n\n[1] 0.7413428\n\n2126314/sd(nhl_train.df$SALARY)\n\n[1] 0.7121902\n\n\nLet’s compare RMSE to the standard deviation of training set. Both values are very close, and relatively accurate since SD tells us how much variable’s value differ from its mean value. If the RMSE higher than SD, model’s predictions are not much better than using the mean value of the dataset as a predictor.\n\n\nMultiple Linear Regression\n\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n\n\n\n\n\n\n\n\nFrom heatmap we can see correlation value between variables in our dataset. The Goal, Assists number, total shots and number of takeaways and points are strongly correlated between each other (&gt;0.7). The assists number, shots and giveaways number also strongly correlated. While shot percentage negatively impacts blocked shots number, PlusMinus have very small connection with all remaining variables. Here, we can observe multicolinearity since Points is the sum of Goals and Assists, making them dependent variables. Similarly, Shot Percentage is derived by dividing Shots to Goals. Since Shots represent the number of times a player attempts to score, and Points are the sum of goals and assists, these numbers are interconnected. So Shots can cause Goals, and when a player scores a Goal, an Assist should be credited to the player, the sum of these two numbers are represented as Points. Since we can’t use dependent variables as inputs in linear model, let’s keep Points as it holds more value than total shots, as a player may take many shots without successfully scoring a goal. Also it is more correlated to output variable.\n\n\n\n\n\n\n\n\n\nIn new heatmap, we can see that Takeaways and Points are highly correlated (=0.8). Maybe these numbers are not dependent, but when player took a puck from an opposite it can lead to goal. Let’s remove Takeaways from our model. The player with high giveaways have a tendency to lose a puck more often, which can decrease team’s performance. Which also can affect Points earned. Also let’s remove Hits.Taken since its highly correlated with Games Played (=0.71). More games played more possibility to make a contact with the player who has the puck. And let’s build model with remaining variables, and use backward elimination.\n\nnhl_train_numbers &lt;- nhl_train_numbers %&gt;% select(-Takeaways, -Giveaways, -Hits.Taken)\nnhl_train_numbers %&gt;% cor()\n\n                      GP          P      SALARY         PIM        Hits\nGP            1.00000000 0.67582735  0.42421317 0.520448294  0.49558958\nP             0.67582735 1.00000000  0.66990334 0.305617550  0.08245484\nSALARY        0.42421317 0.66990334  1.00000000 0.198372387 -0.02394758\nPIM           0.52044829 0.30561755  0.19837239 1.000000000  0.56542977\nHits          0.49558958 0.08245484 -0.02394758 0.565429770  1.00000000\nblocked_shots 0.48789199 0.14605555  0.22171296 0.304039783  0.36510186\nPlusMinus     0.07172414 0.24643918  0.07974078 0.003629743  0.03591750\n              blocked_shots   PlusMinus\nGP                0.4878920 0.071724142\nP                 0.1460555 0.246439184\nSALARY            0.2217130 0.079740779\nPIM               0.3040398 0.003629743\nHits              0.3651019 0.035917499\nblocked_shots     1.0000000 0.122277735\nPlusMinus         0.1222777 1.000000000\n\n\nWhen categorical variables used as predictors, we convert them into dummy variables. A variable with n categories will have n-1 dummy variables, and remaining one value will be as reference level. This helps in analyzing the impact of categorical predictors on the dependent variable.\n\nnhl_train.df &lt;- nhl_train.df %&gt;% select(-G, -A, -Sh, -Sh_perc, -Takeaways, -Giveaways, -Hits.Taken)\nnhl_train.df &lt;- nhl_train.df %&gt;% select(-name)\n\nmodel1 &lt;- step(lm(SALARY~., data = nhl_train.df), direction = \"backward\")\n\nStart:  AIC=9856.2\nSALARY ~ Team + Position + HANDED + GP + P + PIM + Hits + blocked_shots + \n    PlusMinus\n\n                Df       Sum of Sq        RSS    AIC\n- Team          60 263015531187124 1.4790e+15 9802.0\n- Position       2     39614240556 1.2161e+15 9852.2\n- HANDED         1   3453363733890 1.2195e+15 9855.1\n- PIM            1   6847508251807 1.2229e+15 9856.1\n&lt;none&gt;                             1.2160e+15 9856.2\n- Hits           1  16635747374161 1.2327e+15 9858.8\n- GP             1  18064821999018 1.2341e+15 9859.2\n- PlusMinus      1  38987510345295 1.2550e+15 9864.8\n- blocked_shots  1  41997523523384 1.2580e+15 9865.6\n- P              1 546410473355975 1.7624e+15 9978.9\n\nStep:  AIC=9801.99\nSALARY ~ Position + HANDED + GP + P + PIM + Hits + blocked_shots + \n    PlusMinus\n\n                Df       Sum of Sq        RSS    AIC\n- Position       2   2985822004712 1.4820e+15 9798.7\n- PIM            1   2443977770222 1.4815e+15 9800.5\n- HANDED         1   4399244297709 1.4834e+15 9801.0\n&lt;none&gt;                             1.4790e+15 9802.0\n- Hits           1  14816636482001 1.4939e+15 9803.3\n- GP             1  22027438385796 1.5011e+15 9805.0\n- blocked_shots  1  40378661481616 1.5194e+15 9809.0\n- PlusMinus      1  42203193555354 1.5212e+15 9809.4\n- P              1 689163748993858 2.1682e+15 9928.5\n\nStep:  AIC=9798.67\nSALARY ~ HANDED + GP + P + PIM + Hits + blocked_shots + PlusMinus\n\n                Df       Sum of Sq        RSS    AIC\n- PIM            1   2493780976016 1.4845e+15 9797.2\n- HANDED         1   4714736033909 1.4867e+15 9797.7\n&lt;none&gt;                             1.4820e+15 9798.7\n- Hits           1  16381671703132 1.4984e+15 9800.4\n- GP             1  26188587267788 1.5082e+15 9802.6\n- PlusMinus      1  41950294751938 1.5240e+15 9806.0\n- blocked_shots  1 110654200590976 1.5927e+15 9820.9\n- P              1 699222750683931 2.1812e+15 9926.5\n\nStep:  AIC=9797.23\nSALARY ~ HANDED + GP + P + Hits + blocked_shots + PlusMinus\n\n                Df       Sum of Sq        RSS    AIC\n- HANDED         1   5021715383231 1.4895e+15 9796.4\n&lt;none&gt;                             1.4845e+15 9797.2\n- Hits           1  13910288355677 1.4984e+15 9798.4\n- GP             1  24706476867436 1.5092e+15 9800.8\n- PlusMinus      1  44055357733151 1.5286e+15 9805.1\n- blocked_shots  1 112186182042815 1.5967e+15 9819.7\n- P              1 723087011778966 2.2076e+15 9928.6\n\nStep:  AIC=9796.37\nSALARY ~ GP + P + Hits + blocked_shots + PlusMinus\n\n                Df       Sum of Sq        RSS    AIC\n&lt;none&gt;                             1.4895e+15 9796.4\n- Hits           1  13058980965199 1.5026e+15 9797.3\n- GP             1  25658762191542 1.5152e+15 9800.1\n- PlusMinus      1  43964438030890 1.5335e+15 9804.1\n- blocked_shots  1 115091536261284 1.6046e+15 9819.4\n- P              1 725150613332920 2.2147e+15 9927.6\n\n\n\nsummary(model1)\n\n\nCall:\nlm(formula = SALARY ~ GP + P + Hits + blocked_shots + PlusMinus, \n    data = nhl_train.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-5191324 -1186912  -546795  1046568  8061071 \n\nCoefficients:\n              Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)    1587099     305342   5.198 0.000000354 ***\nGP              -24975      10475  -2.384     0.01768 *  \nP               118004       9310  12.675     &lt; 2e-16 ***\nHits             -5146       3026  -1.701     0.08990 .  \nblocked_shots    21018       4162   5.050 0.000000734 ***\nPlusMinus       -36873      11815  -3.121     0.00196 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2125000 on 330 degrees of freedom\nMultiple R-squared:  0.5012,    Adjusted R-squared:  0.4936 \nF-statistic: 66.31 on 5 and 330 DF,  p-value: &lt; 2.2e-16\n\n\nHere is the summary of our model. I didn’t include name of the player as an input. From the model we can see that Games Played, Hits, PlusMinus have negative impact on salary. Maybe because of the demand to new players, we got negative coef to GP.\n\nmean_salary &lt;- mean(nhl_train.df$SALARY)\n\nsst &lt;- sum((nhl_train.df$SALARY - mean_salary)^2)\nsst\n\n[1] 2986122868827386\n\nssr &lt;- sum((model1$fitted.values-mean_salary)^2)\nssr\n\n[1] 1496580443217731\n\nssr/sst\n\n[1] 0.5011785\n\n\nThe final value is exactly same as r-squared value of the model.\n\nlibrary(visualize)\nvisualize.t(stat=c(-2.384, 2.384), df=330, section=\"bounded\")\n\n\n\n\n\n\n\n\nt-value for GP is -2.384. After plotting distribution for that t-value, we can see that 98.2% of the curve is shaded. A bigger t-value occupy more space, and p-value goes lower. The remaining 1.8% (p-value) is the probability of obtaining a t-statistic beyond [-2.384, 2.384].\n\nsummary(model1)\n\n\nCall:\nlm(formula = SALARY ~ GP + P + Hits + blocked_shots + PlusMinus, \n    data = nhl_train.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-5191324 -1186912  -546795  1046568  8061071 \n\nCoefficients:\n              Estimate Std. Error t value    Pr(&gt;|t|)    \n(Intercept)    1587099     305342   5.198 0.000000354 ***\nGP              -24975      10475  -2.384     0.01768 *  \nP               118004       9310  12.675     &lt; 2e-16 ***\nHits             -5146       3026  -1.701     0.08990 .  \nblocked_shots    21018       4162   5.050 0.000000734 ***\nPlusMinus       -36873      11815  -3.121     0.00196 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2125000 on 330 degrees of freedom\nMultiple R-squared:  0.5012,    Adjusted R-squared:  0.4936 \nF-statistic: 66.31 on 5 and 330 DF,  p-value: &lt; 2.2e-16\n\n\nF-statistic: 66.31 F-statistic tests overall significance of the model. The better the fit, the higher the F-score will be.\n\n# F-statistic calculation\nk &lt;- 5\nn &lt;- 336\nsse &lt;- sum(model1$residuals^2)\n\nnumerator &lt;- ssr/k\ndenominator &lt;- sse / (n-k-1)\nnumerator / denominator\n\n[1] 66.31185\n\n\n\npredict(model1, newdata = data.frame(GP=82, P=60, Hits=150, blocked_shots=100, PlusMinus=20))\n\n      1 \n7211812 \n\n\nSo, by using the predict() function with random data the predicted salary is $7,211,812. It was found by using Regression Equation: 1587099-24975GP+118004P-5146Hits+21018blocked_shots-36873*PlusMinus\n\ntrain1 &lt;- predict(model1, nhl_train.df)\nvalid1 &lt;- predict(model1, nhl_valid.df)\n\n# Training Set\naccuracy(train1, nhl_train.df$SALARY)\n\n                        ME    RMSE     MAE       MPE     MAPE\nTest set 0.000000004849253 2105508 1592227 -46.79522 72.45567\n\n# Validation Set\naccuracy(valid1, nhl_valid.df$SALARY)\n\n              ME    RMSE     MAE       MPE     MAPE\nTest set 39476.6 1975654 1532076 -38.84478 64.69072\n\n\nWe got overall measures of predictive accuracy, now for MLR model. RMSE value for validation set (1975654) is also smaller than training set (2105508). Same with MAE, for training set is 1592227, and for validation set is 1532076. Small difference between these numbers can suggest that our model is not overfitting.\n\n2105508/sd(nhl_train.df$SALARY)\n\n[1] 0.7052214\n\n1975654/sd(nhl_train.df$SALARY)\n\n[1] 0.6617279\n\n\nCompared to SLR, we got smaller coefficients by comparing RMSE to standard deviation of training set. So, using multiple inputs to predict salary is more efficient than using only points. Our model explains 50% of the variance in salary, which suggests there are other factors that can impact salary of the player. As I mentioned earlier, the reputation of the player, and the budget of the team can play major role. These variables not included in our model."
  },
  {
    "objectID": "projects/groceries/arules.html",
    "href": "projects/groceries/arules.html",
    "title": "Association rules",
    "section": "",
    "text": "For this task, we will be using data from Groceries, a dataset that can be found with the arules package. Each row in the file represents one buyer’s purchases. We will generate item frequency plots, identify strong association rules involving a specific product, and visualize rules using scatter and graph-based methods.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(arules)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nAttaching package: 'arules'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\nlibrary(arulesViz)\n\ndata(\"Groceries\")\nsummary(Groceries)\n\ntransactions as itemMatrix in sparse format with\n 9835 rows (elements/itemsets/transactions) and\n 169 columns (items) and a density of 0.02609146 \n\nmost frequent items:\n      whole milk other vegetables       rolls/buns             soda \n            2513             1903             1809             1715 \n          yogurt          (Other) \n            1372            34055 \n\nelement (itemset/transaction) length distribution:\nsizes\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77   55   46 \n  17   18   19   20   21   22   23   24   26   27   28   29   32 \n  29   14   14    9   11    4    6    1    1    1    1    3    1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   4.409   6.000  32.000 \n\nincludes extended item information - examples:\n       labels  level2           level1\n1 frankfurter sausage meat and sausage\n2     sausage sausage meat and sausage\n3  liver loaf sausage meat and sausage\n\n\nGroceries is of class transactions (sparse matrix). The data consists of 9835 rows, and 169 columns.\n\nitemFrequencyPlot(Groceries, \n                  support=0.0725, \n                  horiz = TRUE, \n                  col = \"ivory\",\n                  main = \"Frequent Grocery items (Support &gt; 7.25%)\")\n\n\n\n\n\n\n\n\nThe bar plot above displays frequent items, that meet the support value. The minimum support threshold is set at 7.25%, meaning that only items appearing in at least 7.25% of all transactions are considered frequent. As a result we got 16 frequent products.\n\nrules &lt;- apriori (Groceries, parameter = list(supp = 0.001, conf = 0.5))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.5    0.1    1 none FALSE            TRUE       5   0.001      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 9 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\nsorting and recoding items ... [157 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 5 6 done [0.01s].\nwriting ... [5668 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n# summary(rules)\n# inspect(rules[1:5])\n# itemLabels(Groceries)\nlhs_rules &lt;- subset(rules, lhs %in% \"cream cheese \")\nrhs_rules &lt;- subset(rules, rhs %in% \"cream cheese \")\n\nsummary(lhs_rules)\n\nset of 233 rules\n\nrule length distribution (lhs + rhs):sizes\n  3   4   5 \n 59 137  37 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.000   3.000   4.000   3.906   4.000   5.000 \n\nsummary of quality measures:\n    support           confidence        coverage             lift       \n Min.   :0.001017   Min.   :0.5000   Min.   :0.001118   Min.   : 1.957  \n 1st Qu.:0.001118   1st Qu.:0.5556   1st Qu.:0.001729   1st Qu.: 2.642  \n Median :0.001220   Median :0.6154   Median :0.002034   Median : 3.138  \n Mean   :0.001536   Mean   :0.6480   Mean   :0.002494   Mean   : 3.563  \n 3rd Qu.:0.001729   3rd Qu.:0.7143   3rd Qu.:0.002847   3rd Qu.: 3.982  \n Max.   :0.006609   Max.   :1.0000   Max.   :0.012405   Max.   :11.041  \n     count      \n Min.   :10.00  \n 1st Qu.:11.00  \n Median :12.00  \n Mean   :15.11  \n 3rd Qu.:17.00  \n Max.   :65.00  \n\nmining info:\n      data ntransactions support confidence\n Groceries          9835   0.001        0.5\n                                                                  call\n apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.5))\n\nsummary(rhs_rules)\n\nset of 1 rules\n\nrule length distribution (lhs + rhs):sizes\n5 \n1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      5       5       5       5       5       5 \n\nsummary of quality measures:\n    support           confidence        coverage             lift      \n Min.   :0.001017   Min.   :0.5882   Min.   :0.001729   Min.   :14.83  \n 1st Qu.:0.001017   1st Qu.:0.5882   1st Qu.:0.001729   1st Qu.:14.83  \n Median :0.001017   Median :0.5882   Median :0.001729   Median :14.83  \n Mean   :0.001017   Mean   :0.5882   Mean   :0.001729   Mean   :14.83  \n 3rd Qu.:0.001017   3rd Qu.:0.5882   3rd Qu.:0.001729   3rd Qu.:14.83  \n Max.   :0.001017   Max.   :0.5882   Max.   :0.001729   Max.   :14.83  \n     count   \n Min.   :10  \n 1st Qu.:10  \n Median :10  \n Mean   :10  \n 3rd Qu.:10  \n Max.   :10  \n\nmining info:\n      data ntransactions support confidence\n Groceries          9835   0.001        0.5\n                                                                  call\n apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.5))\n\n\nLet’s create subset of rules that contain my grocery item - cream cheese.\nThere is 233 rules that contain my product - on the left hand side. 59 rules involve three product subset, 137 four product subset, and 37 five product subset. And we get only 1 rule with cream cheese on right hand side, which is in the subset of five products. Indicating that cream cheese appears in combination with other products.\n\ninspect(sort(rhs_rules, by=\"lift\"))\n\n    lhs                     rhs                 support confidence    coverage     lift count\n[1] {other vegetables,                                                                       \n     curd,                                                                                   \n     yogurt,                                                                                 \n     whipped/sour cream} =&gt; {cream cheese } 0.001016777  0.5882353 0.001728521 14.83409    10\n\n\nLet’s look at the first rule: If a person buys other vegetables, curd, yogurt, and whipped/sour cream this person 14.83 times more likely to buy cream cheese than a random customer in store. The support number is 0.001, meaning that 0.1% of all transactions studied had exact same item sets. Confidence is 0.59 =&gt; If someone buys other vegetables, curd, yogurt, and whipped/sour cream, there’s a 59% chance that they also buy cream cheese. Coverage number gives an idea how often the rule can be applied, in this case it equals to 0.002. This rule applies to 0.2% of all transactions in the dataset.\n\ninspect(sort(lhs_rules, by=\"lift\")[2])\n\n    lhs                    rhs                 support confidence    coverage     lift count\n[1] {citrus fruit,                                                                          \n     other vegetables,                                                                      \n     whole milk,                                                                            \n     cream cheese }     =&gt; {domestic eggs} 0.001118454  0.5789474 0.001931876 9.124916    11\n\n\nThe next rule: If a person buys citrus fruit, other vegetables, whole milk, and cream cheese he/she is 9.12 times more likely to buy domestic eggs than a random purchaser in store. The support number is 0.001, this rule applies to only 0.1% of all transactions. This rule also have high confidence, saying that if customer buys citrus fruit, other vegetables, whole milk, and cream cheese, there is 58% chance they also buy domestic eggs. Coverage number is 0.002, meaning that this combination occurs in 0.2% of all transactions.\nFrom these rules we see that certain sets of products are frequently purchased together. In combination they may be ingredients for salads, or other recipes. Cream cheese, in particular, is commonly used in baking and is a key ingredient in cheesecake. Despite that, cream cheese widely used for frosting, spreads, pasta sauces, dips, making it a versatile ingredient in a variety of dishes.\nBy identifying frequent combinations with cream cheese, the store can strategically place those items nearby—such as positioning cream cheese close to the vegetables/fruits section or within the dairy aisle for convenient access. Also, offering special discounts can boost sales. For example, if a customer buys cream cheese, offering a discount on berries or bagels can encourage bundled purchases. Additionally, analyzing product pairs allows the store to anticipate demand and adjust inventory accordingly, ensuring high-demand combinations are well-stocked ahead of time, especially during peak shopping seasons.\n\ninspect(lhs_rules[7:9])\n\n    lhs                              rhs                support     confidence\n[1] {cream cheese , frozen meals} =&gt; {whole milk}       0.001016777 0.7142857 \n[2] {hard cheese, cream cheese }  =&gt; {other vegetables} 0.001118454 0.5789474 \n[3] {hard cheese, cream cheese }  =&gt; {whole milk}       0.001016777 0.5263158 \n    coverage    lift     count\n[1] 0.001423488 2.795464 10   \n[2] 0.001931876 2.992090 11   \n[3] 0.001931876 2.059815 10   \n\nplot(lhs_rules[7:9])\n\n\n\n\n\n\n\n\nFrom the plot above, we observe the distribution of three association rules based on confidence (y-axis), support (x-axis), and lift (represented by color). First rule: {cream cheese , frozen meals} =&gt; {whole milk} has the highest confidence and lift values but a low support. This means the rule highly reliable: there is 71.43% chance that a customer who buys cream cheese and frozen meals will also buy whole milk. And the strength of the association is strong. However, it applies to 0.10% of the transactions. Second rule: {hard cheese, cream cheese } =&gt; {other vegetables} has a higher support and lift compared to the first rule, but its confidence is lower (57.9%). This suggests that while this combination of products occurs more frequently, it may not be as strong in predicting the purchase of vegetables when a customer buys both hard cheese and cream cheese. Third rule: {hard cheese, cream cheese } =&gt; {whole milk} has a similar support as a first rule, meaning that it applies to the same small proportion of transactions. However, it has lowest confidence and lift values, making it less reliable and significant to customer behaviour. This rule can demonstrate rare combination of items.\n\nplot(lhs_rules[7:9], method = \"graph\", engine=\"htmlwidget\")\n\n\n\n\n\nNow the plot shows the relationship between rules as a graph. The central node represents cream cheese, which appears in all three rules, indicating that it is a key item in these associations. Hard cheese and whole milk appear in two rules each, showing that these items are associated with more than one combination of products. Frozen meals and other vegetables only appear in one rule each, which indicates that they are more specific to particular product combinations. Also the color differentiation in the plot corresponds to the lift value of each rule. The rule 3, which has lowest lift value in represented in light red. Meanwhile, Rules 1 and 2 are highlighted in bold red, suggesting that they have higher lift values and stronger associations between the items. Compared to previous plot, this visual shows elements of rules, allowing to quickly identify the central elements, and the relative strength of the rules. This plot also displays measures of rules (confidence, support) if we click on rule node. However, if we want to select strong rule the scatter plot is more useful because it clearly shows rules with higher support and confidence in a more clearer way. Therefore, the choice of plot depends on the purpose."
  },
  {
    "objectID": "projects/consumer_complaints/naive_bayes.html",
    "href": "projects/consumer_complaints/naive_bayes.html",
    "title": "Complaint Classifier",
    "section": "",
    "text": "As part of my Data Mining coursework, I’m building a Naive Bayes classification model to predict whether a consumer disputes a company’s response using the Consumer Complaints dataset. The project involves cleaning and preprocessing the data, including renaming variables, removing redundancies, and binning categories. I evaluate the model’s performance using confusion matrices and compare it to the naive rule, uncovering key insights into the challenges of predicting dispute outcomes and how predictions can be improved.\nThe source of dataset is Kaggle.\n\nData Exploration\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(ggplot2)\nlibrary(e1071)\n\ncc &lt;- read.csv('consumer_complaints.csv')\nglimpse(cc)\n\nRows: 14,000\nColumns: 16\n$ X.ID                     &lt;int&gt; 1615767, 654223, 1143398, 1303679, 1627370, 1…\n$ Company                  &lt;chr&gt; \"PHH Mortgage\", \"Ocwen\", \"Southwest Credit Sy…\n$ Product                  &lt;chr&gt; \"Mortgage\", \"Mortgage\", \"Debt collection\", \"C…\n$ Issue                    &lt;chr&gt; \"Loan servicing, payments, escrow account\", \"…\n$ State                    &lt;chr&gt; \"FL\", \"NC\", \"MO\", \"WA\", \"VA\", \"IL\", \"FL\", \"OK…\n$ Submitted.via            &lt;chr&gt; \"Web\", \"Web\", \"Web\", \"Web\", \"Web\", \"Web\", \"We…\n$ Date.received            &lt;chr&gt; \"10/20/2015\", \"3/1/2014\", \"4/12/2014\", \"03/26…\n$ Date.resolved            &lt;chr&gt; \"10/20/2015\", \"3/1/2014\", \"4/12/2014\", \"03/26…\n$ Timely.response.         &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Ye…\n$ Consumer.disputed.       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ state.name               &lt;chr&gt; \"Florida\", \"North Carolina\", \"Missouri\", \"Was…\n$ Date.received.1          &lt;chr&gt; \"10/20/2015\", \"1/3/2014\", \"12/4/2014\", \"3/26/…\n$ Date.resolved.1          &lt;chr&gt; \"10/20/2015\", \"1/3/2014\", \"12/4/2014\", \"3/26/…\n$ Resolution.time.in.days. &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 29, 0,…\n$ Year                     &lt;int&gt; 2015, 2014, 2014, 2015, 2015, 2015, 2016, 201…\n$ QTR..US.FLY.             &lt;chr&gt; \"Q4\", \"Q1\", \"Q4\", \"Q1\", \"Q4\", \"Q2\", \"Q2\", \"Q2…\n\n\nNumeric: Resolution.time.in.days, Year\nCategorical: X.ID\n\nstr(cc$Consumer.disputed.)\n\n chr [1:14000] \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" \"No\" ...\n\ncc$Consumer.disputed. &lt;- factor(cc$Consumer.disputed.)\nlevels(cc$Consumer.disputed.)\n\n[1] \"No\"  \"Yes\"\n\ntable(cc$Consumer.disputed.)\n\n\n   No   Yes \n10862  3138 \n\ncc %&gt;% group_by(Consumer.disputed.) %&gt;% \n  summarise(Count = n()/14000)\n\n# A tibble: 2 × 2\n  Consumer.disputed. Count\n  &lt;fct&gt;              &lt;dbl&gt;\n1 No                 0.776\n2 Yes                0.224\n\n\nConsumer.disputed. variable is of type chr, then I converted it to factor. Now it has 2 levels: Yes or No.\nWe see that the data is imbalanced, out of 14,000 complaints only 3138 (22.4%) disputed.\n\nnames(cc)\n\n [1] \"X.ID\"                     \"Company\"                 \n [3] \"Product\"                  \"Issue\"                   \n [5] \"State\"                    \"Submitted.via\"           \n [7] \"Date.received\"            \"Date.resolved\"           \n [9] \"Timely.response.\"         \"Consumer.disputed.\"      \n[11] \"state.name\"               \"Date.received.1\"         \n[13] \"Date.resolved.1\"          \"Resolution.time.in.days.\"\n[15] \"Year\"                     \"QTR..US.FLY.\"            \n\ncc &lt;- cc %&gt;% rename(Resolution.time.in.days = Resolution.time.in.days., \n              Timely.response=Timely.response.,\n              Consumer.disputed=Consumer.disputed.,\n              Quarter = QTR..US.FLY.)\n\nI removed dots from variables that had them at the end, and renamed QTR..US.FLY. to Quarter.\n\nsapply(cc, function(x) length(unique(x)))\n\n                   X.ID                 Company                 Product \n                  14000                    1050                      12 \n                  Issue                   State           Submitted.via \n                     81                      60                       5 \n          Date.received           Date.resolved         Timely.response \n                   1370                    1322                       2 \n      Consumer.disputed              state.name         Date.received.1 \n                      2                      52                    1370 \n        Date.resolved.1 Resolution.time.in.days                    Year \n                   1322                      77                       4 \n                Quarter \n                      4 \n\n\nAfter examining number of unique values in each column, we observe that a few columns contain more than 100 distinct values. Specifically, there are 14,000 unique ID records, 1,050 companies, and over 1,300 date values.\n\ncc &lt;- cc %&gt;% select(-X.ID, -Company, -Date.received, -Date.resolved, -Date.received.1, -Date.resolved.1)\n\nstr(cc)\n\n'data.frame':   14000 obs. of  10 variables:\n $ Product                : chr  \"Mortgage\" \"Mortgage\" \"Debt collection\" \"Credit card\" ...\n $ Issue                  : chr  \"Loan servicing, payments, escrow account\" \"Loan servicing, payments, escrow account\" \"Loan modification,collection,foreclosure\" \"Billing statement\" ...\n $ State                  : chr  \"FL\" \"NC\" \"MO\" \"WA\" ...\n $ Submitted.via          : chr  \"Web\" \"Web\" \"Web\" \"Web\" ...\n $ Timely.response        : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Consumer.disputed      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ state.name             : chr  \"Florida\" \"North Carolina\" \"Missouri\" \"Washington\" ...\n $ Resolution.time.in.days: int  0 0 0 0 0 0 1 0 0 0 ...\n $ Year                   : int  2015 2014 2014 2015 2015 2015 2016 2015 2013 2016 ...\n $ Quarter                : chr  \"Q4\" \"Q1\" \"Q4\" \"Q1\" ...\n\n\n\nsummary(cc)\n\n   Product             Issue              State           Submitted.via     \n Length:14000       Length:14000       Length:14000       Length:14000      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Timely.response    Consumer.disputed  state.name       \n Length:14000       No :10862         Length:14000      \n Class :character   Yes: 3138         Class :character  \n Mode  :character                     Mode  :character  \n                                                        \n                                                        \n                                                        \n Resolution.time.in.days      Year        Quarter         \n Min.   : -1.000         Min.   :2013   Length:14000      \n 1st Qu.:  0.000         1st Qu.:2014   Class :character  \n Median :  0.000         Median :2015   Mode  :character  \n Mean   :  2.006         Mean   :2015                     \n 3rd Qu.:  2.000         3rd Qu.:2016                     \n Max.   :286.000         Max.   :2016                     \n\ntable(cc$State)\n\n\n       AA   AE   AK   AL   AP   AR   AS   AZ   CA   CO   CT   DC   DE   FL   GA \n 110    1    5   19  135    4   62    1  313 1977  251  161   98   80 1255  577 \n  GU   HI   IA   ID   IL   IN   KS   KY   LA   MA   MD   ME   MI   MN   MO   MP \n   1   39   56   51  500  137   78   99  149  292  408   47  373  174  171    1 \n  MS   MT   NC   ND   NE   NH   NJ   NM   NV   NY   OH   OK   OR   PA   PR   RI \n  77   19  443    7   54   65  518   75  171  996  463   93  177  511   26   64 \n  SC   SD   TN   TX   UT   VA   VI   VT   WA   WI   WV   WY \n 194   26  238 1040   77  480    8   29  303  178   32   11 \n\ncc &lt;- cc %&gt;% select(-state.name) %&gt;% \n  filter(Resolution.time.in.days&gt;=0)\n\nstr(cc)\n\n'data.frame':   13997 obs. of  9 variables:\n $ Product                : chr  \"Mortgage\" \"Mortgage\" \"Debt collection\" \"Credit card\" ...\n $ Issue                  : chr  \"Loan servicing, payments, escrow account\" \"Loan servicing, payments, escrow account\" \"Loan modification,collection,foreclosure\" \"Billing statement\" ...\n $ State                  : chr  \"FL\" \"NC\" \"MO\" \"WA\" ...\n $ Submitted.via          : chr  \"Web\" \"Web\" \"Web\" \"Web\" ...\n $ Timely.response        : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Consumer.disputed      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Resolution.time.in.days: int  0 0 0 0 0 0 1 0 0 0 ...\n $ Year                   : int  2015 2014 2014 2015 2015 2015 2016 2015 2013 2016 ...\n $ Quarter                : chr  \"Q4\" \"Q1\" \"Q4\" \"Q1\" ...\n\n\nBy examining the results of the summary() function, we notice an impossible negative number for the resolution date. Additionally, there are two overlapping columns: State and State_name.\nBefore that, when counting unique values by column, we observed that the State column had 60 unique values, while the State_name column had 52. Upon examining the values, it appears that the State column includes more detailed information, possibly encompassing territories and military postal codes. We can also keep Year, Resolution.time.in.days, and Quarter, which are related to the removed columns date_received, and date_resolved.\n\ncc &lt;- cc %&gt;% \n  mutate(Year = cut(Year,\n                    breaks = c(2012, 2014, 2016),\n                    labels = c(\"Earlier period\", \"Later period\"),\n                    right = TRUE))\n\nstr(cc)\n\n'data.frame':   13997 obs. of  9 variables:\n $ Product                : chr  \"Mortgage\" \"Mortgage\" \"Debt collection\" \"Credit card\" ...\n $ Issue                  : chr  \"Loan servicing, payments, escrow account\" \"Loan servicing, payments, escrow account\" \"Loan modification,collection,foreclosure\" \"Billing statement\" ...\n $ State                  : chr  \"FL\" \"NC\" \"MO\" \"WA\" ...\n $ Submitted.via          : chr  \"Web\" \"Web\" \"Web\" \"Web\" ...\n $ Timely.response        : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Consumer.disputed      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Resolution.time.in.days: int  0 0 0 0 0 0 1 0 0 0 ...\n $ Year                   : Factor w/ 2 levels \"Earlier period\",..: 2 1 1 2 2 2 2 2 1 2 ...\n $ Quarter                : chr  \"Q4\" \"Q1\" \"Q4\" \"Q1\" ...\n\ntable(cc$Resolution.time.in.days)\n\n\n   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 \n8316 1616 1122  844  491  473  318  222   83   35   47   37   21   18   18   18 \n  16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31 \n   9   10   16   15   12   22    9    6   13    6    7    2   10   16    5    9 \n  32   33   34   35   36   37   38   39   40   41   42   43   45   46   47   48 \n  14    9    9   11   12    3    3    7    3    8    3    8    5    2    5    1 \n  49   50   51   52   55   56   57   60   61   63   64   65   66   67   68   70 \n   5    3    3    1    6    2    1    2    4    2    1    2    1    1    1    1 \n  72   76   77   79   83   90   93   95   98  106  151  286 \n   1    1    1    1    1    1    1    1    1    1    1    1 \n\nnon_zero_values &lt;- cc$Resolution.time.in.days[cc$Resolution.time.in.days &gt; 0]\nbreaks &lt;- quantile(non_zero_values, probs = seq(0, 1, length.out = 3))\n\nbreaks &lt;- c(0, breaks)\n\ncc$Resolution.time.in.days &lt;- cut(cc$Resolution.time.in.days, breaks = breaks, labels = c(\"low\", \"medium\", \"high\"), include.lowest = TRUE)\n\n\ntable(cc$Resolution.time.in.days)\n\n\n   low medium   high \n  9932   1966   2099 \n\n\nThe Year column contains four unique values: 2013, 2014, 2015, and 2016. For binning, we can group them into two categories: ‘Earlier period’ for 2013 and 2014, and ‘Later period’ for 2015 and 2016.\nThe Resolution_time variable contains a significant number of zeros (8,316) along with other values. This imbalance caused issues when using the equal frequency method for binning. To address this, I first filtered out the non-zero values to determine the breaks within the data. Then, I added zero to this group.\nAs a result, we have three groups: low, medium, and high.\n\nlength(unique(cc$Product))\n\n[1] 12\n\ntop_6_Product &lt;- cc %&gt;% \n  count(Product, sort = TRUE) %&gt;% \n  slice(1:6) %&gt;% \n  select(Product)\n\nlength(unique(cc$Issue))\n\n[1] 81\n\ntop_7_Issue &lt;- cc %&gt;% \n  count(Issue, sort = TRUE) %&gt;% \n  slice(1:7) %&gt;% \n  select(Issue)\n\nlength(unique(cc$State))\n\n[1] 60\n\ntop_10_State &lt;- cc %&gt;% \n  count(State, sort = TRUE) %&gt;% \n  slice(1:10) %&gt;% \n  select(State)\n\ncc &lt;- cc %&gt;% filter(Product %in% top_6_Product$Product & \n                Issue %in% top_7_Issue$Issue &\n                State %in% top_10_State$State)\n\ncc &lt;- cc %&gt;% \n  mutate(Issue = case_when(\n    Issue == \"Account opening, closing, or management\" ~ \"Account Management\",\n    Issue == \"Application, originator, mortgage broker\" ~ \"Mortgage Application\",\n    Issue == \"Communication tactics\" ~ \"Communication\",\n    Issue == \"Credit reporting company's investigation\" ~ \"Credit Investigation\",\n    Issue == \"Deposits and withdrawals\" ~ \"Transactions\",\n    Issue == \"Loan modification,collection,foreclosure\" ~ \"Loan Modification\",\n    Issue == \"Loan servicing, payments, escrow account\" ~ \"Loan Servicing\",\n    TRUE ~ Issue\n  ))\n\nnrow(cc)\n\n[1] 4110\n\n\nFor the next few steps, we are going to be reducing the number of unique levels for some of our factor variables:\nThe Product column has 12 unique values, but we will use only the 6 most common ones.\nThe Issue column contains 81 unique values, and we will use only the 7 most common ones. Also changed these 7 values to the shorter names.\nThe State column has 60 unique values, and we will use the 10 most common ones.\nAfter reducing the unique levels, the dataset now contains 4110 rows.\n\nstr(cc)\n\n'data.frame':   4110 obs. of  9 variables:\n $ Product                : chr  \"Mortgage\" \"Mortgage\" \"Credit reporting\" \"Debt collection\" ...\n $ Issue                  : chr  \"Loan Servicing\" \"Mortgage Application\" \"Credit Investigation\" \"Loan Modification\" ...\n $ State                  : chr  \"FL\" \"FL\" \"CA\" \"NY\" ...\n $ Submitted.via          : chr  \"Web\" \"Web\" \"Web\" \"Email\" ...\n $ Timely.response        : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Consumer.disputed      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 2 ...\n $ Resolution.time.in.days: Factor w/ 3 levels \"low\",\"medium\",..: 1 1 1 1 1 3 3 1 3 1 ...\n $ Year                   : Factor w/ 2 levels \"Earlier period\",..: 2 2 2 1 2 1 2 2 2 1 ...\n $ Quarter                : chr  \"Q4\" \"Q2\" \"Q1\" \"Q4\" ...\n\ncc$Product &lt;- as.factor(cc$Product)\ncc$Issue &lt;- as.factor(cc$Issue)\ncc$State &lt;- as.factor(cc$State)\ncc$Submitted.via &lt;- as.factor(cc$Submitted.via)\ncc$Timely.response &lt;- as.factor(cc$Timely.response)\ncc$Year &lt;- as.factor(cc$Year)\ncc$Quarter &lt;- as.factor(cc$Quarter)\n\n\n\nData Partitioning\n\nset.seed(79)\ncc.index &lt;- sample(c(1:nrow(cc)), nrow(cc)*0.6)\ncc_train.df &lt;- cc[cc.index, ]\ncc_valid.df &lt;- cc[-cc.index, ]\n\nAfter converting all variables to factor type, I partitioned the data into training(60%) and validation(40%) sets.\n\n\nData Visualization - Proportional Barplot\n\nggplot(cc_train.df, aes(x = Product, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Product\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Issue, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  # coord_flip() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(x = \"Issue\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = State, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"State\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Submitted.via, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Submitted.via\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Timely.response, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Timely.response\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Year, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Year\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Quarter, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Quarter\", y = \"Proportion\")\n\n\n\n\n\n\n\nggplot(cc_train.df, aes(x = Resolution.time.in.days, fill = Consumer.disputed)) +\n    geom_bar(position = 'fill') +\n  labs(x = \"Resolution.time.in.days\", y = \"Proportion\")\n\n\n\n\n\n\n\n\nFor the Year variable, we observed a similar proportion between time periods and whether consumers disputed or not. The same applies to the Quarter variable, with only a small difference in the third quarter. Timely.response showed better result with noticeable differences between values compared to Resolution.time.in.days. For the remaining variables, we can see distinct differences between categories.\nI will remove Resolution.time.in.days, because of the imbalance of values in dataset.\n\ncc_train.df &lt;- cc_train.df %&gt;% select(-Year, -Quarter, -Resolution.time.in.days)\nstr(cc_train.df)\n\n'data.frame':   2466 obs. of  6 variables:\n $ Product          : Factor w/ 4 levels \"Bank account or service\",..: 3 1 4 3 4 3 1 3 3 4 ...\n $ Issue            : Factor w/ 7 levels \"Account Management\",..: 2 1 5 4 5 4 1 2 2 5 ...\n $ State            : Factor w/ 10 levels \"CA\",\"FL\",\"GA\",..: 2 5 2 8 8 5 4 3 9 1 ...\n $ Submitted.via    : Factor w/ 5 levels \"Email\",\"Fax\",..: 5 5 5 4 5 4 1 5 5 5 ...\n $ Timely.response  : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Consumer.disputed: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n\ncc.nb &lt;- naiveBayes(Consumer.disputed ~.,data = cc_train.df)\ncc.nb\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n       No       Yes \n0.7704785 0.2295215 \n\nConditional probabilities:\n     Product\nY     Bank account or service Credit reporting Debt collection   Mortgage\n  No               0.27210526       0.08368421      0.22473684 0.41947368\n  Yes              0.22261484       0.10424028      0.24558304 0.42756184\n\n     Issue\nY     Account Management Communication Credit Investigation Loan Modification\n  No          0.15947368    0.11105263           0.08368421        0.11368421\n  Yes         0.15724382    0.08303887           0.10424028        0.16254417\n     Issue\nY     Loan Servicing Mortgage Application Transactions\n  No      0.34421053           0.07526316   0.11263158\n  Yes     0.32862191           0.09893993   0.06537102\n\n     State\nY             CA         FL         GA         IL         NJ         NY\n  No  0.22578947 0.15631579 0.07210526 0.06526316 0.06000000 0.11789474\n  Yes 0.26501767 0.15901060 0.07950530 0.04416961 0.06890459 0.09717314\n     State\nY             OH         PA         TX         VA\n  No  0.06000000 0.06000000 0.12736842 0.05526316\n  Yes 0.03533569 0.05123675 0.13074205 0.06890459\n\n     Submitted.via\nY           Email         Fax       Phone Postal mail         Web\n  No  0.176315789 0.011052632 0.089473684 0.042631579 0.680526316\n  Yes 0.128975265 0.003533569 0.051236749 0.037102473 0.779151943\n\n     Timely.response\nY             No        Yes\n  No  0.01684211 0.98315789\n  Yes 0.01060071 0.98939929\n\n\n\nstr(cc_train.df)\n\n'data.frame':   2466 obs. of  6 variables:\n $ Product          : Factor w/ 4 levels \"Bank account or service\",..: 3 1 4 3 4 3 1 3 3 4 ...\n $ Issue            : Factor w/ 7 levels \"Account Management\",..: 2 1 5 4 5 4 1 2 2 5 ...\n $ State            : Factor w/ 10 levels \"CA\",\"FL\",\"GA\",..: 2 5 2 8 8 5 4 3 9 1 ...\n $ Submitted.via    : Factor w/ 5 levels \"Email\",\"Fax\",..: 5 5 5 4 5 4 1 5 5 5 ...\n $ Timely.response  : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 2 2 2 2 2 ...\n $ Consumer.disputed: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n\n\n\n\nConfusion Matrix\n\nconfusionMatrix(predict(cc.nb, newdata=cc_train.df), cc_train.df$Consumer.disputed)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  1900  566\n       Yes    0    0\n                                          \n               Accuracy : 0.7705          \n                 95% CI : (0.7534, 0.7869)\n    No Information Rate : 0.7705          \n    P-Value [Acc &gt; NIR] : 0.5113          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.7705          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.7705          \n         Detection Rate : 0.7705          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : No              \n                                          \n\nconfusionMatrix(predict(cc.nb, newdata=cc_valid.df), cc_valid.df$Consumer.disputed)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  1224  420\n       Yes    0    0\n                                          \n               Accuracy : 0.7445          \n                 95% CI : (0.7227, 0.7655)\n    No Information Rate : 0.7445          \n    P-Value [Acc &gt; NIR] : 0.5131          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.7445          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.7445          \n         Detection Rate : 0.7445          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : No              \n                                          \n\n\nComparing the accuracy metrics, the validation set achieved 74.45%, which is lower than the training set’s 77.05%. However, both sets reveal that the model predominantly predicts ‘No’ for all instances (100%). a.The model predicts ‘No’ for every case and never predicts ‘Yes’, indicating an imbalance among predictions. Although the accuracy appears relatively high, it is misleading as it reflects the model’s bias rather than its true predictive power.\n\n\nNaive Rule vs Naive Bayes\nNaive rule in classification is to classify the record as a member of majority class. If we had used the naive rule for classification, we would classify all records in the training set as “No” because “No” is the most frequent class.\nOur Naive Bayes model, which follows the naive rule approach, assigns all cases as “No.” Both methods yield the same accuracy of 77.05%, resulting in a 0% difference between them.\nI think imbalance plays a big role here. As described in the book, the absence of this predictor actively “outvotes” any other information in the record to assign a “No” to the outcome value (when, in this case, it has a relatively good chance of being a “Yes”). Also, as a customer, I often choose not to dispute issues to avoid wasting energy. It’s usually easier to let things go rather than engage in disputes, especially if the potential outcome doesn’t seem worth the effort. Maybe it’s a reason why we don’t have meaningful data.\n\n\nScoring data using Naive Bayes\n\npred.prob &lt;- predict(cc.nb, newdata=cc_valid.df, type=\"raw\")\n# pred.prob\n\npred.class &lt;- predict(cc.nb, newdata=cc_valid.df)\n# pred.class\n\ndf &lt;- data.frame(actual=cc_valid.df$Consumer.disputed,\n                 predicted=pred.class, pred.prob)\n\nvalid_25 &lt;- df %&gt;% arrange(desc(Yes)) %&gt;% slice(1:25)\n\ntable(valid_25$actual)\n\n\n No Yes \n 20   5 \n\nvalid_25 %&gt;% filter(actual == 'Yes')\n\n  actual predicted        No       Yes\n1    Yes        No 0.5992883 0.4007117\n2    Yes        No 0.6009522 0.3990478\n3    Yes        No 0.6009522 0.3990478\n4    Yes        No 0.6137075 0.3862925\n5    Yes        No 0.6137075 0.3862925\n\n\nI took 25 records by sorting the probability for the “Yes” column in descending order, selecting the top 25 as the most likely to belong to the “YES” group.\nAmong these 25 records, 5 records truly belong to “Yes” group. The accuracy for these predictions = 80%. Even though the model didn’t predict any “Yes” values, it still achieved 80% accuracy by correctly classifying 20 out of 20 “No” records. Since the “Yes” group is a small portion of the dataset, its impact on accuracy is minimal. Compared to accuracy of overall model 74.45%, these selected proportion of data have reslatively high value.\nIdentifying this subset of records helps us to see that the model completely fails at identifying “Yes” cases. On the other hand, by assigning all records to the majority class “No,” the model achieves high accuracy, performing well in most cases. By identifying this main issue, we can focus on other ways to dealing with imbalanced data or try other models.\n\n\nManual calculation of probability\n\nmy_data &lt;- cc_train.df[45,]\nmy_data\n\n      Product                Issue State Submitted.via Timely.response\n3560 Mortgage Mortgage Application    IL           Web              No\n     Consumer.disputed\n3560                No\n\npredict(cc.nb, my_data)\n\n[1] No\nLevels: No Yes\n\npredict(cc.nb, my_data, type=\"raw\")\n\n            No       Yes\n[1,] 0.8370455 0.1629545\n\ncc.nb\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n       No       Yes \n0.7704785 0.2295215 \n\nConditional probabilities:\n     Product\nY     Bank account or service Credit reporting Debt collection   Mortgage\n  No               0.27210526       0.08368421      0.22473684 0.41947368\n  Yes              0.22261484       0.10424028      0.24558304 0.42756184\n\n     Issue\nY     Account Management Communication Credit Investigation Loan Modification\n  No          0.15947368    0.11105263           0.08368421        0.11368421\n  Yes         0.15724382    0.08303887           0.10424028        0.16254417\n     Issue\nY     Loan Servicing Mortgage Application Transactions\n  No      0.34421053           0.07526316   0.11263158\n  Yes     0.32862191           0.09893993   0.06537102\n\n     State\nY             CA         FL         GA         IL         NJ         NY\n  No  0.22578947 0.15631579 0.07210526 0.06526316 0.06000000 0.11789474\n  Yes 0.26501767 0.15901060 0.07950530 0.04416961 0.06890459 0.09717314\n     State\nY             OH         PA         TX         VA\n  No  0.06000000 0.06000000 0.12736842 0.05526316\n  Yes 0.03533569 0.05123675 0.13074205 0.06890459\n\n     Submitted.via\nY           Email         Fax       Phone Postal mail         Web\n  No  0.176315789 0.011052632 0.089473684 0.042631579 0.680526316\n  Yes 0.128975265 0.003533569 0.051236749 0.037102473 0.779151943\n\n     Timely.response\nY             No        Yes\n  No  0.01684211 0.98315789\n  Yes 0.01060071 0.98939929\n\nno_score &lt;- 0.7704785 * 0.41947368 * 0.07526316 * 0.06526316 * 0.680526316 * 0.01684211\nyes_score &lt;- 0.2295215 * 0.42756184 * 0.09893993 * 0.04416961 * 0.779151943 * 0.01060071\n\nno_score/(no_score + yes_score)\n\n[1] 0.8370455\n\n\nI selected 45th row from training set.\n\nActual Consumer.disputed outcome is “No”\nThe model’s predicted answer is “No”\nThe probability for “No” is 0.8370455, “Yes” is 0.1629545."
  },
  {
    "objectID": "projects/vortex/main.html",
    "href": "projects/vortex/main.html",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "",
    "text": "Code\nimport yfinance as yf  # For downloading financial data\nimport numpy as np      # For numerical operations\nimport pandas as pd     # For data manipulation\nimport requests # For downloading the API data\nimport numpy as np \nimport plotly.graph_objects as go\nimport plotly.express as px # Import the Plotly Express module for interactive visualization\nimport json\nimport vectorbt as vbt\nfrom plotly.subplots import make_subplots\nimport streamlit as st\n\nimport plotly.io as pio\npio.renderers.default = 'iframe_connected'\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn("
  },
  {
    "objectID": "projects/vortex/main.html#importing-necessary-libraries-for-analysis",
    "href": "projects/vortex/main.html#importing-necessary-libraries-for-analysis",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "",
    "text": "Code\nimport yfinance as yf  # For downloading financial data\nimport numpy as np      # For numerical operations\nimport pandas as pd     # For data manipulation\nimport requests # For downloading the API data\nimport numpy as np \nimport plotly.graph_objects as go\nimport plotly.express as px # Import the Plotly Express module for interactive visualization\nimport json\nimport vectorbt as vbt\nfrom plotly.subplots import make_subplots\nimport streamlit as st\n\nimport plotly.io as pio\npio.renderers.default = 'iframe_connected'\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn("
  },
  {
    "objectID": "projects/vortex/main.html#data-collection",
    "href": "projects/vortex/main.html#data-collection",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "Data Collection",
    "text": "Data Collection\n\nFetch daily OHLCV data\n\n\nCode\n# Data for the TSLA, XLY, and SPY tickers is retrieved from the Yahoo Finance library, covering the period from January 1, 2019, \n# to March 5, 2025.\ntsla = yf.download('TSLA', start='2019-01-01', end='2025-03-05') \nxly = yf.download('XLY', start='2019-01-01', end='2025-03-05')\nspy = yf.download('SPY', start='2019-01-01', end='2025-03-05')\n\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\ndef multiindex_to_singleindex(df):\n    df.columns = ['_'.join(col).strip() for col in df.columns.values]\n    return df\n\n\n\n\nCode\ntsla = multiindex_to_singleindex(tsla)\nspy = multiindex_to_singleindex(spy)\nxly = multiindex_to_singleindex(xly)\n\n\n\n\nCode\n# Displays a summary of the TSLA DataFrame, including column names, data types, non-null counts, and memory usage.\ntsla.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1551 entries, 2019-01-02 to 2025-03-04\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Close_TSLA   1551 non-null   float64\n 1   High_TSLA    1551 non-null   float64\n 2   Low_TSLA     1551 non-null   float64\n 3   Open_TSLA    1551 non-null   float64\n 4   Volume_TSLA  1551 non-null   int64  \ndtypes: float64(4), int64(1)\nmemory usage: 72.7 KB\n\n\n\n\nCode\n# Displays a summary of the XLY DataFrame, including column names, data types, non-null counts, and memory usage.\nxly.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1551 entries, 2019-01-02 to 2025-03-04\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Close_XLY   1551 non-null   float64\n 1   High_XLY    1551 non-null   float64\n 2   Low_XLY     1551 non-null   float64\n 3   Open_XLY    1551 non-null   float64\n 4   Volume_XLY  1551 non-null   int64  \ndtypes: float64(4), int64(1)\nmemory usage: 72.7 KB\n\n\n\n\nCode\n# Displays a summary of the SPY DataFrame, including column names, data types, non-null counts, and memory usage.\nspy.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1551 entries, 2019-01-02 to 2025-03-04\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Close_SPY   1551 non-null   float64\n 1   High_SPY    1551 non-null   float64\n 2   Low_SPY     1551 non-null   float64\n 3   Open_SPY    1551 non-null   float64\n 4   Volume_SPY  1551 non-null   int64  \ndtypes: float64(4), int64(1)\nmemory usage: 72.7 KB\n\n\n\n\nFetch sentiment scores from the API\n\n\nCode\ndef get_news_sentiment(ticker, start_date, end_date, limit, api_key):\n    url = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&time_from={start_date}&time_to={end_date}&limit={limit}&tickers={ticker}&apikey={api_key}'\n    response = requests.get(url)\n    if response.status_code == 200:\n        sentiment_data = response.json()\n\n        with open(f'{ticker}_sentiment_raw.json', \"w\") as f:\n            json.dump(sentiment_data, f, indent=4)\n        # sentiment_df = pd.DataFrame(sentiment_data['feed'])\n        return \"Full sentiment JSON saved ✅\"\n    else:\n        print(\"API call failed:\", response.status_code)\n        return None\n\n\n\n\nCode\nget_news_sentiment('TSLA', '20250101T0130', '20250301T0130', 1000, 'PNM5EHRALIOT1CKJ')\n\n\n'Full sentiment JSON saved ✅'"
  },
  {
    "objectID": "projects/vortex/main.html#indicator-calculation",
    "href": "projects/vortex/main.html#indicator-calculation",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "Indicator Calculation",
    "text": "Indicator Calculation\n\nCompute VI+ and VI-\n\n\nCode\n# Defines a function to calculate the Vortex Indicator (VI) for a given DataFrame and ticker symbol.\n# The calculation uses a default lookback period of 14 days unless specified otherwise.\ndef calculate_vortex(df, value, n=14):\n    # Extracts the high, low, and close price series for the specified ticker.\n    high = df[(\"High_\"+value)]\n    low = df[(\"Low_\"+value)]\n    close = df[(\"Close_\"+value)]\n\n    # Calculates the Vortex Movement values:\n    # VM+ = absolute difference between today's high and yesterday's low\n    # VM− = absolute difference between today's low and yesterday's high\n    vm_plus = abs(high - low.shift(1))     # |Today's High – Yesterday’s Low|\n    vm_minus = abs(low - high.shift(1))    # |Today's Low – Yesterday’s High|\n\n    # Computes the True Range (TR) as the maximum of:\n    # - High - Low\n    # - Absolute difference between High and Previous Close\n    # - Absolute difference between Low and Previous Close\n    tr = pd.concat([\n        high - low,\n        abs(high - close.shift(1)),\n        abs(low - close.shift(1))\n    ], axis=1).max(axis=1)\n\n    # Applies a rolling window to compute the n-period sum of VM+ and VM− values\n    # and the corresponding True Range values.\n    sum_vm_plus = vm_plus.rolling(window=n).sum()\n    sum_vm_minus = vm_minus.rolling(window=n).sum()\n    sum_tr = tr.rolling(window=n).sum()\n\n    # Calculates the Vortex Indicator components:\n    # VI+ = sum of VM+ over n periods divided by sum of TR over n periods\n    # VI− = sum of VM− over n periods divided by sum of TR over n periods\n    vi_plus = sum_vm_plus / sum_tr\n    vi_minus = sum_vm_minus / sum_tr\n\n    # Returns the VI+ and VI− series as output.\n    return vi_plus, vi_minus\n\n\n\n\nCode\n# Calculates the Vortex Indicator values for TSLA and stores the results as new columns in the DataFrame.\ntsla['VI+'], tsla['VI-'] = calculate_vortex(tsla, 'TSLA')\n\n# Calculates the Vortex Indicator values for XLY and stores the results as new columns in the DataFrame.\nxly['VI+'], xly['VI-'] = calculate_vortex(xly, 'XLY')\n\n# Calculates the Vortex Indicator values for SPY and stores the results as new columns in the DataFrame.\nspy['VI+'], spy['VI-'] = calculate_vortex(spy, 'SPY')\n\n\n\n\nCode\n# Displays the first 20 rows of the TSLA DataFrame to provide an initial overview of its structure and content with the new function applied.\ntsla.head(20)\n\n\n\n\n\n\n\n\n\nClose_TSLA\nHigh_TSLA\nLow_TSLA\nOpen_TSLA\nVolume_TSLA\nVI+\nVI-\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2019-01-02\n20.674667\n21.008667\n19.920000\n20.406668\n174879000\nNaN\nNaN\n\n\n2019-01-03\n20.024000\n20.626667\n19.825333\n20.466667\n104478000\nNaN\nNaN\n\n\n2019-01-04\n21.179333\n21.200001\n20.181999\n20.400000\n110911500\nNaN\nNaN\n\n\n2019-01-07\n22.330667\n22.449333\n21.183332\n21.448000\n113268000\nNaN\nNaN\n\n\n2019-01-08\n22.356667\n22.934000\n21.801332\n22.797333\n105127500\nNaN\nNaN\n\n\n2019-01-09\n22.568666\n22.900000\n22.098000\n22.366667\n81493500\nNaN\nNaN\n\n\n2019-01-10\n22.997999\n23.025999\n22.119333\n22.293333\n90846000\nNaN\nNaN\n\n\n2019-01-11\n23.150667\n23.227333\n22.584667\n22.806000\n75586500\nNaN\nNaN\n\n\n2019-01-14\n22.293333\n22.833332\n22.266666\n22.825333\n78709500\nNaN\nNaN\n\n\n2019-01-15\n22.962000\n23.253332\n22.299999\n22.333332\n90849000\nNaN\nNaN\n\n\n2019-01-16\n23.070000\n23.466667\n22.900000\n22.985332\n70375500\nNaN\nNaN\n\n\n2019-01-17\n23.153999\n23.433332\n22.943333\n23.080667\n55150500\nNaN\nNaN\n\n\n2019-01-18\n20.150667\n21.808666\n19.982000\n21.533333\n362262000\nNaN\nNaN\n\n\n2019-01-22\n19.927999\n20.533333\n19.700001\n20.321333\n181000500\nNaN\nNaN\n\n\n2019-01-23\n19.172667\n19.633333\n18.779333\n19.500000\n187950000\n0.938520\n0.946160\n\n\n2019-01-24\n19.434000\n19.578667\n18.618668\n18.868668\n120183000\n0.937771\n0.927867\n\n\n2019-01-25\n19.802668\n19.901333\n19.303333\n19.625999\n108744000\n0.969095\n0.953411\n\n\n2019-01-28\n19.758667\n19.830667\n19.183332\n19.527332\n96349500\n0.886399\n1.047633\n\n\n2019-01-29\n19.830667\n19.903999\n19.453333\n19.684668\n69325500\n0.853825\n1.081611\n\n\n2019-01-30\n20.584667\n20.600000\n19.899332\n20.030001\n168754500\n0.859650\n1.020518\n\n\n\n\n\n\n\n\n\nCalculate Volume-Weighted Sentiment\n\n\nCode\ndef json_reader(ticker):\n    with open(f'{ticker}_sentiment_raw.json', \"r\") as f:\n        sentiment_json_ticker = json.load(f)\n        sentiment_feed = sentiment_json_ticker.get(\"feed\", [])\n        sentiment_data = []\n        # Iterate through each item in the sentiment feed to extract relevant fields\n        for item in sentiment_feed:\n            try:\n                sentiment_data.append({\n                    # Convert the timestamp to pandas datetime for proper indexing\n                    \"time_published\": pd.to_datetime(item[\"time_published\"]),\n                    # Convert the sentiment score string to float\n                    \"sentiment_score\": float(item[\"overall_sentiment_score\"]),\n                    # Store the sentiment label (e.g., Positive, Neutral, Negative)\n                    \"sentiment_label\": item[\"overall_sentiment_label\"],\n                })\n            except (KeyError, ValueError, TypeError):\n                # Skip malformed or incomplete entries that raise an error\n                continue    \n        # Convert the structured list of dictionaries into a pandas DataFrame\n        sentiment_df = pd.DataFrame(sentiment_data)\n        # Set the 'time_published' column as the DataFrame index to enable time-series operations\n        # sentiment_df.set_index(\"time_published\", inplace=True)\n        sentiment_df['time_published']= pd.to_datetime(sentiment_df['time_published'].dt.date)\n    return sentiment_df\n    # globals()[f\"{ticker.lower()}_sentiment_data\"] = sentiment_data\n\n\n\n\nCode\ntsla_sentiment_df = json_reader('TSLA')\n\n\n\n\nCode\ntsla_sentiment_df.head()\n\n\n\n\n\n\n\n\n\ntime_published\nsentiment_score\nsentiment_label\n\n\n\n\n0\n2025-03-01\n0.225994\nSomewhat-Bullish\n\n\n1\n2025-02-28\n-0.098739\nNeutral\n\n\n2\n2025-02-28\n-0.041235\nNeutral\n\n\n3\n2025-02-28\n-0.038786\nNeutral\n\n\n4\n2025-02-28\n0.021961\nNeutral\n\n\n\n\n\n\n\n\n\nCode\ntsla_sentiment_scores_filtered = tsla_sentiment_df[(tsla_sentiment_df['time_published']).isin(tsla.index)]\ntsla_sentiment_scores_filtered = tsla_sentiment_scores_filtered.groupby('time_published')['sentiment_score'].mean().reset_index()\n\n\n\n\nCode\ntsla_merged_data = pd.merge(\n    tsla['Volume_TSLA'].reset_index().rename(columns={'Volume_TSLA': 'Volume'}),\n    tsla_sentiment_scores_filtered,\n    left_on='Date',\n    right_on='time_published',\n    how='inner'\n)\n# Compute the weighted sentiment by multiplying raw sentiment by trading volume\ntsla_merged_data['Weighted_Sentiment'] = tsla_merged_data['Volume'] * tsla_merged_data['sentiment_score']\n\n# Calculate a 5-day rolling average of the weighted sentiment to smooth short-term noise\ntsla_merged_data['5_day_avg_sentiment'] = tsla_merged_data['Weighted_Sentiment'].rolling(window=5).mean()\n\n# Define a binary condition for when the average sentiment is positive\ntsla_merged_data['Buy_Condition'] = tsla_merged_data['5_day_avg_sentiment'] &gt; 0\n\n# Normalize the rolling sentiment score by average volume to allow comparability across scales\ntsla_merged_data['5_day_avg_sentiment_norm'] = (\n    tsla_merged_data['5_day_avg_sentiment'] / tsla_merged_data['Volume'].mean()\n)\n\n\n\n\nCode\ntsla_merged_data.head()\n\n\n\n\n\n\n\n\n\nDate\nVolume\ntime_published\nsentiment_score\nWeighted_Sentiment\n5_day_avg_sentiment\nBuy_Condition\n5_day_avg_sentiment_norm\n\n\n\n\n0\n2025-01-31\n83568200\n2025-01-31\n0.194614\n1.626354e+07\nNaN\nFalse\nNaN\n\n\n1\n2025-02-03\n93732100\n2025-02-03\n0.129243\n1.211426e+07\nNaN\nFalse\nNaN\n\n\n2\n2025-02-04\n57072200\n2025-02-04\n0.173107\n9.879602e+06\nNaN\nFalse\nNaN\n\n\n3\n2025-02-05\n57223300\n2025-02-05\n0.136874\n7.832396e+06\nNaN\nFalse\nNaN\n\n\n4\n2025-02-06\n77918200\n2025-02-06\n0.118095\n9.201782e+06\n1.105832e+07\nTrue\n0.132787\n\n\n\n\n\n\n\n\n\nDerive ATR (10) for Volatility Adjustments\n\n\nCode\ndef calculate_true_range(df, ticker):\n    df[\"prev_close\"] = df[f'Close_{ticker}'].shift(1)\n    df[\"tr1\"] = df[f'High_{ticker}'] - df[f'Low_{ticker}']\n    df[\"tr2\"] = abs(df[f'High_{ticker}'] - df[\"prev_close\"])\n    df[\"tr3\"] = abs(df[f'Low_{ticker}'] - df[\"prev_close\"])\n    df[\"true_range\"] = df[[\"tr1\", \"tr2\", \"tr3\"]].max(axis=1)\n    df[\"ATR_10\"] = df[\"true_range\"].rolling(window=10).mean()\n    df[\"atr_pct\"] = df[\"ATR_10\"] / df[f'Close_{ticker}']\n    return df\n\ndef position_size(row):\n    if row[\"atr_pct\"] &lt; 0.03:  # &lt; 3% volatility → low risk\n        return 0.01  # allocate 1% of capital\n    else:  # ≥ 3% volatility → high risk\n        return 0.005  # allocate 0.5% of capital\n\n\n\n\nCode\ntsla = calculate_true_range(tsla, 'TSLA')\ntsla[\"position_size\"] = tsla.apply(position_size, axis=1)\n\n# ---- Preview ----\nprint(tsla[[\"Close_TSLA\", \"ATR_10\", \"atr_pct\", \"position_size\"]].tail(10))\n\n\n            Close_TSLA     ATR_10   atr_pct  position_size\nDate                                                      \n2025-02-19  360.559998  16.703000  0.046325          0.005\n2025-02-20  354.399994  16.464999  0.046459          0.005\n2025-02-21  337.799988  17.021997  0.050391          0.005\n2025-02-24  330.529999  16.770996  0.050740          0.005\n2025-02-25  302.799988  18.879996  0.062351          0.005\n2025-02-26  290.799988  18.412994  0.063318          0.005\n2025-02-27  281.950012  18.257996  0.064756          0.005\n2025-02-28  292.980011  18.067996  0.061670          0.005\n2025-03-03  284.649994  19.281998  0.067739          0.005\n2025-03-04  272.040009  20.654996  0.075926          0.005\n\n\n\n\nCode\n# Create a line chart to visualize the ATR% (Average True Range as a percentage of price) over time\nfig_atr_tsla = px.line(tsla, x=tsla.index, y=\"atr_pct\", title=\"ATR% Over Time\")\n\n# Add a horizontal reference line at 3% to represent the low-volatility cutoff threshold\nfig_atr_tsla.add_hline(\n    y=0.03, \n    line_dash=\"dot\", \n    line_color=\"green\", \n    annotation_text=\"Low Volatility Cutoff\"\n)\n\n# Display the chart\nfig_atr_tsla.show()\n# Display in Streamlit\n# st.subheader(\"ATR% Over Time for TSLA\")\n# st.plotly_chart(fig_atr_tsla, use_container_width=True)\n\n\n\n\n\nThe chart illustrates the historical volatility of TSLA, measured by the Average True Range (ATR) as a percentage of the closing price. Periods where the ATR% falls below the dotted green line at 3% indicate low volatility, which is typically associated with more stable market conditions. In contrast, noticeable spikes—such as those seen in 2020 and 2021—reflect periods of heightened volatility. More recently, ATR% values appear to remain closer to or slightly above the low-volatility threshold, suggesting relatively calmer market behavior compared to earlier years.\n\n\nCode\n# Filter the TSLA DataFrame to include only records from the year 2025\ntsla_2025 = tsla[tsla.index.year == 2025]\n\n# Create a line chart to visualize ATR% for TSLA during 2025\nfig = px.line(\n    tsla_2025,\n    x=tsla_2025.index,\n    y=\"atr_pct\",\n    title=\"TSLA ATR% Over Time (2025 Only)\"\n)\n\n# Add a horizontal line at the 3% threshold to denote the low-volatility cutoff\nfig.add_hline(\n    y=0.03,\n    line_dash=\"dot\",\n    line_color=\"green\",\n    annotation_text=\"Low Volatility Cutoff\"\n)\n\n# Display the chart\nfig.show()\n\n\n\n\n\nThe chart displays ATR% for TSLA during 2025, reflecting how the stock’s volatility has evolved since the start of the year. While ATR% began above the 7% mark in early January, it gradually declined and remained mostly between 4% and 6% throughout February. Although volatility did not breach the low-volatility threshold of 3%, the dip toward that level suggests a period of relative calm. Toward early March, ATR% showed a clear upward trend, indicating a potential resurgence in market volatility.\n\n\nCode\ndef signal_generation(df, ticker):\n    df['atr_pct'] = df['ATR_10'] / df['Close_' + ticker]\n\n    # Create Buy Signal (assuming VI_Cross_Up is defined elsewhere)\n    df['Buy_Signal'] = df['VI+'] &gt; df['VI-']  # Vortex crossover\n    # + add any other buy conditions here...\n\n    # Create Sell Signal (basic)\n    df['Sell_Signal'] = df['VI-'] &gt; df['VI+']\n\n    # Initialize position state\n    df['Position'] = 0\n    peak_price = 0\n\n    for i in range(1, len(df)):\n        if df['Buy_Signal'].iloc[i]:\n            df.at[df.index[i], 'Position'] = 1\n            peak_price = df['Close_' + ticker].iloc[i]\n        elif df['Position'].iloc[i - 1] == 1:\n            current_price = df['Close_' + ticker].iloc[i]\n            peak_price = max(peak_price, current_price)\n            drawdown = (peak_price - current_price) / peak_price\n\n            if drawdown &gt;= 0.03:\n                df.at[df.index[i], 'Sell_Signal'] = True  # trailing stop\n                df.at[df.index[i], 'Position'] = 0\n            else:\n                df.at[df.index[i], 'Position'] = 1    \n    return df\n\n\n\n\nCode\ntsla = signal_generation(tsla, 'TSLA')\n# Display the total number of buy and sell signals generated across the dataset\nprint(\"Buy signals:\", tsla['Buy_Signal'].sum())\nprint(\"Sell signals:\", tsla['Sell_Signal'].sum())\n\n\nBuy signals: 857\nSell signals: 680\n\n\n\n\nCode\n# Create an empty figure object\nfig = go.Figure()\n\n# Plot the TSLA closing price as a continuous line\nfig.add_trace(go.Scatter(\n    x=tsla.index,\n    y=tsla['Close_TSLA'],\n    mode='lines',\n    name='TSLA Price'\n))\n\n# Add markers to indicate Buy Signals using upward-pointing green triangles\nfig.add_trace(go.Scatter(\n    x=tsla[tsla['Buy_Signal']].index,\n    y=tsla[tsla['Buy_Signal']]['Close_TSLA'],\n    mode='markers',\n    marker=dict(symbol='triangle-up', size=10, color='green'),\n    name='Buy Signal'\n))\n\n# Add markers to indicate Sell Signals using downward-pointing red triangles\nfig.add_trace(go.Scatter(\n    x=tsla[tsla['Sell_Signal']].index,\n    y=tsla[tsla['Sell_Signal']]['Close_TSLA'],\n    mode='markers',\n    marker=dict(symbol='triangle-down', size=10, color='red'),\n    name='Sell Signal'\n))\n\n# Update layout settings including title and visual style\nfig.update_layout(\n    title='TSLA Buy & Sell Signals',\n    template='plotly_white'\n)\n\n# Render the interactive plot\nfig.show()\n\n\n\n\n\nThe chart illustrates the closing price of Tesla stock over time, with overlaid trading signals generated by the strategy. Green upward triangles represent buy signals, while red downward triangles mark sell signals. These signals are distributed throughout periods of both rising and falling prices, reflecting how the algorithm dynamically enters and exits positions based on market conditions. Clusters of signals during high-volatility periods—such as 2020, 2021, and early 2025—indicate frequent entries and exits, whereas more stable phases show fewer trades.\n\n\nCode\n# Calculate ATR as a percentage of the closing price to normalize volatility\ntsla['atr_pct'] = tsla['ATR_10'] / tsla['Close_TSLA']\n\n# Define Vortex Indicator crossover signals:\n# - VI_Cross_Up: Identifies when VI+ crosses above VI− (potential bullish signal)\n# - VI_Cross_Down: Identifies when VI− crosses above VI+ (potential bearish signal)\ntsla['VI_Cross_Up'] = (tsla['VI+'] &gt; tsla['VI-']) & (tsla['VI+'].shift(1) &lt;= tsla['VI-'].shift(1))\ntsla['VI_Cross_Down'] = (tsla['VI-'] &gt; tsla['VI+']) & (tsla['VI-'].shift(1) &lt;= tsla['VI+'].shift(1))\n\n# Initialize signal and state columns\ntsla['Buy_Signal'] = False          # Flag for buy signal\ntsla['Sell_Signal'] = False         # Flag for sell signal\ntsla['Position'] = 0                # Position state: 1 = in position, 0 = no position\ntsla['Entry_Type'] = None           # Strategy classification: 'aggressive' or 'conservative'\n\n# Initialize control variables for trailing stop and price tracking\nin_position = False                 # Boolean flag for current position state\npeak_price = 0                      # Highest price observed during an open position\n\n# Iterate through the DataFrame to simulate trading logic based on Vortex signals and volatility\nfor i in range(1, len(tsla)):\n    row = tsla.iloc[i]\n    idx = tsla.index[i]\n\n    # Buy condition: Enter a new position if VI_Cross_Up occurs and no current position is held\n    if not in_position and row['VI_Cross_Up']:\n        tsla.at[idx, 'Buy_Signal'] = True\n        tsla.at[idx, 'Position'] = 1\n        in_position = True\n        peak_price = row['Close_TSLA']\n\n        # Classify entry type based on volatility threshold\n        if row['atr_pct'] &lt; 0.03:\n            tsla.at[idx, 'Entry_Type'] = 'aggressive'\n        else:\n            tsla.at[idx, 'Entry_Type'] = 'conservative'\n\n    # While in position, evaluate for trailing stop or VI_Cross_Down exit condition\n    elif in_position:\n        current_price = row['Close_TSLA']\n        peak_price = max(peak_price, current_price)\n        drawdown = (peak_price - current_price) / peak_price\n\n        # Sell condition: Exit if drawdown exceeds 3% or VI_Cross_Down occurs\n        if drawdown &gt;= 0.03 or row['VI_Cross_Down']:\n            tsla.at[idx, 'Sell_Signal'] = True\n            tsla.at[idx, 'Position'] = 0\n            in_position = False\n        else:\n            tsla.at[idx, 'Position'] = 1  # Maintain position\n\n# Output the total count of each type of signal and entry classification\nprint(\"Buy signals:\", tsla['Buy_Signal'].sum())\nprint(\"Sell signals:\", tsla['Sell_Signal'].sum())\nprint(\"Aggressive entries:\", (tsla['Entry_Type'] == 'aggressive').sum())\nprint(\"Conservative entries:\", (tsla['Entry_Type'] == 'conservative').sum())\n\n\nBuy signals: 80\nSell signals: 80\nAggressive entries: 5\nConservative entries: 75\n\n\n\n\nCode\n# Create an empty figure to hold all plot layers\nfig = go.Figure()\n\n# Plot the tsla closing price as a continuous blue line\nfig.add_trace(go.Scatter(\n    x=tsla.index,\n    y=tsla['Close_TSLA'],\n    mode='lines',\n    name='TSLA Price',\n    line=dict(color='blue')\n))\n\n# Add markers for aggressive buy signals (Entry_Type = 'aggressive')\nfig.add_trace(go.Scatter(\n    x=tsla[(tsla['Buy_Signal']) & (tsla['Entry_Type'] == 'aggressive')].index,\n    y=tsla[(tsla['Buy_Signal']) & (tsla['Entry_Type'] == 'aggressive')]['Close_TSLA'],\n    mode='markers',\n    name='Buy (Aggressive)',\n    marker=dict(symbol='triangle-up', color='limegreen', size=10)\n))\n\n# Add markers for conservative buy signals (Entry_Type = 'conservative')\nfig.add_trace(go.Scatter(\n    x=tsla[(tsla['Buy_Signal']) & (tsla['Entry_Type'] == 'conservative')].index,\n    y=tsla[(tsla['Buy_Signal']) & (tsla['Entry_Type'] == 'conservative')]['Close_TSLA'],\n    mode='markers',\n    name='Buy (Conservative)',\n    marker=dict(symbol='triangle-up', color='green', size=10)\n))\n\n# Add markers for sell signals using red downward-pointing triangles\nfig.add_trace(go.Scatter(\n    x=tsla[tsla['Sell_Signal']].index,\n    y=tsla[tsla['Sell_Signal']]['Close_TSLA'],\n    mode='markers',\n    name='Sell Signal',\n    marker=dict(symbol='triangle-down', color='red', size=10)\n))\n\n# Configure chart layout with appropriate title, axis labels, and style\nfig.update_layout(\n    title='TSLA Buy/Sell Signals Over Time',\n    xaxis_title='Date',\n    yaxis_title='Price (USD)',\n    template='plotly_white',\n    height=600\n)\n\n# Render the figure\nfig.show()\n\n\n\n\n\nThe chart displays the historical closing price of Tesla (TSLA) stock alongside algorithmically generated buy and sell signals. The blue line represents TSLA’s closing price, while the green upward-pointing triangles indicate buy entries—distinguished by lime green for aggressive entries (lower volatility) and dark green for conservative entries (higher volatility). Red downward-pointing triangles represent sell signals.\nThe buy signals are generally aligned with upward momentum, and sell signals frequently follow periods of short-term retracement or heightened volatility. The system shows particularly dense activity around highly volatile phases, such as mid-2020 to early 2022, capturing many entries and exits. In contrast, during more stable periods, the signals are more spaced out. Overall, the plot provides a clear visual assessment of how the strategy adapts dynamically to changing market conditions by modulating its entries based on volatility and exiting with protective trailing logic."
  },
  {
    "objectID": "projects/vortex/main.html#tesla-analysis-results",
    "href": "projects/vortex/main.html#tesla-analysis-results",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "Tesla Analysis Results",
    "text": "Tesla Analysis Results\n\n\nCode\ntsla_merged_data = pd.merge(\n    tsla_merged_data, \n    tsla[['Close_TSLA', 'High_TSLA', 'Low_TSLA', 'Open_TSLA', 'Volume_TSLA',\n          'VI+', 'VI-', 'prev_close', 'tr1', 'tr2', 'tr3', 'true_range', 'ATR_10', 'position_size']], \n    on='Date', \n    how='left')\ntsla_merged_data.head()\n\n\n\n\n\n\n\n\n\nDate\nVolume\ntime_published\nsentiment_score\nWeighted_Sentiment\n5_day_avg_sentiment\nBuy_Condition\n5_day_avg_sentiment_norm\nClose_TSLA\nHigh_TSLA\n...\nVolume_TSLA\nVI+\nVI-\nprev_close\ntr1\ntr2\ntr3\ntrue_range\nATR_10\nposition_size\n\n\n\n\n0\n2025-01-31\n83568200\n2025-01-31\n0.194614\n1.626354e+07\nNaN\nFalse\nNaN\n404.600006\n419.989990\n...\n83568200\n1.012243\n0.857439\n400.279999\n18.649994\n19.709991\n1.059998\n19.709991\n18.478998\n0.005\n\n\n1\n2025-02-03\n93732100\n2025-02-03\n0.129243\n1.211426e+07\nNaN\nFalse\nNaN\n383.679993\n389.170013\n...\n93732100\n0.941453\n0.927890\n404.600006\n14.810028\n15.429993\n30.240021\n30.240021\n18.911002\n0.005\n\n\n2\n2025-02-04\n57072200\n2025-02-04\n0.173107\n9.879602e+06\nNaN\nFalse\nNaN\n392.209991\n394.000000\n...\n57072200\n0.911693\n0.973944\n383.679993\n12.600006\n10.320007\n2.279999\n12.600006\n17.482001\n0.005\n\n\n3\n2025-02-05\n57223300\n2025-02-05\n0.136874\n7.832396e+06\nNaN\nFalse\nNaN\n378.170013\n388.390015\n...\n57223300\n0.862377\n1.041572\n392.209991\n12.860016\n3.819977\n16.679993\n16.679993\n17.809000\n0.005\n\n\n4\n2025-02-06\n77918200\n2025-02-06\n0.118095\n9.201782e+06\n1.105832e+07\nTrue\n0.132787\n374.320007\n375.399994\n...\n77918200\n0.805785\n1.075550\n378.170013\n12.220001\n2.770020\n14.990021\n14.990021\n18.130002\n0.005\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\nCode\n# Calculate ATR percentage\ntsla_merged_data['atr_pct'] = tsla_merged_data['ATR_10'] / tsla_merged_data['Close_TSLA']\n\n# Vortex crossover logic\ntsla_merged_data['VI_Cross_Up'] = (tsla_merged_data['VI+'] &gt; tsla_merged_data['VI-']) & (tsla_merged_data['VI+'].shift(1) &lt;= tsla_merged_data['VI-'].shift(1))\ntsla_merged_data['VI_Cross_Down'] = (tsla_merged_data['VI-'] &gt; tsla_merged_data['VI+']) & (tsla_merged_data['VI-'].shift(1) &lt;= tsla_merged_data['VI+'].shift(1))\n\n# Initialize signal & state columns\ntsla_merged_data['Buy_Signal'] = False\ntsla_merged_data['Sell_Signal'] = False\ntsla_merged_data['Position'] = 0\ntsla_merged_data['Entry_Type'] = None  # aggressive/conservative\n\n# Trailing stop logic variables\nin_position = False\npeak_price = 0\n\nfor i in range(1, len(tsla_merged_data)):\n    row = tsla_merged_data.iloc[i]\n    idx = tsla_merged_data.index[i]\n    # Buy condition\n    if not in_position or row['VI_Cross_Up'] or row['5_day_avg_sentiment_norm']&gt;0:\n        tsla_merged_data.at[idx, 'Buy_Signal'] = True\n        tsla_merged_data.at[idx, 'Position'] = 1\n        in_position = True\n        peak_price = row['Close_TSLA']\n\n        # Entry Type: aggressive if ATR &lt; 3%, else conservative\n        if row['atr_pct'] &lt; 0.03:\n            tsla_merged_data.at[idx, 'Entry_Type'] = 'aggressive'\n        else:\n            tsla_merged_data.at[idx, 'Entry_Type'] = 'conservative'\n\n    # While in position, check for trailing stop or VI cross down\n    elif in_position:\n        current_price = row['Close_TSLA']\n        peak_price = max(peak_price, current_price)\n        drawdown = (peak_price - current_price) / peak_price\n\n        if drawdown &gt;= 0.03 or row['VI_Cross_Down']:\n            tsla_merged_data.at[idx, 'Sell_Signal'] = True\n            tsla_merged_data.at[idx, 'Position'] = 0\n            in_position = False\n        else:\n            tsla_merged_data.at[idx, 'Position'] = 1\n\n# Show result counts\nprint(\"Buy signals:\", tsla_merged_data['Buy_Signal'].sum())\nprint(\"Sell signals:\", tsla_merged_data['Sell_Signal'].sum())\nprint(\"Aggressive entries:\", (tsla_merged_data['Entry_Type'] == 'aggressive').sum())\nprint(\"Conservative entries:\", (tsla_merged_data['Entry_Type'] == 'conservative').sum())\n\n\nBuy signals: 18\nSell signals: 1\nAggressive entries: 0\nConservative entries: 18\n\n\n\n\nCode\n# Ensure 'Date' is datetime and set as index if needed\ntsla_merged_data['Date'] = pd.to_datetime(tsla_merged_data['Date'])\n\nfig = go.Figure()\n\n# Plot 5-day Avg Sentiment\nfig.add_trace(go.Scatter(\n    x=tsla_merged_data['Date'],\n    y=tsla_merged_data['5_day_avg_sentiment_norm'],\n    mode='lines+markers',\n    name='5-Day Avg Sentiment',\n    line=dict(color='blue')\n))\n\n# Plot ATR %\nfig.add_trace(go.Scatter(\n    x=tsla_merged_data['Date'],\n    y=tsla_merged_data['atr_pct'],\n    mode='lines+markers',\n    name='ATR %',\n    yaxis='y2',\n    line=dict(color='orange')\n))\n\n# Optional: Highlight Buy Signal Dates (even though there are none now)\nfig.add_trace(go.Scatter(\n    x=tsla_merged_data.loc[tsla_merged_data['Buy_Signal'], 'Date'],\n    y=tsla_merged_data.loc[tsla_merged_data['Buy_Signal'], '5_day_avg_sentiment_norm'],\n    mode='markers',\n    marker=dict(color='green', size=10, symbol='star'),\n    name='Buy Signal'\n))\n\n# Add dual axis layout\nfig.update_layout(\n    title=\"5-Day Sentiment vs ATR % (with Buy Signals)\",\n    xaxis_title='Date',\n    yaxis=dict(title='5-Day Avg Sentiment'),\n    yaxis2=dict(title='ATR %', overlaying='y', side='right'),\n    legend=dict(x=0.01, y=0.99),\n    height=500\n)\n\nfig.show()\n\n\n\n\n\n\n\nCode\ndef backtest(df, ticker):\n    capital = 100000\n    in_position = False\n    entry_price = 0\n    position_value = 0\n    cash = capital\n    returns = []\n\n    for i in range(len(df)):\n        row = df.iloc[i]\n\n        if row['Buy_Signal'] and not in_position:\n            position_size = row['position_size']\n            position_value = cash * position_size\n            entry_price = row['Close_' + ticker]\n            shares_bought = position_value / entry_price\n            cash -= position_value\n            in_position = True\n        elif row['Sell_Signal'] and in_position:\n            exit_price = row['Close_' + ticker]\n            proceeds = shares_bought * exit_price\n            profit = proceeds - position_value\n            cash += proceeds\n            returns.append(profit)\n            in_position = False\n            position_value = 0\n            entry_price = 0\n\n    final_value = cash + (shares_bought * row['Close_' + ticker] if in_position else 0)\n    total_return = final_value - capital\n    result = f\"Final Capital: ${final_value:.2f} \\nTotal Return: ${total_return:.2f} \\nTotal Trades: {len(returns)}\\nAverage Profit per Trade: ${np.mean(returns):.2f}\"\n    return result\n\n\n\n\nCode\nprint(backtest(tsla_merged_data, 'TSLA')) #w/ sentiment data\n\n\nFinal Capital: $99898.47 \nTotal Return: $-101.53 \nTotal Trades: 1\nAverage Profit per Trade: $11.12\n\n\n\n\nCode\nprint(backtest(tsla, 'TSLA')) #w/o sentiment data\n\n\nFinal Capital: $100575.32 \nTotal Return: $575.32 \nTotal Trades: 80\nAverage Profit per Trade: $7.19\n\n\n\n\nCode\ndef f_portfolio(df, ticker):\n    df = df.dropna(subset=['Close_' + ticker])\n    entries = df['Buy_Signal'].astype(bool)\n    exits = df['Sell_Signal'].astype(bool)\n\n    price = df['Close_' + ticker]\n    portfolio = vbt.Portfolio.from_signals(\n        close=price,\n        entries=entries,\n        exits=exits,\n        init_cash=100_000,\n        fees=0.001\n    )\n    return portfolio\n\n\n\n\nCode\n# without centiment data\ntsla_portfolio = f_portfolio(tsla, 'TSLA')\n\nprint(tsla_portfolio.stats())\ntsla_portfolio.plot().show()\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n\n\nStart                         2019-01-02 00:00:00\nEnd                           2025-03-04 00:00:00\nPeriod                                       1551\nStart Value                              100000.0\nEnd Value                           162759.235978\nTotal Return [%]                        62.759236\nBenchmark Return [%]                  1215.813231\nMax Gross Exposure [%]                      100.0\nTotal Fees Paid                      24054.581607\nMax Drawdown [%]                        55.348959\nMax Drawdown Duration                       730.0\nTotal Trades                                   80\nTotal Closed Trades                            80\nTotal Open Trades                               0\nOpen Trade PnL                                0.0\nWin Rate [%]                                 32.5\nBest Trade [%]                          46.283397\nWorst Trade [%]                         -9.410141\nAvg Winning Trade [%]                   11.344578\nAvg Losing Trade [%]                    -3.847352\nAvg Winning Trade Duration               7.076923\nAvg Losing Trade Duration                2.537037\nProfit Factor                            1.194803\nExpectancy                              784.49045\ndtype: object\n\n\n\n\n\nThe backtest results show that while the strategy achieved a total return of approximately 62.76%, it significantly underperformed compared to a simple buy-and-hold strategy on TSLA, which yielded a 1215.81% return. The strategy executed 80 trades with a low win rate of 32.5%, indicating that most trades were unprofitable. Although it had a few strong winners, the average profit per trade was marginal, with a profit factor of 1.19. Additionally, the portfolio experienced a substantial maximum drawdown of 55.35% and a prolonged recovery period lasting two years, signaling high risk. Visuals further confirm that many trades resulted in small losses or gains, with only a few notable profitable exits. Overall, while the strategy demonstrates some profitability, its risk-return profile is weak and may require optimization in entry/exit logic, volatility filtering, or sentiment integration to compete with the benchmark performance."
  },
  {
    "objectID": "projects/vortex/main.html#xly-analysis-results",
    "href": "projects/vortex/main.html#xly-analysis-results",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "XLY Analysis Results",
    "text": "XLY Analysis Results\n\n\nCode\nxly = calculate_true_range(xly, 'XLY')\nxly[\"position_size\"] = xly.apply(position_size, axis=1)\n\n# ---- Preview ----\nprint(xly[[\"Close_XLY\", \"ATR_10\", \"atr_pct\", \"position_size\"]].tail(10))\n\n\n             Close_XLY    ATR_10   atr_pct  position_size\nDate                                                     \n2025-02-19  225.618988  2.870099  0.012721           0.01\n2025-02-20  223.674316  2.919964  0.013055           0.01\n2025-02-21  217.790527  3.453495  0.015857           0.01\n2025-02-24  216.972778  3.270997  0.015076           0.01\n2025-02-25  215.835892  3.511334  0.016269           0.01\n2025-02-26  214.948349  3.602083  0.016758           0.01\n2025-02-27  211.846878  3.751672  0.017709           0.01\n2025-02-28  215.367203  3.836439  0.017813           0.01\n2025-03-03  211.398117  4.429805  0.020955           0.01\n2025-03-04  207.668396  4.845659  0.023334           0.01\n\n\n\n\nCode\nfig = px.line(xly, x=xly.index, y=\"atr_pct\", title=\"ATR% Over Time\")\nfig.add_hline(y=0.03, line_dash=\"dot\", line_color=\"green\", annotation_text=\"Low Volatility Cutoff\")\nfig.show()\n\n\n\n\n\n\n\nCode\n# Filter only 2025 data\nxly_2025 = xly[xly.index.year == 2025]\n\n# Plot\nfig = px.line(xly_2025, x=xly_2025.index, y=\"atr_pct\", title=\"XLY ATR% Over Time (2025 Only)\")\nfig.add_hline(y=0.03, line_dash=\"dot\", line_color=\"green\", annotation_text=\"Low Volatility Cutoff\")\nfig.show()\n\n\n\n\n\n\n\nCode\nxly = signal_generation(xly, 'XLY')\n\n\n\n\nCode\nprint(backtest(xly, 'XLY'))\n\n\nFinal Capital: $100729.52 \nTotal Return: $729.52 \nTotal Trades: 76\nAverage Profit per Trade: $9.60\n\n\n\n\nCode\nxly_portfolio = f_portfolio(xly, 'XLY')\nprint(xly_portfolio.stats())\nxly_portfolio.plot().show()\n\n\nStart                         2019-01-02 00:00:00\nEnd                           2025-03-04 00:00:00\nPeriod                                       1551\nStart Value                              100000.0\nEnd Value                           170848.194798\nTotal Return [%]                        70.848195\nBenchmark Return [%]                   120.815504\nMax Gross Exposure [%]                      100.0\nTotal Fees Paid                      21558.870642\nMax Drawdown [%]                        33.668417\nMax Drawdown Duration                       793.0\nTotal Trades                                   76\nTotal Closed Trades                            76\nTotal Open Trades                               0\nOpen Trade PnL                                0.0\nWin Rate [%]                            44.736842\nBest Trade [%]                          37.025745\nWorst Trade [%]                        -13.070482\nAvg Winning Trade [%]                    4.635492\nAvg Losing Trade [%]                    -2.172936\nAvg Winning Trade Duration              22.558824\nAvg Losing Trade Duration                4.690476\nProfit Factor                            1.512842\nExpectancy                             932.213089\ndtype: object\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set"
  },
  {
    "objectID": "projects/vortex/main.html#spy-analysis-results",
    "href": "projects/vortex/main.html#spy-analysis-results",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "SPY Analysis Results",
    "text": "SPY Analysis Results\n\n\nCode\nspy = calculate_true_range(spy, 'SPY')\nspy[\"position_size\"] = spy.apply(position_size, axis=1)\n\n# ---- Preview ----\nprint(spy[[\"Close_SPY\", \"ATR_10\", \"atr_pct\", \"position_size\"]].tail(10))\n\n\n             Close_SPY    ATR_10   atr_pct  position_size\nDate                                                     \n2025-02-19  611.091675  4.794563  0.007846           0.01\n2025-02-20  608.549377  4.806522  0.007898           0.01\n2025-02-21  598.140686  5.513399  0.009218           0.01\n2025-02-24  595.418884  5.359863  0.009002           0.01\n2025-02-25  592.457764  5.718790  0.009653           0.01\n2025-02-26  592.756836  6.146507  0.010369           0.01\n2025-02-27  583.295288  6.801538  0.011661           0.01\n2025-02-28  592.397949  7.353875  0.012414           0.01\n2025-03-03  582.019165  8.901222  0.015294           0.01\n2025-03-04  575.129883  9.901217  0.017216           0.01\n\n\n\n\nCode\nfig = px.line(spy, x=spy.index, y=\"atr_pct\", title=\"SPY ATR% Over Time\")\nfig.add_hline(y=0.03, line_dash=\"dot\", line_color=\"green\", annotation_text=\"Low Volatility Cutoff\")\nfig.show()\n\n\n\n\n\n\n\nCode\n# Filter only 2025 data\nspy_2025 = spy[spy.index.year == 2025]\n\n# Plot\nfig = px.line(spy_2025, x=spy_2025.index, y=\"atr_pct\", title=\"SPY ATR% Over Time (2025 Only)\")\nfig.add_hline(y=0.03, line_dash=\"dot\", line_color=\"green\", annotation_text=\"Low Volatility Cutoff\")\nfig.show()\n\n\n\n\n\n\n\nCode\nspy = signal_generation(spy, 'SPY')\n\n\n\n\nCode\nprint(backtest(spy, 'SPY'))\n\n\nFinal Capital: $100515.03 \nTotal Return: $515.03 \nTotal Trades: 56\nAverage Profit per Trade: $9.20\n\n\n\n\nCode\nspy_portfolio = f_portfolio(spy, 'SPY')\nprint(spy_portfolio.stats())\nspy_portfolio.plot().show()\n\n\nStart                         2019-01-02 00:00:00\nEnd                           2025-03-04 00:00:00\nPeriod                                       1551\nStart Value                              100000.0\nEnd Value                           149876.046124\nTotal Return [%]                        49.876046\nBenchmark Return [%]                   153.411688\nMax Gross Exposure [%]                      100.0\nTotal Fees Paid                      14500.381668\nMax Drawdown [%]                        19.809446\nMax Drawdown Duration                       584.0\nTotal Trades                                   56\nTotal Closed Trades                            56\nTotal Open Trades                               0\nOpen Trade PnL                                0.0\nWin Rate [%]                            55.357143\nBest Trade [%]                           7.385099\nWorst Trade [%]                         -9.885438\nAvg Winning Trade [%]                    3.135409\nAvg Losing Trade [%]                    -2.130089\nAvg Winning Trade Duration              28.258065\nAvg Losing Trade Duration                    7.56\nProfit Factor                            1.712196\nExpectancy                             890.643681\ndtype: object\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set"
  },
  {
    "objectID": "projects/vortex/main.html#optimization-tsla",
    "href": "projects/vortex/main.html#optimization-tsla",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "Optimization (TSLA)",
    "text": "Optimization (TSLA)\n\n\nCode\n# Define a list of different smoothing periods to test for the Vortex Indicator\nperiods = [7, 14, 21, 30]\nresults = {}  # Dictionary to store performance metrics for each period\n\n# Loop through each smoothing period\nfor n in periods:\n    # === Compute Vortex Indicator for the given period ===\n    tsla[f'VI+{n}'], tsla[f'VI-{n}'] = calculate_vortex(tsla, 'TSLA', n)\n\n    # === Generate Buy/Sell signals based on crossover logic ===\n    # Buy when VI+ crosses above VI-\n    tsla[f'Buy_{n}'] = tsla[f'VI+{n}'] &gt; tsla[f'VI-{n}']\n    # Sell when VI- crosses above VI+\n    tsla[f'Sell_{n}'] = tsla[f'VI-{n}'] &gt; tsla[f'VI+{n}']\n\n    # === Convert boolean signals to actual entry/exit Series ===\n    entries = tsla[f'Buy_{n}']\n    exits = tsla[f'Sell_{n}']\n\n    # === Run a backtest using vectorbt Portfolio object ===\n    portfolio = vbt.Portfolio.from_signals(\n        close=tsla['Close_TSLA'],  # TSLA closing prices\n        entries=entries,\n        exits=exits,\n        size=1,  # Assume buying 1 share per trade\n        init_cash=10_000  # Initial capital for backtest\n    )\n\n    # === Store backtest performance metrics in results dict ===\n    stats = portfolio.stats()\n    results[n] = stats\n\n# Identify the period with the highest total return\nbest_period = max(results, key=lambda x: results[x]['Total Return [%]'])\nprint(f\"✅ Best Performing Period: {best_period} days\")\n\n# Rebuild portfolio using the best period to visualize it\nportfolio = vbt.Portfolio.from_signals(\n    close=tsla['Close_TSLA'],\n    entries=tsla[f'VI+{best_period}'] &gt; tsla[f'VI-{best_period}'],\n    exits=tsla[f'VI-{best_period}'] &gt; tsla[f'VI+{best_period}'],\n    size=1,\n    init_cash=10_000\n)\n\n# Plot the results of the best strategy\nportfolio.plot().show()\nprint(portfolio.stats())\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n\n\n✅ Best Performing Period: 7 days\n\n\n\n\n\nStart                         2019-01-02 00:00:00\nEnd                           2025-03-04 00:00:00\nPeriod                                       1551\nStart Value                               10000.0\nEnd Value                            10480.194603\nTotal Return [%]                         4.801946\nBenchmark Return [%]                  1215.813231\nMax Gross Exposure [%]                   4.554966\nTotal Fees Paid                               0.0\nMax Drawdown [%]                         0.793073\nMax Drawdown Duration                       351.0\nTotal Trades                                  113\nTotal Closed Trades                           113\nTotal Open Trades                               0\nOpen Trade PnL                                0.0\nWin Rate [%]                            44.247788\nBest Trade [%]                         128.434899\nWorst Trade [%]                        -15.721837\nAvg Winning Trade [%]                   14.052436\nAvg Losing Trade [%]                    -4.125181\nAvg Winning Trade Duration                  11.44\nAvg Losing Trade Duration                4.206349\nProfit Factor                            2.096188\nExpectancy                                4.24951\ndtype: object\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set"
  },
  {
    "objectID": "projects/vortex/main.html#peer-comparison-apple-analysis-results",
    "href": "projects/vortex/main.html#peer-comparison-apple-analysis-results",
    "title": "Vortex–Sentiment Adaptive Volatility (VSAV) Strategy",
    "section": "Peer Comparison: Apple Analysis Results",
    "text": "Peer Comparison: Apple Analysis Results\n\n\nCode\naapl = yf.download('AAPL', start='2019-01-01', end='2025-03-05')\naapl = multiindex_to_singleindex(aapl)\nget_news_sentiment('AAPL', '20250101T0130', '20250301T0130', 1000, 'PNM5EHRALIOT1CKJ')\naapl['VI+'], aapl['VI-'] = calculate_vortex(aapl, 'AAPL')\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nCode\naapl = calculate_true_range(aapl, 'AAPL')\naapl[\"position_size\"] = aapl.apply(position_size, axis=1)\n\n# ---- Preview ----\nprint(aapl[[\"Close_AAPL\", \"ATR_10\", \"atr_pct\", \"position_size\"]].tail(10))\n\n\n            Close_AAPL    ATR_10   atr_pct  position_size\nDate                                                     \n2025-02-19  244.869995  4.939392  0.020171           0.01\n2025-02-20  245.830002  4.735891  0.019265           0.01\n2025-02-21  245.550003  4.746260  0.019329           0.01\n2025-02-24  247.100006  4.517000  0.018280           0.01\n2025-02-25  247.039993  4.687000  0.018973           0.01\n2025-02-26  240.360001  4.719998  0.019637           0.01\n2025-02-27  237.300003  4.631998  0.019520           0.01\n2025-02-28  241.839996  5.143999  0.021270           0.01\n2025-03-03  238.029999  5.479999  0.023022           0.01\n2025-03-04  235.929993  5.685001  0.024096           0.01\n\n\n\n\nCode\naapl = signal_generation(aapl, 'AAPL')\n# Display the total number of buy and sell signals generated across the dataset\nprint(\"Buy signals:\", aapl['Buy_Signal'].sum())\nprint(\"Sell signals:\", aapl['Sell_Signal'].sum())\n\n\nBuy signals: 985\nSell signals: 552\n\n\n\n\nCode\n# Calculate ATR as a percentage of the closing price to normalize volatility\naapl['atr_pct'] = aapl['ATR_10'] / aapl['Close_AAPL']\n\n# Define Vortex Indicator crossover signals:\n# - VI_Cross_Up: Identifies when VI+ crosses above VI− (potential bullish signal)\n# - VI_Cross_Down: Identifies when VI− crosses above VI+ (potential bearish signal)\naapl['VI_Cross_Up'] = (aapl['VI+'] &gt; aapl['VI-']) & (aapl['VI+'].shift(1) &lt;= aapl['VI-'].shift(1))\naapl['VI_Cross_Down'] = (aapl['VI-'] &gt; aapl['VI+']) & (aapl['VI-'].shift(1) &lt;= aapl['VI+'].shift(1))\n\n# Initialize signal and state columns\naapl['Buy_Signal'] = False          # Flag for buy signal\naapl['Sell_Signal'] = False         # Flag for sell signal\naapl['Position'] = 0                # Position state: 1 = in position, 0 = no position\naapl['Entry_Type'] = None           # Strategy classification: 'aggressive' or 'conservative'\n\n# Initialize control variables for trailing stop and price tracking\nin_position = False                 # Boolean flag for current position state\npeak_price = 0                      # Highest price observed during an open position\n\n# Iterate through the DataFrame to simulate trading logic based on Vortex signals and volatility\nfor i in range(1, len(aapl)):\n    row = aapl.iloc[i]\n    idx = aapl.index[i]\n\n    # Buy condition: Enter a new position if VI_Cross_Up occurs and no current position is held\n    if not in_position and row['VI_Cross_Up']:\n        aapl.at[idx, 'Buy_Signal'] = True\n        aapl.at[idx, 'Position'] = 1\n        in_position = True\n        peak_price = row['Close_AAPL']\n\n        # Classify entry type based on volatility threshold\n        if row['atr_pct'] &lt; 0.03:\n            aapl.at[idx, 'Entry_Type'] = 'aggressive'\n        else:\n            aapl.at[idx, 'Entry_Type'] = 'conservative'\n\n    # While in position, evaluate for trailing stop or VI_Cross_Down exit condition\n    elif in_position:\n        current_price = row['Close_AAPL']\n        peak_price = max(peak_price, current_price)\n        drawdown = (peak_price - current_price) / peak_price\n\n        # Sell condition: Exit if drawdown exceeds 3% or VI_Cross_Down occurs\n        if drawdown &gt;= 0.03 or row['VI_Cross_Down']:\n            aapl.at[idx, 'Sell_Signal'] = True\n            aapl.at[idx, 'Position'] = 0\n            in_position = False\n        else:\n            aapl.at[idx, 'Position'] = 1  # Maintain position\n\n# Output the total count of each type of signal and entry classification\nprint(\"Buy signals:\", aapl['Buy_Signal'].sum())\nprint(\"Sell signals:\", aapl['Sell_Signal'].sum())\nprint(\"Aggressive entries:\", (aapl['Entry_Type'] == 'aggressive').sum())\nprint(\"Conservative entries:\", (aapl['Entry_Type'] == 'conservative').sum())\n\n\nBuy signals: 66\nSell signals: 66\nAggressive entries: 45\nConservative entries: 21\n\n\n\n\nCode\naapl_sentiment_df = json_reader('AAPL')\n\n\n\n\nCode\naapl_sentiment_scores_filtered = aapl_sentiment_df[(aapl_sentiment_df['time_published']).isin(aapl.index)]\naapl_sentiment_scores_filtered = aapl_sentiment_scores_filtered.groupby('time_published')['sentiment_score'].mean().reset_index()\n\n\n\n\nCode\naapl_merged_data = pd.merge(\n    aapl['Volume_AAPL'].reset_index().rename(columns={'Volume_AAPL': 'Volume'}),\n    aapl_sentiment_scores_filtered,\n    left_on='Date',\n    right_on='time_published',\n    how='inner'\n)\n# Compute the weighted sentiment by multiplying raw sentiment by trading volume\naapl_merged_data['Weighted_Sentiment'] = aapl_merged_data['Volume'] * aapl_merged_data['sentiment_score']\n\n# Calculate a 5-day rolling average of the weighted sentiment to smooth short-term noise\naapl_merged_data['5_day_avg_sentiment'] = aapl_merged_data['Weighted_Sentiment'].rolling(window=5).mean()\n\n# Define a binary condition for when the average sentiment is positive\naapl_merged_data['Buy_Condition'] = aapl_merged_data['5_day_avg_sentiment'] &gt; 0\n\n# Normalize the rolling sentiment score by average volume to allow comparability across scales\naapl_merged_data['5_day_avg_sentiment_norm'] = (\n    aapl_merged_data['5_day_avg_sentiment'] / aapl_merged_data['Volume'].mean()\n)\n\n\n\n\nCode\naapl = calculate_true_range(aapl, 'AAPL')\naapl[\"position_size\"] = aapl.apply(position_size, axis=1)\n\n# ---- Preview ----\nprint(aapl[[\"Close_AAPL\", \"ATR_10\", \"atr_pct\", \"position_size\"]].tail(10))\n\n\n            Close_AAPL    ATR_10   atr_pct  position_size\nDate                                                     \n2025-02-19  244.869995  4.939392  0.020171           0.01\n2025-02-20  245.830002  4.735891  0.019265           0.01\n2025-02-21  245.550003  4.746260  0.019329           0.01\n2025-02-24  247.100006  4.517000  0.018280           0.01\n2025-02-25  247.039993  4.687000  0.018973           0.01\n2025-02-26  240.360001  4.719998  0.019637           0.01\n2025-02-27  237.300003  4.631998  0.019520           0.01\n2025-02-28  241.839996  5.143999  0.021270           0.01\n2025-03-03  238.029999  5.479999  0.023022           0.01\n2025-03-04  235.929993  5.685001  0.024096           0.01\n\n\n\n\nCode\naapl_merged_data = pd.merge(\n    aapl_merged_data, \n    aapl[['Close_AAPL', 'High_AAPL', 'Low_AAPL', 'Open_AAPL', 'Volume_AAPL',\n          'VI+', 'VI-', 'prev_close', 'tr1', 'tr2', 'tr3', 'true_range', 'ATR_10', 'position_size']], \n    on='Date', \n    how='left')\naapl_merged_data.head()\n\n\n\n\n\n\n\n\n\nDate\nVolume\ntime_published\nsentiment_score\nWeighted_Sentiment\n5_day_avg_sentiment\nBuy_Condition\n5_day_avg_sentiment_norm\nClose_AAPL\nHigh_AAPL\n...\nVolume_AAPL\nVI+\nVI-\nprev_close\ntr1\ntr2\ntr3\ntrue_range\nATR_10\nposition_size\n\n\n\n\n0\n2025-01-15\n39832000\n2025-01-15\n0.223177\n8.889575e+06\nNaN\nFalse\nNaN\n237.608749\n238.697564\n...\n39832000\n0.595080\n1.102317\n233.023788\n4.525039\n5.673775\n1.148737\n5.673775\n5.283191\n0.01\n\n\n1\n2025-01-16\n71759100\n2025-01-16\n0.237567\n1.704756e+07\nNaN\nFalse\nNaN\n228.009308\n237.748600\n...\n71759100\n0.524560\n1.139218\n237.608749\n9.969035\n0.139851\n9.829185\n9.969035\n5.895516\n0.01\n\n\n2\n2025-01-17\n68488300\n2025-01-17\n0.130304\n8.924326e+06\nNaN\nFalse\nNaN\n229.727417\n232.034878\n...\n68488300\n0.506950\n1.231532\n228.009308\n3.805813\n4.025570\n0.219757\n4.025570\n5.439019\n0.01\n\n\n3\n2025-01-21\n98070400\n2025-01-21\n0.169273\n1.660064e+07\nNaN\nFalse\nNaN\n222.395477\n224.173521\n...\n98070400\n0.514695\n1.233423\n229.727417\n5.034458\n5.553896\n10.588354\n10.588354\n6.269107\n0.01\n\n\n4\n2025-01-22\n64126500\n2025-01-22\n0.182421\n1.169803e+07\n1.263203e+07\nTrue\n0.231401\n223.584167\n223.873842\n...\n64126500\n0.570450\n1.200538\n222.395477\n4.325246\n1.478365\n2.846881\n4.325246\n6.289084\n0.01\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\nCode\n# Calculate ATR percentage\naapl_merged_data['atr_pct'] = aapl_merged_data['ATR_10'] / aapl_merged_data['Close_AAPL']\n\n# Vortex crossover logic\naapl_merged_data['VI_Cross_Up'] = (aapl_merged_data['VI+'] &gt; aapl_merged_data['VI-']) & (aapl_merged_data['VI+'].shift(1) &lt;= aapl_merged_data['VI-'].shift(1))\naapl_merged_data['VI_Cross_Down'] = (aapl_merged_data['VI-'] &gt; aapl_merged_data['VI+']) & (aapl_merged_data['VI-'].shift(1) &lt;= aapl_merged_data['VI+'].shift(1))\n\n# Initialize signal & state columns\naapl_merged_data['Buy_Signal'] = False\naapl_merged_data['Sell_Signal'] = False\naapl_merged_data['Position'] = 0\naapl_merged_data['Entry_Type'] = None  # aggressive/conservative\n\n# Trailing stop logic variables\nin_position = False\npeak_price = 0\n\nfor i in range(1, len(aapl_merged_data)):\n    row = aapl_merged_data.iloc[i]\n    idx = aapl_merged_data.index[i]\n    # Buy condition\n    if not in_position or row['VI_Cross_Up'] or row['5_day_avg_sentiment_norm']&gt;0:\n        aapl_merged_data.at[idx, 'Buy_Signal'] = True\n        aapl_merged_data.at[idx, 'Position'] = 1\n        in_position = True\n        peak_price = row['Close_AAPL']\n\n        # Entry Type: aggressive if ATR &lt; 3%, else conservative\n        if row['atr_pct'] &lt; 0.03:\n            aapl_merged_data.at[idx, 'Entry_Type'] = 'aggressive'\n        else:\n            aapl_merged_data.at[idx, 'Entry_Type'] = 'conservative'\n\n    # While in position, check for trailing stop or VI cross down\n    elif in_position:\n        current_price = row['Close_AAPL']\n        peak_price = max(peak_price, current_price)\n        drawdown = (peak_price - current_price) / peak_price\n\n        if drawdown &gt;= 0.03 or row['VI_Cross_Down']:\n            aapl_merged_data.at[idx, 'Sell_Signal'] = True\n            aapl_merged_data.at[idx, 'Position'] = 0\n            in_position = False\n        else:\n            aapl_merged_data.at[idx, 'Position'] = 1\n\n# Show result counts\nprint(\"Buy signals:\", aapl_merged_data['Buy_Signal'].sum())\nprint(\"Sell signals:\", aapl_merged_data['Sell_Signal'].sum())\nprint(\"Aggressive entries:\", (aapl_merged_data['Entry_Type'] == 'aggressive').sum())\nprint(\"Conservative entries:\", (aapl_merged_data['Entry_Type'] == 'conservative').sum())\n\n\nBuy signals: 28\nSell signals: 1\nAggressive entries: 23\nConservative entries: 5\n\n\n\n\nCode\n# Ensure 'Date' is datetime and set as index if needed\naapl_merged_data['Date'] = pd.to_datetime(aapl_merged_data['Date'])\n\nfig = go.Figure()\n\n# Plot 5-day Avg Sentiment\nfig.add_trace(go.Scatter(\n    x=aapl_merged_data['Date'],\n    y=aapl_merged_data['5_day_avg_sentiment_norm'],\n    mode='lines+markers',\n    name='5-Day Avg Sentiment',\n    line=dict(color='blue')\n))\n\n# Plot ATR %\nfig.add_trace(go.Scatter(\n    x=aapl_merged_data['Date'],\n    y=aapl_merged_data['atr_pct'],\n    mode='lines+markers',\n    name='ATR %',\n    yaxis='y2',\n    line=dict(color='orange')\n))\n\n# Optional: Highlight Buy Signal Dates (even though there are none now)\nfig.add_trace(go.Scatter(\n    x=aapl_merged_data.loc[aapl_merged_data['Buy_Signal'], 'Date'],\n    y=aapl_merged_data.loc[aapl_merged_data['Buy_Signal'], '5_day_avg_sentiment_norm'],\n    mode='markers',\n    marker=dict(color='green', size=10, symbol='star'),\n    name='Buy Signal'\n))\n\n# Add dual axis layout\nfig.update_layout(\n    title=\"5-Day Sentiment vs ATR % (with Buy Signals)\",\n    xaxis_title='Date',\n    yaxis=dict(title='5-Day Avg Sentiment'),\n    yaxis2=dict(title='ATR %', overlaying='y', side='right'),\n    legend=dict(x=0.01, y=0.99),\n    height=500\n)\n\nfig.show()\n\n\n\n\n\n\n\nCode\nprint(backtest(aapl_merged_data, 'AAPL')) #w/ sentiment data\n\n\nFinal Capital: $100057.01 \nTotal Return: $57.01 \nTotal Trades: 1\nAverage Profit per Trade: $-24.62\n\n\n\n\nCode\nprint(backtest(aapl, 'AAPL')) #w/o sentiment data\n\n\nFinal Capital: $101198.29 \nTotal Return: $1198.29 \nTotal Trades: 66\nAverage Profit per Trade: $18.16\n\n\n\n\nCode\n# without centiment data\naapl_portfolio = f_portfolio(aapl, 'AAPL')\n\nprint(aapl_portfolio.stats())\naapl_portfolio.plot().show()\n\n\nStart                         2019-01-02 00:00:00\nEnd                           2025-03-04 00:00:00\nPeriod                                       1551\nStart Value                              100000.0\nEnd Value                           255472.954051\nTotal Return [%]                       155.472954\nBenchmark Return [%]                   526.354355\nMax Gross Exposure [%]                      100.0\nTotal Fees Paid                        23330.2112\nMax Drawdown [%]                        14.949616\nMax Drawdown Duration                       238.0\nTotal Trades                                   66\nTotal Closed Trades                            66\nTotal Open Trades                               0\nOpen Trade PnL                                0.0\nWin Rate [%]                            48.484848\nBest Trade [%]                          18.319731\nWorst Trade [%]                         -5.776766\nAvg Winning Trade [%]                     5.44886\nAvg Losing Trade [%]                    -2.071776\nAvg Winning Trade Duration                16.5625\nAvg Losing Trade Duration                4.176471\nProfit Factor                            2.213935\nExpectancy                            2355.650819\ndtype: object\n\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sharpe_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'calmar_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'omega_ratio' requires frequency to be set\n\n/Users/dinarazhorabek/Library/Python/3.9/lib/python/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning:\n\nMetric 'sortino_ratio' requires frequency to be set\n\n\n\n\n\n\nBased on the results from applying the trading strategy to the Apple (AAPL) ticker, we can reasonably conclude that the strategy does work on peers like AAPL. The strategy delivered a total return of approximately 282% over the backtest period (2019–2025), compared to a benchmark return of about 526%, which indicates it captured a significant portion of the upward trend while actively managing trades. Although it underperformed the benchmark in absolute terms, this is typical of signal-driven strategies that trade in and out of the market. The profit factor of 2.11, expectancy of 4204, and a win rate of 45.5% suggest the strategy was profitable overall. Additionally, the drawdown was moderate (20.87%), reflecting a reasonable risk exposure relative to the potential reward.\nThe cumulative returns graph further supports this interpretation. The strategy closely follows the broader market trend, generating consistent gains and outperforming during certain periods. The trade PnL distribution shows a good number of winning trades with healthy profitability, and although there were losses, the downside was generally contained. Therefore, this peer comparison confirms that the strategy generalizes reasonably well beyond TSLA, making it a potentially viable approach for other high-liquidity technology stocks like AAPL."
  },
  {
    "objectID": "projects/video_games/classification.html",
    "href": "projects/video_games/classification.html",
    "title": "Classification Tree",
    "section": "",
    "text": "Developed a classification model to predict video game sales performance using a real-world video game dataset. Preprocessed the data through binning, factor conversion, and top-category filtering. Built, visualized, and pruned decision trees using cross-validation, and evaluated model performance using confusion matrices.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n\nvg &lt;- read.csv('vgchartz-2024.csv')\nhead(vg)\n\n                                                                   img\n1                        /games/boxart/full_6510540AmericaFrontccc.jpg\n2                        /games/boxart/full_5563178AmericaFrontccc.jpg\n3                                          /games/boxart/827563ccc.jpg\n4                        /games/boxart/full_9218923AmericaFrontccc.jpg\n5                        /games/boxart/full_4990510AmericaFrontccc.jpg\n6 /games/boxart/full_call-of-duty-modern-warfare-3_517AmericaFront.jpg\n                           title console   genre      publisher      developer\n1             Grand Theft Auto V     PS3  Action Rockstar Games Rockstar North\n2             Grand Theft Auto V     PS4  Action Rockstar Games Rockstar North\n3    Grand Theft Auto: Vice City     PS2  Action Rockstar Games Rockstar North\n4             Grand Theft Auto V    X360  Action Rockstar Games Rockstar North\n5      Call of Duty: Black Ops 3     PS4 Shooter     Activision       Treyarch\n6 Call of Duty: Modern Warfare 3    X360 Shooter     Activision  Infinity Ward\n  critic_score total_sales na_sales jp_sales pal_sales other_sales release_date\n1          9.4       20.32     6.37     0.99      9.85        3.12   2013-09-17\n2          9.7       19.39     6.06     0.60      9.71        3.02   2014-11-18\n3          9.6       16.15     8.41     0.47      5.49        1.78   2002-10-28\n4           NA       15.86     9.06     0.06      5.33        1.42   2013-09-17\n5          8.1       15.09     6.18     0.41      6.05        2.44   2015-11-06\n6          8.7       14.82     9.07     0.13      4.29        1.33   2011-11-08\n  last_update\n1            \n2  2018-01-03\n3            \n4            \n5  2018-01-14\n6            \n\n\nThis dataset appears to contain information about video games, including the title, the console it was developed for, genre, publisher, developer, game rating, various sales numbers, release date, and, if available, the last update date. While some titles appear multiple times, other variables indicate that these entries correspond to the same game released on different consoles or different versions, such as remakes. The critic_score represents the overall rating of a game, with a maximum score of 10. And the dataset includes sales data across different regions (NA, JP, PAL, Other), and difference between numbers suggest the game popularity across various geographical areas. Furthermore, there is huge number of NAs.\n\nvg$total_sales &lt;- cut(vg$total_sales, \n                      breaks = quantile(vg$total_sales, probs = seq(0, 1, length.out = 4), na.rm = TRUE), \n                      include.lowest = TRUE,\n                      labels = c(\"Low\", \"Medium\", \"High\"))\ntable(vg$total_sales)\n\n\n   Low Medium   High \n  6339   6386   6197 \n\n\nAfter using equal frequency binning method to our output variable, we can see that the data equally was distributed within three groups: Low, Medium, High.\n\nstr(vg)\n\n'data.frame':   64016 obs. of  14 variables:\n $ img         : chr  \"/games/boxart/full_6510540AmericaFrontccc.jpg\" \"/games/boxart/full_5563178AmericaFrontccc.jpg\" \"/games/boxart/827563ccc.jpg\" \"/games/boxart/full_9218923AmericaFrontccc.jpg\" ...\n $ title       : chr  \"Grand Theft Auto V\" \"Grand Theft Auto V\" \"Grand Theft Auto: Vice City\" \"Grand Theft Auto V\" ...\n $ console     : chr  \"PS3\" \"PS4\" \"PS2\" \"X360\" ...\n $ genre       : chr  \"Action\" \"Action\" \"Action\" \"Action\" ...\n $ publisher   : chr  \"Rockstar Games\" \"Rockstar Games\" \"Rockstar Games\" \"Rockstar Games\" ...\n $ developer   : chr  \"Rockstar North\" \"Rockstar North\" \"Rockstar North\" \"Rockstar North\" ...\n $ critic_score: num  9.4 9.7 9.6 NA 8.1 8.7 8.8 9.8 8.4 8 ...\n $ total_sales : Factor w/ 3 levels \"Low\",\"Medium\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ na_sales    : num  6.37 6.06 8.41 9.06 6.18 9.07 9.76 5.26 8.27 4.99 ...\n $ jp_sales    : num  0.99 0.6 0.47 0.06 0.41 0.13 0.11 0.21 0.07 0.65 ...\n $ pal_sales   : num  9.85 9.71 5.49 5.33 6.05 4.29 3.73 6.21 4.32 5.88 ...\n $ other_sales : num  3.12 3.02 1.78 1.42 2.44 1.33 1.14 2.26 1.2 2.28 ...\n $ release_date: chr  \"2013-09-17\" \"2014-11-18\" \"2002-10-28\" \"2013-09-17\" ...\n $ last_update : chr  \"\" \"2018-01-03\" \"\" \"\" ...\n\n\n\nvg$img &lt;- as.factor(vg$img)\nvg$title &lt;- as.factor(vg$title)\nvg$console &lt;- as.factor(vg$console)\nvg$genre &lt;- as.factor(vg$genre)\nvg$publisher &lt;- as.factor(vg$publisher)\nvg$developer &lt;- as.factor(vg$developer)\nvg$release_date &lt;- as.factor(vg$release_date)\nvg$last_update &lt;- as.factor(vg$last_update)\n\nstr(vg)\n\n'data.frame':   64016 obs. of  14 variables:\n $ img         : Factor w/ 56177 levels \"/games/boxart/1000386ccc.jpg\",..: 31526 27433 6702 42887 25079 47407 47389 23589 12344 23571 ...\n $ title       : Factor w/ 39798 levels \"_summer ##\",\"- Arcane preRaise -\",..: 13767 13767 13779 13767 5281 5291 5276 27100 5283 5283 ...\n $ console     : Factor w/ 81 levels \"2600\",\"3DO\",\"3DS\",..: 57 58 56 76 58 76 76 58 76 57 ...\n $ genre       : Factor w/ 20 levels \"Action\",\"Action-Adventure\",..: 1 1 1 1 16 16 16 2 16 16 ...\n $ publisher   : Factor w/ 3383 levels \"][ Games\",\"@unepic_fran\",..: 2514 2514 2514 2514 84 84 84 2514 84 84 ...\n $ developer   : Factor w/ 8863 levels \"\",\".theprodukkt\",..: 6580 6580 6580 6580 8072 3787 8072 6576 8072 8072 ...\n $ critic_score: num  9.4 9.7 9.6 NA 8.1 8.7 8.8 9.8 8.4 8 ...\n $ total_sales : Factor w/ 3 levels \"Low\",\"Medium\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ na_sales    : num  6.37 6.06 8.41 9.06 6.18 9.07 9.76 5.26 8.27 4.99 ...\n $ jp_sales    : num  0.99 0.6 0.47 0.06 0.41 0.13 0.11 0.21 0.07 0.65 ...\n $ pal_sales   : num  9.85 9.71 5.49 5.33 6.05 4.29 3.73 6.21 4.32 5.88 ...\n $ other_sales : num  3.12 3.02 1.78 1.42 2.44 1.33 1.14 2.26 1.2 2.28 ...\n $ release_date: Factor w/ 7923 levels \"\",\"1971-12-03\",..: 6007 6357 2845 6007 6573 5506 5197 7244 5769 5769 ...\n $ last_update : Factor w/ 1546 levels \"\",\"2017-11-28\",..: 1 15 1 1 26 1 1 285 109 109 ...\n\n\nAfter converting all chr parameters to factor, our dataset contains only numbers or factors.\nNext we are going to do some “topN” filtering.\n\ntop_7_Console &lt;- vg %&gt;% \n  count(console, sort = TRUE) %&gt;% \n  slice(1:7)\ntop_7_Console\n\n  console     n\n1      PC 12617\n2     PS2  3565\n3      DS  3288\n4     PS4  2878\n5      PS  2707\n6      NS  2337\n7     XBL  2120\n\n\n7 popular console types: PC, PS2, DS, PS4, PS, NS, XBL. Also there is huge difference between PC and other remaining values, making PC much more popular. This is likely due to its open system, allowing users to install and download games freely without strict platform restrictions.\n\ntop_7_Genre &lt;- vg %&gt;% \n  count(genre, sort = TRUE) %&gt;% \n  slice(1:7)\ntop_7_Genre\n\n         genre    n\n1         Misc 9304\n2       Action 8557\n3    Adventure 6260\n4 Role-Playing 5721\n5       Sports 5586\n6      Shooter 5410\n7     Platform 4001\n\n\n7 most common genres: misc, action, adventure, role-playing, sports, shooter, platform. The most prevalent genre is Miscellaneous, likely due to games that don’t fit into traditional categories or are poorly documented. Following that, Action games are the second most popular, with Adventure games ranking third. The popularity of platform games (Mario, Zelda or Sonic) shows that traditional gameplay styles still have a strong place in market.\n\ntop_7_Publisher &lt;- vg %&gt;% \n  count(publisher, sort = TRUE) %&gt;% \n  slice(1:7) \ntop_7_Publisher\n\n        publisher    n\n1         Unknown 8842\n2            Sega 2207\n3         Ubisoft 1663\n4 Electronic Arts 1619\n5      Activision 1582\n6          Konami 1544\n7        Nintendo 1476\n\n\n7 most common publishers: unknown, sega, ubisoft, electronic arts, activision, konami, nintendo. Notably, 8,842 records lack publisher information, categorized under “Unknown.” Among named publishers, Sega remains a key player. Also, Ubisoft, EA, Activision have released most of the popular games over the last few years.\n\ntop_7_Developer &lt;- vg %&gt;% \n  count(developer, sort = TRUE) %&gt;% \n  slice(1:7)\ntop_7_Developer\n\n        developer    n\n1         Unknown 4435\n2          Konami  976\n3            Sega  915\n4          Capcom  870\n5           Namco  489\n6     Square Enix  425\n7 SNK Corporation  408\n\n\n7 most common developers: unknown, konami, sega, namco, square enix, capcom, snk corporation. Similarly, 4,435 records lack developer information. Several major developers, such as Sega and Konami, also appear among the top publishers, reinforcing their influence in the industry.\n\nvg &lt;- vg %&gt;% filter(console %in% top_7_Console$console & \n                      genre %in% top_7_Genre$genre &\n                      publisher %in% top_7_Publisher$publisher &\n                      developer %in% top_7_Developer$developer)\n\nnrow(vg)\n\n[1] 939\n\n\nAfter applying these filters, the dataset is reduced to 939 rows.\n\nvg &lt;- droplevels(vg)\nnrow(vg)\n\n[1] 939\n\n\nThe dataset contains 939 records.\n\nset.seed(79)\nvg.index &lt;- sample(c(1:nrow(vg)), nrow(vg)*0.6)\nvg_train &lt;- vg[vg.index, ]\nvg_valid &lt;- vg[-vg.index, ]\n\nNext we will use rpart.plot to display a classification tree that depicts your model.\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# names(vg)\nmodel &lt;- rpart(total_sales ~ console + genre + publisher + developer + critic_score, method = 'class', data = vg_train)\n\nrpart.plot(model)\n\n\n\n\n\n\n\n\n\nrpart.plot(model, extra=2, fallen.leaves = FALSE)\n\n\n\n\n\n\n\n\n\nrpart.plot(model, type=5, extra = 2, fallen.leaves=FALSE)\n\n\n\n\n\n\n\n\nThe initial plot starts with default parameters, where the tree starts from console varaible at the root. Each node represents type of total_sales (Low, Medium, High) differentiated by color. Within each node, we see class probabilities and the percentage of observations classified at that node. After adding extra=2, in the new plot the class rate is relative to the total number of observations in each class. For instance, in the second left node, 50 observations belong to Low sales category out of 102 total observations. And setting fallen.leaves = FALSE, moves the leaf nodes away from the bottom, changing the structure of trees. In the third plot used type=5, which adds variable names for each split line, and class labels appear only at the leaves. This layout more intuitive, as it avoids overwhelming details such as numbers and labels.\nThe root node is console variable, and rule is Console = DS or PC. The root node is starting point of model, and have highest impact on outcome variable. In our model, the type of console is primary factor to decide total sales number. So, this approach can help strategize future plans for the game publisher, focusing more on high-performance consoles.\nWe see that from 5 input variables (console, genre, publisher, developer, critic score), only 2 (console, genre) appears in the tree model, as a useful parameters to predict price. Variable subset selection is automatic since it is part of the split selection.\n\nrpart.rules(model)\n\n total_sales   Low Medi High                                                                                                    \n         Low [ .56  .27  .16] when console is DS or NS or PC or PS or PS2 & genre is Action or Adventure or Misc or Role-Playing\n      Medium [ .19  .57  .23] when console is DS or NS or PC or PS or PS2 & genre is                                      Sports\n        High [ .11  .22  .67] when console is DS or NS or PC or PS or PS2 & genre is                         Platform or Shooter\n        High [ .00  .00 1.00] when console is                         PS4                                                       \n\n\nFrom the rules of our model, let’s describe second rule (index = 10): The video game is for PS console and Sports genre, by following tree nodes this game will be classified into Medium sales group.\n\ncomplex_model &lt;- rpart(total_sales ~ console + genre + publisher + developer + critic_score, method=\"class\", cp=0, minsplit=2, data = vg_train)\n\nrpart.plot(complex_model, extra=1, fallen.leaves=FALSE)\n\n\n\n\n\n\n\n\n\nfive_fold_cv &lt;- rpart(total_sales ~ console + genre + publisher + developer + critic_score, method=\"class\",\n                      cp=0.00001, minsplit=5, xval=5, data=vg_train)\n\na &lt;- printcp(five_fold_cv)\n\n\nClassification tree:\nrpart(formula = total_sales ~ console + genre + publisher + developer + \n    critic_score, data = vg_train, method = \"class\", cp = 1e-05, \n    minsplit = 5, xval = 5)\n\nVariables actually used in tree construction:\n[1] console      critic_score genre       \n\nRoot node error: 81/127 = 0.6378\n\nn=127 (436 observations deleted due to missingness)\n\n         CP nsplit rel error  xerror     xstd\n1 0.1666667      0   1.00000 1.08642 0.064178\n2 0.0925926      2   0.66667 0.85185 0.069303\n3 0.0493827      4   0.48148 0.85185 0.069303\n4 0.0370370      5   0.43210 0.77778 0.069562\n5 0.0246914      6   0.39506 0.76543 0.069545\n6 0.0123457      7   0.37037 0.77778 0.069562\n7 0.0061728     10   0.33333 0.77778 0.069562\n8 0.0000100     12   0.32099 0.80247 0.069545\n\na &lt;- data.frame(a)\na\n\n          CP nsplit rel.error    xerror       xstd\n1 0.16666667      0 1.0000000 1.0864198 0.06417809\n2 0.09259259      2 0.6666667 0.8518519 0.06930294\n3 0.04938272      4 0.4814815 0.8518519 0.06930294\n4 0.03703704      5 0.4320988 0.7777778 0.06956221\n5 0.02469136      6 0.3950617 0.7654321 0.06954496\n6 0.01234568      7 0.3703704 0.7777778 0.06956221\n7 0.00617284     10 0.3333333 0.7777778 0.06956221\n8 0.00001000     12 0.3209877 0.8024691 0.06954496\n\n\nFrom the complexity parameter table for eight trees, we see that xerror value decreased at some point and started to increase again. This minimum value of cross validation error (0.7037037) gives optimal cp value. In our case cp = 0.03703704\n\npruned.ct &lt;- prune(five_fold_cv,\n                   cp=five_fold_cv$cptable[which.min(five_fold_cv$cptable[,\"xerror\"]),\"CP\"])\n\nrpart.plot(pruned.ct,type=5, extra = 2, fallen.leaves=FALSE)\n\n\n\n\n\n\n\n\n\n# Huge tree results\ncomplex_model.pred &lt;- predict(complex_model, vg_train, type=\"class\")\nconfusionMatrix(complex_model.pred, vg_train$total_sales)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low     39     10    4\n    Medium   5     33    9\n    High     1      3   23\n\nOverall Statistics\n                                          \n               Accuracy : 0.748           \n                 95% CI : (0.6633, 0.8208)\n    No Information Rate : 0.3622          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.617           \n                                          \n Mcnemar's Test P-Value : 0.09099         \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity              0.8667        0.7174      0.6389\nSpecificity              0.8293        0.8272      0.9560\nPos Pred Value           0.7358        0.7021      0.8519\nNeg Pred Value           0.9189        0.8375      0.8700\nPrevalence               0.3543        0.3622      0.2835\nDetection Rate           0.3071        0.2598      0.1811\nDetection Prevalence     0.4173        0.3701      0.2126\nBalanced Accuracy        0.8480        0.7723      0.7975\n\ncomplex_model.pred2 &lt;- predict(complex_model, vg_valid, type=\"class\")\nconfusionMatrix(complex_model.pred2, vg_valid$total_sales)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low     18     15    3\n    Medium   4      8    5\n    High     5      2    7\n\nOverall Statistics\n                                          \n               Accuracy : 0.4925          \n                 95% CI : (0.3682, 0.6176)\n    No Information Rate : 0.403           \n    P-Value [Acc &gt; NIR] : 0.08620         \n                                          \n                  Kappa : 0.2096          \n                                          \n Mcnemar's Test P-Value : 0.04293         \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity              0.6667        0.3200      0.4667\nSpecificity              0.5500        0.7857      0.8654\nPos Pred Value           0.5000        0.4706      0.5000\nNeg Pred Value           0.7097        0.6600      0.8491\nPrevalence               0.4030        0.3731      0.2239\nDetection Rate           0.2687        0.1194      0.1045\nDetection Prevalence     0.5373        0.2537      0.2090\nBalanced Accuracy        0.6083        0.5529      0.6660\n\n\nThe fully grown tree model have a high accuracy for training data (74.8%), but its performance drops significantly on the validation data (49.25%). This suggests that the model is overfitting, meaning it has learned the noise and specific patterns of the training set that do not generalize well to new, unseen data.\n\n# Pruned tree results\npruned.ct.pred &lt;- predict(pruned.ct, vg_train, type=\"class\")\nconfusionMatrix(pruned.ct.pred, vg_train$total_sales)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low     32     10    4\n    Medium  12     34   17\n    High     1      2   15\n\nOverall Statistics\n                                          \n               Accuracy : 0.6378          \n                 95% CI : (0.5478, 0.7212)\n    No Information Rate : 0.3622          \n    P-Value [Acc &gt; NIR] : 2.688e-10       \n                                          \n                  Kappa : 0.4443          \n                                          \n Mcnemar's Test P-Value : 0.003155        \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity              0.7111        0.7391      0.4167\nSpecificity              0.8293        0.6420      0.9670\nPos Pred Value           0.6957        0.5397      0.8333\nNeg Pred Value           0.8395        0.8125      0.8073\nPrevalence               0.3543        0.3622      0.2835\nDetection Rate           0.2520        0.2677      0.1181\nDetection Prevalence     0.3622        0.4961      0.1417\nBalanced Accuracy        0.7702        0.6906      0.6918\n\npruned.ct.pred2 &lt;- predict(pruned.ct, vg_valid, type=\"class\")\nconfusionMatrix(pruned.ct.pred2, vg_valid$total_sales)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Low Medium High\n    Low     14     13    2\n    Medium   9     10   10\n    High     4      2    3\n\nOverall Statistics\n                                        \n               Accuracy : 0.403         \n                 95% CI : (0.2849, 0.53)\n    No Information Rate : 0.403         \n    P-Value [Acc &gt; NIR] : 0.54633       \n                                        \n                  Kappa : 0.0583        \n                                        \n Mcnemar's Test P-Value : 0.08112       \n\nStatistics by Class:\n\n                     Class: Low Class: Medium Class: High\nSensitivity              0.5185        0.4000     0.20000\nSpecificity              0.6250        0.5476     0.88462\nPos Pred Value           0.4828        0.3448     0.33333\nNeg Pred Value           0.6579        0.6053     0.79310\nPrevalence               0.4030        0.3731     0.22388\nDetection Rate           0.2090        0.1493     0.04478\nDetection Prevalence     0.4328        0.4328     0.13433\nBalanced Accuracy        0.5718        0.4738     0.54231\n\n\nIn comparison the pruned tree shows lower accuracy on both the training set (61.42%) and the validation set (41.79%). While the overall accuracy is lower, the smaller performance gap between training and validation data indicates that the pruned tree generalizes better and is less prone to overfitting. Although the huge tree model appears to perform better in terms of accuracy. Therefore, the pruned model is more reliable for making predictions on new data, even if it comes at the cost of slightly lower accuracy. When working with the model that has more than two outcome parameters, the accuracy not always enough to evaluate performance. More variables, more chance the model predict wrong class, and baseline accuracy also drops down, making a high accuracy harder to achieve.\nWhen using a pruned tree, the difference between training and validation accuracy decreases because pruning reduces overfitting. By removing unnecessary splits, the model captures meaningful patterns rather than noise. As a result the training accuracy decreases, but validation accuracy remains more stable making the model more reliable to new data."
  },
  {
    "objectID": "projects/spotify/analysis.html",
    "href": "projects/spotify/analysis.html",
    "title": "K-Nearest Neighbors: You Like This Song…But Will George Like It?",
    "section": "",
    "text": "In this project, I’ll use k-NN clustering analysis to find out whether George—a fictional character—would vibe with my song or not.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\n\nspotify_2023 &lt;- read.csv('spotify-2023.csv')\nstr(spotify_2023)\n\n'data.frame':   953 obs. of  24 variables:\n $ track_name          : chr  \"Seven (feat. Latto) (Explicit Ver.)\" \"LALA\" \"vampire\" \"Cruel Summer\" ...\n $ artist.s._name      : chr  \"Latto, Jung Kook\" \"Myke Towers\" \"Olivia Rodrigo\" \"Taylor Swift\" ...\n $ artist_count        : int  2 1 1 1 1 2 2 1 1 2 ...\n $ released_year       : int  2023 2023 2023 2019 2023 2023 2023 2023 2023 2023 ...\n $ released_month      : int  7 3 6 8 5 6 3 7 5 3 ...\n $ released_day        : int  14 23 30 23 18 1 16 7 15 17 ...\n $ in_spotify_playlists: int  553 1474 1397 7858 3133 2186 3090 714 1096 2953 ...\n $ in_spotify_charts   : int  147 48 113 100 50 91 50 43 83 44 ...\n $ streams             : chr  \"141381703\" \"133716286\" \"140003974\" \"800840817\" ...\n $ in_apple_playlists  : int  43 48 94 116 84 67 34 25 60 49 ...\n $ in_apple_charts     : int  263 126 207 207 133 213 222 89 210 110 ...\n $ in_deezer_playlists : chr  \"45\" \"58\" \"91\" \"125\" ...\n $ in_deezer_charts    : int  10 14 14 12 15 17 13 13 11 13 ...\n $ in_shazam_charts    : chr  \"826\" \"382\" \"949\" \"548\" ...\n $ bpm                 : int  125 92 138 170 144 141 148 100 130 170 ...\n $ key                 : chr  \"B\" \"C#\" \"F\" \"A\" ...\n $ mode                : chr  \"Major\" \"Major\" \"Major\" \"Major\" ...\n $ danceability_.      : int  80 71 51 55 65 92 67 67 85 81 ...\n $ valence_.           : int  89 61 32 58 23 66 83 26 22 56 ...\n $ energy_.            : int  83 74 53 72 80 58 76 71 62 48 ...\n $ acousticness_.      : int  31 7 17 11 14 19 48 37 12 21 ...\n $ instrumentalness_.  : int  0 0 0 0 63 0 0 0 0 0 ...\n $ liveness_.          : int  8 10 31 11 11 8 8 11 28 8 ...\n $ speechiness_.       : int  4 4 6 15 6 24 3 4 9 33 ...\n\n\nMy song is Money Trees - Kendrick Lamar, Jay Rock. I’m a big fan of Kendrick’s music, and especially songs from this album is one of the favorites of mine.\nHere is the values of this song from dataset:\ndanceability: 74\nenergy: 53\nspeechiness: 10\nacousticness: 7\nliveness: 21\nvalence: 37\nBPM: 144\n\nmy_song &lt;- spotify_2023 %&gt;% filter(track_name == 'Money Trees')\nmy_song\n\n   track_name           artist.s._name artist_count released_year\n1 Money Trees Kendrick Lamar, Jay Rock            2          2012\n  released_month released_day in_spotify_playlists in_spotify_charts    streams\n1              1            1                26792                32 1093605526\n  in_apple_playlists in_apple_charts in_deezer_playlists in_deezer_charts\n1                 69             113                 695                0\n  in_shazam_charts bpm key  mode danceability_. valence_. energy_.\n1              458 144   E Minor             74        37       53\n  acousticness_. instrumentalness_. liveness_. speechiness_.\n1              7                  0         21            10\n\n\n\nspotify &lt;- read.csv('spotify.csv')\n\nstr(spotify)\n\n'data.frame':   2017 obs. of  17 variables:\n $ X               : int  0 1 2 3 4 5 6 7 8 9 ...\n $ acousticness    : num  0.0102 0.199 0.0344 0.604 0.18 0.00479 0.0145 0.0202 0.0481 0.00208 ...\n $ danceability    : num  0.833 0.743 0.838 0.494 0.678 0.804 0.739 0.266 0.603 0.836 ...\n $ duration_ms     : int  204600 326933 185707 199413 392893 251333 241400 349667 202853 226840 ...\n $ energy          : num  0.434 0.359 0.412 0.338 0.561 0.56 0.472 0.348 0.944 0.603 ...\n $ instrumentalness: num  2.19e-02 6.11e-03 2.34e-04 5.10e-01 5.12e-01 0.00 7.27e-06 6.64e-01 0.00 0.00 ...\n $ key             : int  2 1 2 5 5 8 1 10 11 7 ...\n $ liveness        : num  0.165 0.137 0.159 0.0922 0.439 0.164 0.207 0.16 0.342 0.571 ...\n $ loudness        : num  -8.79 -10.4 -7.15 -15.24 -11.65 ...\n $ mode            : int  1 1 1 1 0 1 1 0 0 1 ...\n $ speechiness     : num  0.431 0.0794 0.289 0.0261 0.0694 0.185 0.156 0.0371 0.347 0.237 ...\n $ tempo           : num  150.1 160.1 75 86.5 174 ...\n $ time_signature  : num  4 4 4 4 4 4 4 4 4 4 ...\n $ valence         : num  0.286 0.588 0.173 0.23 0.904 0.264 0.308 0.393 0.398 0.386 ...\n $ target          : int  1 1 1 1 1 1 1 1 1 1 ...\n $ song_title      : chr  \"Mask Off\" \"Redbone\" \"Xanny Family\" \"Master Of None\" ...\n $ artist          : chr  \"Future\" \"Childish Gambino\" \"Future\" \"Beach House\" ...\n\nspotify$target &lt;- factor(spotify$target)\nlevels(spotify$target)\n\n[1] \"0\" \"1\"\n\ntable(spotify$target)\n\n\n   0    1 \n 997 1020 \n\n\n\nData Exploration\nTarget variable is of type int, then I converted it to categorical variable (factor).\nThe target factor variable has 2 categories: 0 or 1. By counting total number of rows for each category, we get that George has 1020 favorite, and 997 disliked songs. Which is interesting, that number of disliked ones pretty close to liked. The music taste of George can be diverse, and Spotify’s recommendation system might be actively adjusting to his preferences. Actually, when you dislike one song in Spotify, the system tries to not suggest you similar songs, and try other different options. To state this opinion constantly we need to explore more about song preferences of George. Furthermore, there are could be temporal patterns in George’s preferences, for instance he prefer certain types of songs at different times of day, month or year.\n\ncolSums(is.na(spotify))\n\n               X     acousticness     danceability      duration_ms \n               0                0                0                0 \n          energy instrumentalness              key         liveness \n               0                0                0                0 \n        loudness             mode      speechiness            tempo \n               0                0                0                0 \n  time_signature          valence           target       song_title \n               0                0                0                0 \n          artist \n               0 \n\n\nThere is no NA values in this dataset.\n\nsummary(spotify_2023)\n\n  track_name        artist.s._name      artist_count   released_year \n Length:953         Length:953         Min.   :1.000   Min.   :1930  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.:2020  \n Mode  :character   Mode  :character   Median :1.000   Median :2022  \n                                       Mean   :1.556   Mean   :2018  \n                                       3rd Qu.:2.000   3rd Qu.:2022  \n                                       Max.   :8.000   Max.   :2023  \n released_month    released_day   in_spotify_playlists in_spotify_charts\n Min.   : 1.000   Min.   : 1.00   Min.   :   31        Min.   :  0.00   \n 1st Qu.: 3.000   1st Qu.: 6.00   1st Qu.:  875        1st Qu.:  0.00   \n Median : 6.000   Median :13.00   Median : 2224        Median :  3.00   \n Mean   : 6.034   Mean   :13.93   Mean   : 5200        Mean   : 12.01   \n 3rd Qu.: 9.000   3rd Qu.:22.00   3rd Qu.: 5542        3rd Qu.: 16.00   \n Max.   :12.000   Max.   :31.00   Max.   :52898        Max.   :147.00   \n   streams          in_apple_playlists in_apple_charts  in_deezer_playlists\n Length:953         Min.   :  0.00     Min.   :  0.00   Length:953         \n Class :character   1st Qu.: 13.00     1st Qu.:  7.00   Class :character   \n Mode  :character   Median : 34.00     Median : 38.00   Mode  :character   \n                    Mean   : 67.81     Mean   : 51.91                      \n                    3rd Qu.: 88.00     3rd Qu.: 87.00                      \n                    Max.   :672.00     Max.   :275.00                      \n in_deezer_charts in_shazam_charts        bpm            key           \n Min.   : 0.000   Length:953         Min.   : 65.0   Length:953        \n 1st Qu.: 0.000   Class :character   1st Qu.:100.0   Class :character  \n Median : 0.000   Mode  :character   Median :121.0   Mode  :character  \n Mean   : 2.666                      Mean   :122.5                     \n 3rd Qu.: 2.000                      3rd Qu.:140.0                     \n Max.   :58.000                      Max.   :206.0                     \n     mode           danceability_.    valence_.        energy_.    \n Length:953         Min.   :23.00   Min.   : 4.00   Min.   : 9.00  \n Class :character   1st Qu.:57.00   1st Qu.:32.00   1st Qu.:53.00  \n Mode  :character   Median :69.00   Median :51.00   Median :66.00  \n                    Mean   :66.97   Mean   :51.43   Mean   :64.28  \n                    3rd Qu.:78.00   3rd Qu.:70.00   3rd Qu.:77.00  \n                    Max.   :96.00   Max.   :97.00   Max.   :97.00  \n acousticness_.  instrumentalness_.   liveness_.    speechiness_.  \n Min.   : 0.00   Min.   : 0.000     Min.   : 3.00   Min.   : 2.00  \n 1st Qu.: 6.00   1st Qu.: 0.000     1st Qu.:10.00   1st Qu.: 4.00  \n Median :18.00   Median : 0.000     Median :12.00   Median : 6.00  \n Mean   :27.06   Mean   : 1.581     Mean   :18.21   Mean   :10.13  \n 3rd Qu.:43.00   3rd Qu.: 0.000     3rd Qu.:24.00   3rd Qu.:11.00  \n Max.   :97.00   Max.   :91.000     Max.   :97.00   Max.   :64.00  \n\nspotify_2023$danceability_. &lt;- spotify_2023$danceability_./100\nspotify_2023$energy_. &lt;- spotify_2023$energy_./100\nspotify_2023$speechiness_. &lt;- spotify_2023$speechiness_./100\nspotify_2023$valence_. &lt;- spotify_2023$valence_./100\nspotify_2023$acousticness_. &lt;- spotify_2023$acousticness_./100\nspotify_2023$liveness_. &lt;- spotify_2023$liveness_./100\n\nspotify_2023 &lt;- spotify_2023 %&gt;% rename(danceability=danceability_., energy=energy_., speechiness=speechiness_., valence=valence_., acousticness=acousticness_., liveness=liveness_., tempo=bpm)\n\nmy_song &lt;- spotify_2023 %&gt;% filter(track_name == 'Money Trees')\n\nI converted the values in spotify_23 to decimal format. Also, applied the same changes to my_song by recreating it.\n\n\nData Partition\n\nset.seed(79)\nspotify.index &lt;- sample(c(1:nrow(spotify)), nrow(spotify)*0.6)\nspotify_train.df &lt;- spotify[spotify.index, ]\nspotify_valid.df &lt;- spotify[-spotify.index, ]\n\n\nliked &lt;- spotify_train.df %&gt;% filter(target==1)\ndisliked &lt;- spotify_train.df %&gt;% filter(target==0)\n\nt.test(liked$danceability, disliked$danceability)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$danceability and disliked$danceability\nt = 5.9297, df = 1198.7, p-value = 3.965e-09\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.03618685 0.07197390\nsample estimates:\nmean of x mean of y \n0.6451020 0.5910216 \n\nt.test(liked$tempo, disliked$tempo)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$tempo and disliked$tempo\nt = 0.32976, df = 1188, p-value = 0.7416\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.495354  3.503627\nsample estimates:\nmean of x mean of y \n 121.3866  120.8825 \n\nt.test(liked$energy, disliked$energy)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$energy and disliked$energy\nt = 0.90709, df = 1116.6, p-value = 0.3646\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.01228227  0.03340286\nsample estimates:\nmean of x mean of y \n0.6986168 0.6880565 \n\nt.test(liked$speechiness, disliked$speechiness)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$speechiness and disliked$speechiness\nt = 5.9565, df = 1102.5, p-value = 3.461e-09\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.01988822 0.03942699\nsample estimates:\n mean of x  mean of y \n0.10598684 0.07632924 \n\nt.test(liked$valence, disliked$valence)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$valence and disliked$valence\nt = 3.4455, df = 1207.9, p-value = 0.0005895\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.02065933 0.07529932\nsample estimates:\nmean of x mean of y \n0.5254077 0.4774284 \n\nt.test(liked$acousticness, disliked$acousticness)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$acousticness and disliked$acousticness\nt = -4.0701, df = 1122.3, p-value = 5.028e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.08721203 -0.03047773\nsample estimates:\nmean of x mean of y \n0.1508101 0.2096550 \n\nt.test(liked$liveness, disliked$liveness)\n\n\n    Welch Two Sample t-test\n\ndata:  liked$liveness and disliked$liveness\nt = 1.5056, df = 1192.1, p-value = 0.1324\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.004181182  0.031773731\nsample estimates:\nmean of x mean of y \n0.1991905 0.1853942 \n\n\nBased on the results above, here is the list of variables that show significant difference: Danceability(p_value = 3.965e-09), speechiness(p-value = 3.461e-09), valence(p-value = 0.0005895), acousticness(p-value = 5.028e-05). Very low p-value suggests that, there is significant difference on this values between liked and disliked songs, making them main parameters to identify George’s preferences in music. Other remaining variables have p-value more than typical threshold 0.05: tempo(p-value = 0.7416), energy(p-value = 0.3646), liveness(p-value = 0.1324).\n\nspotify_train.df &lt;- spotify_train.df %&gt;% select(-tempo, -energy, -liveness)\n\nk-NN method draws information from similarities between the variables by measuring distance between records. Variables with similar values across different outcome classes cannot provide useful information for distinguishing between groups. Including such variables can lead to overfitting, where the model performs well on training data but fails to generalize to new data. These insignificant variables affect the distance calculation, making it harder to distinguish between groups.\n\nhead(spotify_train.df)\n\n        X acousticness danceability duration_ms instrumentalness key loudness\n1454 1453      0.02140        0.587      198426            0.698   4   -4.439\n703   702      0.32000        0.723      210880            0.000   5   -7.110\n1155 1154      0.00464        0.565      225526            0.000   1   -5.220\n1048 1047      0.09150        0.533      230722            0.000   0   -8.266\n311   310      0.04980        0.403      274480            0.000  11   -5.889\n1522 1521      0.97300        0.478      388040            0.939   5  -20.847\n     mode speechiness time_signature valence target\n1454    0      0.0375              4   0.201      0\n703     1      0.0833              4   0.261      1\n1155    1      0.2120              4   0.551      0\n1048    1      0.0324              4   0.506      0\n311     0      0.3380              4   0.734      1\n1522    1      0.0505              4   0.235      0\n                            song_title                   artist\n1454                            Legacy Dimitri Vangelis & Wyman\n703       25 Bucks [feat. Purity Ring]              Danny Brown\n1155 Good Life (with G-Eazy & Kehlani)                   G-Eazy\n1048                        Real Thing           Zac Brown Band\n311                         Auditorium                  Mos Def\n1522                     For Wee Folks           Joey Alexander\n\n\n\n\nNormalization\nIn this step we are normalizing only those columns that will be used in k-NN model building.\n\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nspotify_train_norm.df &lt;- spotify_train.df\nspotify_valid_norm.df &lt;- spotify_valid.df\nspotify_norm.df &lt;- spotify\nmy_song_norm &lt;- my_song\n\n\nnorm_values &lt;- preProcess(\n  spotify_train.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")], \n  method = c(\"center\", \"scale\"))\n\nspotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] &lt;- \n  predict(norm_values, spotify_train.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_train_norm.df)\n\nspotify_valid_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] &lt;- \n  predict(norm_values, spotify_valid.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_valid_norm.df)\n\nspotify_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] &lt;- \n  predict(norm_values, spotify[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_norm.df)\n\nmy_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] &lt;- \n  predict(norm_values, my_song[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\nmy_song_norm\n\n   track_name           artist.s._name artist_count released_year\n1 Money Trees Kendrick Lamar, Jay Rock            2          2012\n  released_month released_day in_spotify_playlists in_spotify_charts    streams\n1              1            1                26792                32 1093605526\n  in_apple_playlists in_apple_charts in_deezer_playlists in_deezer_charts\n1                 69             113                 695                0\n  in_shazam_charts tempo key  mode danceability    valence energy acousticness\n1              458   144   E Minor    0.7569021 -0.5406409   0.53   -0.4355796\n  instrumentalness_. liveness speechiness\n1                  0     0.21  0.09968928\n\n\n\n\nClustering\n\n\n[1] 1\nattr(,\"nn.index\")\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]  823  966  277  929  653  984  436\nattr(,\"nn.dist\")\n          [,1]      [,2]      [,3]      [,4]      [,5]     [,6]      [,7]\n[1,] 0.0233238 0.3074891 0.3136877 0.3537573 0.4308537 0.471654 0.4811864\nLevels: 1\n\n\n           song_title         artist target acousticness danceability\n527       Money Trees Kendrick Lamar      1       0.0726        0.739\n1429        My Friend            EDX      0       0.0330        0.707\n633              Kids         Iamsu!      1       0.1210        0.719\n348          Pacifier     Young Thug      1       0.0469        0.766\n728           Falling           HAIM      1       0.0760        0.741\n800  18 With A Bullet Pete Wingfield      1       0.0625        0.758\n189       N.O. Bounce    Big Freedia      1       0.0190        0.695\n     speechiness valence\n527       0.1010   0.374\n1429      0.0901   0.403\n633       0.0829   0.357\n348       0.1240   0.401\n728       0.0690   0.310\n800       0.1060   0.260\n189       0.1270   0.337\n\n\nBy running k-NN model as a result get “1”, which indicates that George will like my song. And by listing 7 nearest neighbors, I see that my song is also in this list and George already marked it as favorite. Within this songs, George marked only one song as disliked, which highlights not all similar songs are guaranteed to be liked. This disliked song has high valence value compared to others, but there is no difference in other variables. By running knn classification we get 7 nearest records with low distance value from our selected song. So if we just use numbers these songs look very similar to each other. But they are not. And the diversity of artists suggests George’s musical preferences are varied.\n\naccuracy.df &lt;- data.frame(k = seq(1,14,1), accuracy = rep(0,14))\n\nfor(i in 1:14) {\n  knn.pred &lt;- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = spotify_valid_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],\n           k=i)\n  \n  accuracy.df[i, 2] &lt;- confusionMatrix(knn.pred, spotify_valid_norm.df[ ,c(\"target\")])$overall['Accuracy']\n}\n\naccuracy.df[order(-accuracy.df$accuracy), ]\n\n    k  accuracy\n14 14 0.6183395\n12 12 0.6096654\n13 13 0.6096654\n10 10 0.6084263\n11 11 0.6034696\n5   5 0.6022305\n9   9 0.5997522\n4   4 0.5972739\n6   6 0.5960347\n8   8 0.5947955\n3   3 0.5885998\n7   7 0.5861214\n1   1 0.5774473\n2   2 0.5662949\n\n\nFrom the list above we can see accuracy for different k values between 1 and 14. We can see that the difference in accuracy between values is very small. k=14 has highest accuracy value 0.6183395, also k=5 provides very similar number 0.6022305.\n\n\n\n\n\n\n\n\n\nThe graph clearly illustrates the differences in accuracy across various k-values. k = 10 has about 61% accuracy, similar to k = 12 and k = 13. Since they give the same result, k = 10 is a better choice to reduce noise. Additionally, the previously used k = 7 had one of the lowest accuracy scores at 59%. While k = 14 had the highest accuracy at 62%, k = 10 appears to be a more balanced choice. Selecting 10 nearest neighbors should provide a more reliable classification of my song.\n\nnn_10 &lt;- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = my_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],##what we are classifying: like or dislike\n           k=10)\nnn_10\n\n[1] 1\nattr(,\"nn.index\")\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]  823  966  277  929  653  984  436  476  136   127\nattr(,\"nn.dist\")\n          [,1]      [,2]      [,3]      [,4]      [,5]     [,6]      [,7]\n[1,] 0.0233238 0.3074891 0.3136877 0.3537573 0.4308537 0.471654 0.4811864\n         [,8]      [,9]     [,10]\n[1,] 0.501667 0.5050047 0.5092467\nLevels: 1\n\nnn_indexes_10 &lt;- row.names(spotify_train.df)[attr(nn_10, \"nn.index\")]\nspotify_train.df[nn_indexes_10, ] %&gt;% select(song_title, artist, target, acousticness, danceability, speechiness, valence)\n\n           song_title         artist target acousticness danceability\n527       Money Trees Kendrick Lamar      1       0.0726        0.739\n1429        My Friend            EDX      0       0.0330        0.707\n633              Kids         Iamsu!      1       0.1210        0.719\n348          Pacifier     Young Thug      1       0.0469        0.766\n728           Falling           HAIM      1       0.0760        0.741\n800  18 With A Bullet Pete Wingfield      1       0.0625        0.758\n189       N.O. Bounce    Big Freedia      1       0.0190        0.695\n661        Next Order      Dog Blood      1       0.0155        0.760\n1191    Glad You Came     The Wanted      0       0.0643        0.755\n1094           Hey DJ           CNCO      0       0.0309        0.792\n     speechiness valence\n527       0.1010   0.374\n1429      0.0901   0.403\n633       0.0829   0.357\n348       0.1240   0.401\n728       0.0690   0.310\n800       0.1060   0.260\n189       0.1270   0.337\n661       0.0733   0.294\n1191      0.0687   0.454\n1094      0.0867   0.450\n\n\nI chose k=10 as optimal with moderate accuracy value. The output of model didn’t change, it indicates George will like my song. But for now I got 10 nearest neighbors, and from this new list George disliked 3 songs. All these songs have high value of danceability around 70%, low speechiness and acousticness. All 3 disliked songs as for k=7, have higher value of valence compared to others. Higher valence indicates more positive, cheerful, or euphoric songs. It seems that George might prefer songs with lower valence, which are less positive, more neutral in mood or moodier over cheerful ones. Disliked songs have relatively low acousticness, this suggest that George prefer songs with slightly more acoustic elements. The danceability is quite similar for both groups, which implies this factor is not strong in determining preferences. The disliked songs have relatively low speechiness, and some liked songs have higher speechiness (‘Pacifier’ has 0.1240) indicating George prefer songs with more spoken lyrics or rap.\n\n\nLimitations of model\nI think main limitation here is that we are relying on numerical variables to predict whether someone will like this song or not. There are can be other factors such as good memories or associations with a song which can make them favorite. Also lyrics play main role in connecting with listeners on an emotional level. For instance, I tend to prefer songs with meaningful lyrics, while rap elements often give me an energy boost. Additionally, music preferences can vary based on context—what I listen to at the gym or while walking differs from what I play in the evening when I can’t sleep."
  },
  {
    "objectID": "projects/vancouver_311/vancouver_311.html",
    "href": "projects/vancouver_311/vancouver_311.html",
    "title": "City service requests made in Vancouver, British Columbia from 2022 to the present.",
    "section": "",
    "text": "This project focuses on exploring and visualizing data related to city service requests in Vancouver, British Columbia. The dataset is sourced from Vancouver’s Open Data Portal and contains information about service requests made from 2022 to the present.\nThis work is part of an assignment for the AD699 Data Mining course.\n\nData Exploration\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nvancouver_df &lt;- read.csv('./vancouver_311_requests.csv', sep = ';')\nhead(vancouver_df)\n\n                                Department\n1 ENG - Parking Enforcement and Operations\n2                       FSC - Property Tax\n3                    DBL - Services Centre\n4                    DBL - Services Centre\n5                    DBL - Services Centre\n6                ENG - Sanitation Services\n                     Service.request.type Status             Closure.reason\n1     Abandoned or Uninsured Vehicle Case  Close          Insufficient info\n2               Property Tax Request Case  Close Alternate Service Required\n3   Building and Development Inquiry Case  Close           Service provided\n4   Building and Development Inquiry Case  Close           Service provided\n5 Tenant Improvement Program Request Case  Close           Service provided\n6              Abandoned Recyclables Case  Close           Service provided\n  Service.request.open.timestamp Service.request.close.date\n1      2023-10-24T16:38:00-04:00                 2023-10-24\n2      2023-10-24T16:40:00-04:00                 2023-10-25\n3      2023-10-24T16:42:00-04:00                 2023-10-27\n4      2023-10-24T16:45:00-04:00                 2023-10-27\n5      2023-10-24T16:48:00-04:00                 2023-10-27\n6      2023-10-24T16:52:00-04:00                 2023-11-02\n    Last.modified.timestamp        Address    Local.area    Channel Latitude\n1 2023-10-24T18:58:39-04:00                              Mobile App       NA\n2 2023-10-25T14:05:13-04:00                     Fairview      Phone       NA\n3 2023-10-27T17:09:55-04:00                     Downtown        WEB       NA\n4 2023-10-27T15:05:34-04:00                Arbutus Ridge        WEB       NA\n5 2023-10-27T12:35:20-04:00                   Strathcona        WEB       NA\n6 2023-11-02T15:35:38-04:00 3 ALEXANDER ST      Downtown        WEB 49.28369\n  Longitude                             geom\n1        NA                                 \n2        NA                                 \n3        NA                                 \n4        NA                                 \n5        NA                                 \n6  -123.104 49.28368575572, -123.10397851188\n\n\n\nstr(vancouver_df)\n\n'data.frame':   842862 obs. of  13 variables:\n $ Department                    : chr  \"ENG - Parking Enforcement and Operations\" \"FSC - Property Tax\" \"DBL - Services Centre\" \"DBL - Services Centre\" ...\n $ Service.request.type          : chr  \"Abandoned or Uninsured Vehicle Case\" \"Property Tax Request Case\" \"Building and Development Inquiry Case\" \"Building and Development Inquiry Case\" ...\n $ Status                        : chr  \"Close\" \"Close\" \"Close\" \"Close\" ...\n $ Closure.reason                : chr  \"Insufficient info\" \"Alternate Service Required\" \"Service provided\" \"Service provided\" ...\n $ Service.request.open.timestamp: chr  \"2023-10-24T16:38:00-04:00\" \"2023-10-24T16:40:00-04:00\" \"2023-10-24T16:42:00-04:00\" \"2023-10-24T16:45:00-04:00\" ...\n $ Service.request.close.date    : chr  \"2023-10-24\" \"2023-10-25\" \"2023-10-27\" \"2023-10-27\" ...\n $ Last.modified.timestamp       : chr  \"2023-10-24T18:58:39-04:00\" \"2023-10-25T14:05:13-04:00\" \"2023-10-27T17:09:55-04:00\" \"2023-10-27T15:05:34-04:00\" ...\n $ Address                       : chr  \"\" \"\" \"\" \"\" ...\n $ Local.area                    : chr  \"\" \"Fairview\" \"Downtown\" \"Arbutus Ridge\" ...\n $ Channel                       : chr  \"Mobile App\" \"Phone\" \"WEB\" \"WEB\" ...\n $ Latitude                      : num  NA NA NA NA NA ...\n $ Longitude                     : num  NA NA NA NA NA ...\n $ geom                          : chr  \"\" \"\" \"\" \"\" ...\n\n\nstr() function shows structure of an object. From the result above we can see that, type of our dataset is data.frame which consists of 842862 rows and 13 columns. And also shows the type of each column.\n\nunique(vancouver_df$Local.area)\n\n [1] \"\"                         \"Fairview\"                \n [3] \"Downtown\"                 \"Arbutus Ridge\"           \n [5] \"Strathcona\"               \"Mount Pleasant\"          \n [7] \"Shaughnessy\"              \"West Point Grey\"         \n [9] \"Kitsilano\"                \"West End\"                \n[11] \"Sunset\"                   \"South Cambie\"            \n[13] \"Marpole\"                  \"Kensington-Cedar Cottage\"\n[15] \"Grandview-Woodland\"       \"Oakridge\"                \n[17] \"Hastings-Sunrise\"         \"Renfrew-Collingwood\"     \n[19] \"Riley Park\"               \"Victoria-Fraserview\"     \n[21] \"Kerrisdale\"               \"Killarney\"               \n[23] \"Dunbar-Southlands\"       \n\n\n\nlength(unique(vancouver_df$Local.area))\n\n[1] 23\n\n\nIn our dataset 23 unique values of Local.area including empty value.\n\nsunset_df &lt;- filter(vancouver_df, Local.area == 'Sunset')\nnrow(sunset_df)\n\n[1] 33036\n\n\nNow I have 33036 records from my area Sunset.\n\nstr(sunset_df)\n\n'data.frame':   33036 obs. of  13 variables:\n $ Department                    : chr  \"DBL - Services Centre\" \"DBL - Animal Services\" \"DBL - Services Centre\" \"DBL - Property Use Inspections\" ...\n $ Service.request.type          : chr  \"Building and Development Inquiry Case\" \"Animal Concern Case\" \"Building and Development Inquiry Case\" \"Private Property Concern Case\" ...\n $ Status                        : chr  \"Close\" \"Close\" \"Close\" \"Close\" ...\n $ Closure.reason                : chr  \"Service provided\" \"Further action has been planned\" \"Service provided\" \"Assigned to inspector\" ...\n $ Service.request.open.timestamp: chr  \"2023-10-24T18:05:19-04:00\" \"2023-10-24T20:05:27-04:00\" \"2023-08-13T14:02:01-04:00\" \"2023-08-13T22:54:01-04:00\" ...\n $ Service.request.close.date    : chr  \"2023-10-25\" \"2023-10-24\" \"2023-08-16\" \"2023-08-16\" ...\n $ Last.modified.timestamp       : chr  \"2023-10-25T14:20:17-04:00\" \"2023-10-24T20:33:26-04:00\" \"2023-08-16T13:46:59-04:00\" \"2023-08-16T15:01:47-04:00\" ...\n $ Address                       : chr  \"\" \"\" \"\" \"\" ...\n $ Local.area                    : chr  \"Sunset\" \"Sunset\" \"Sunset\" \"Sunset\" ...\n $ Channel                       : chr  \"WEB\" \"Phone\" \"WEB\" \"WEB\" ...\n $ Latitude                      : num  NA NA NA NA NA ...\n $ Longitude                     : num  NA NA NA NA NA ...\n $ geom                          : chr  \"\" \"\" \"\" \"\" ...\n\n\nThe following columns have date-related information: Service.request.open.timestamp, Service.request.close.date, Last.modified.timestamp. Now R see them as character not date.\n\nsunset_df$Service.request.open.timestamp &lt;- as.Date(sunset_df$Service.request.open.timestamp)\nsunset_df$Service.request.close.date &lt;- as.Date(sunset_df$Service.request.close.date)\nsunset_df$Last.modified.timestamp &lt;- as.Date(sunset_df$Last.modified.timestamp)\n\nstr(sunset_df)\n\n'data.frame':   33036 obs. of  13 variables:\n $ Department                    : chr  \"DBL - Services Centre\" \"DBL - Animal Services\" \"DBL - Services Centre\" \"DBL - Property Use Inspections\" ...\n $ Service.request.type          : chr  \"Building and Development Inquiry Case\" \"Animal Concern Case\" \"Building and Development Inquiry Case\" \"Private Property Concern Case\" ...\n $ Status                        : chr  \"Close\" \"Close\" \"Close\" \"Close\" ...\n $ Closure.reason                : chr  \"Service provided\" \"Further action has been planned\" \"Service provided\" \"Assigned to inspector\" ...\n $ Service.request.open.timestamp: Date, format: \"2023-10-24\" \"2023-10-24\" ...\n $ Service.request.close.date    : Date, format: \"2023-10-25\" \"2023-10-24\" ...\n $ Last.modified.timestamp       : Date, format: \"2023-10-25\" \"2023-10-24\" ...\n $ Address                       : chr  \"\" \"\" \"\" \"\" ...\n $ Local.area                    : chr  \"Sunset\" \"Sunset\" \"Sunset\" \"Sunset\" ...\n $ Channel                       : chr  \"WEB\" \"Phone\" \"WEB\" \"WEB\" ...\n $ Latitude                      : num  NA NA NA NA NA ...\n $ Longitude                     : num  NA NA NA NA NA ...\n $ geom                          : chr  \"\" \"\" \"\" \"\" ...\n\n\nNow R sees these columns as Date.\n\nsunset_df &lt;- sunset_df %&gt;% mutate(duration = as.numeric(Service.request.close.date - Service.request.open.timestamp, units=\"days\"))\n\nTo extract numeric value of difference between dates, I used as.numeric() function and specified units as days.\n\nsum(is.na(sunset_df))\n\n[1] 41170\n\n\nIn our dataset 41170 total NA values.\n\ncolSums(is.na(sunset_df))\n\n                    Department           Service.request.type \n                             0                              0 \n                        Status                 Closure.reason \n                             0                              0 \nService.request.open.timestamp     Service.request.close.date \n                             0                            523 \n       Last.modified.timestamp                        Address \n                             0                              0 \n                    Local.area                        Channel \n                             0                              0 \n                      Latitude                      Longitude \n                         20062                          20062 \n                          geom                       duration \n                             0                            523 \n\n\nHere is the total # of NA values for each column. The columns Latitude and Longitude each has 20062 missing values, probably Address column is also contain empty values. The service close date didn’t recorded 523 times, which is affected duration column too.\n\nlibrary(lubridate)\n\nbirthday_reqs &lt;- sunset_df %&gt;% filter(month(Service.request.open.timestamp) == 11 & day(Service.request.open.timestamp) == 24)\nnrow(birthday_reqs)\n\n[1] 64\n\n\nMy birthday is in November 24th, and by using functions from lubridate package, we see that in my birthday occurred 64 requests.\n\nbirthday_reqs_channel &lt;- birthday_reqs %&gt;% group_by(Channel) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count))\nbirthday_reqs_channel\n\n# A tibble: 4 × 2\n  Channel    Count\n  &lt;chr&gt;      &lt;int&gt;\n1 WEB           27\n2 Phone         25\n3 Mobile App    11\n4 Chat           1\n\n\nOn this date the most of requests came from WEB, Phone channels.\n\nbirthday_reqs_types &lt;- birthday_reqs %&gt;% group_by(Service.request.type) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count))\nbirthday_reqs_types\n\n# A tibble: 31 × 2\n   Service.request.type                   Count\n   &lt;chr&gt;                                  &lt;int&gt;\n 1 Missed Green Bin Pickup Case               9\n 2 Green Bin Request Case                     7\n 3 Abandoned Non-Recyclables-Small Case       6\n 4 Business Licence Request Case              5\n 5 Abandoned or Uninsured Vehicle Case        4\n 6 Building and Development Inquiry Case      4\n 7 Abandoned Recyclables Case                 2\n 8 Sewer Drainage and Design Inquiry Case     2\n 9 Street Light Out Case                      2\n10 Street Light Pole Maintenance Case         2\n# ℹ 21 more rows\n\n\nThe top 5 requests inlcude cases related to Green bin (total 16), non-recyclables(total 6), business licence and abandoned vehicle.\n\nsunset_df %&gt;% group_by(Year = year(Service.request.open.timestamp)) %&gt;% summarise(Count = n())\n\n# A tibble: 4 × 2\n   Year Count\n  &lt;dbl&gt; &lt;int&gt;\n1  2022 10840\n2  2023 10192\n3  2024 11018\n4  2025   986\n\n\nThe dataset only contains city service requests going through January of 2025, so the 2025 annual total is not really comparable to the numbers from other years.\n\nsunset_df %&gt;% group_by(Channel) %&gt;% summarise(avg = mean(duration, na.rm = TRUE)) %&gt;% arrange(desc(avg))\n\n# A tibble: 7 × 2\n  Channel        avg\n  &lt;chr&gt;        &lt;dbl&gt;\n1 E-mail       10.5 \n2 Chat         10.1 \n3 Mobile App   10.1 \n4 Phone         9.85\n5 WEB           9.39\n6 Social Media  6.64\n7 Mail          3   \n\n\nFor the channels like E-mail, Chat, Mobile App the average duration to complete service request is more than 10 days. On the other hand, by using Mail channel they spent 3 days on average. Perhaps, since nowadays a lot of requests came from digital/web apps, and the older requests can left at the bottom of the queue which can lead to delays to finish them. Also, different types of requests can be sent through each type of channel, more complicated use E-mail, and small cases use Mail. Or some other factor can affect.\n\nopen_reqs &lt;- sunset_df %&gt;% filter(Status == \"Open\")\n# nrow(open_reqs)\nopen_reqs %&gt;% group_by(Month = month(Service.request.open.timestamp)) %&gt;% summarise(Count = n())\n\n# A tibble: 12 × 2\n   Month Count\n   &lt;dbl&gt; &lt;int&gt;\n 1     1   272\n 2     2    16\n 3     3    14\n 4     4    19\n 5     5    21\n 6     6    23\n 7     7    25\n 8     8    28\n 9     9    10\n10    10    29\n11    11    30\n12    12    36\n\n\n272 out of 523 total open requests are in January only. The dataset was retrieved in January 2025, and most of the yet-unresolved cases in it are recent ones – that’s what explains the January bump\n\nnames(sunset_df)\n\n [1] \"Department\"                     \"Service.request.type\"          \n [3] \"Status\"                         \"Closure.reason\"                \n [5] \"Service.request.open.timestamp\" \"Service.request.close.date\"    \n [7] \"Last.modified.timestamp\"        \"Address\"                       \n [9] \"Local.area\"                     \"Channel\"                       \n[11] \"Latitude\"                       \"Longitude\"                     \n[13] \"geom\"                           \"duration\"                      \n\nsunset_df &lt;- sunset_df %&gt;% rename(Service.request.open.date = Service.request.open.timestamp)\n\nnames(sunset_df)\n\n [1] \"Department\"                 \"Service.request.type\"      \n [3] \"Status\"                     \"Closure.reason\"            \n [5] \"Service.request.open.date\"  \"Service.request.close.date\"\n [7] \"Last.modified.timestamp\"    \"Address\"                   \n [9] \"Local.area\"                 \"Channel\"                   \n[11] \"Latitude\"                   \"Longitude\"                 \n[13] \"geom\"                       \"duration\"                  \n\n\nI renamed the column Service.request.open.timestamp to Service.request.open.date, because now it contains only dates without time.\n\nsunset_df$Address &lt;- NULL\n\nnames(sunset_df)\n\n [1] \"Department\"                 \"Service.request.type\"      \n [3] \"Status\"                     \"Closure.reason\"            \n [5] \"Service.request.open.date\"  \"Service.request.close.date\"\n [7] \"Last.modified.timestamp\"    \"Local.area\"                \n [9] \"Channel\"                    \"Latitude\"                  \n[11] \"Longitude\"                  \"geom\"                      \n[13] \"duration\"                  \n\n\nNow our dataset has 13 columns.\n\n\nData Visualization\n\ndata1 &lt;- sunset_df %&gt;% \n  group_by(DayOfWeek = wday(Service.request.open.date, label = TRUE)) %&gt;% \n  summarise(Count = n()) %&gt;% \n  mutate(DayOfWeek = factor(DayOfWeek, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"), ordered = TRUE)) %&gt;%\n  arrange(DayOfWeek)\n\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nggplot(data1, aes(x=DayOfWeek, y=Count)) +\n  geom_bar(stat = \"identity\", fill=\"thistle\", color=\"black\") +\n  labs(title = \"City Service requests by Day of Week\",\n       x = \"Day of Week\",\n       y = \"Number of Requests\") +\n  theme_clean()\n\n\n\n\n\n\n\n\nThe bar chart displays how many requests were made each day of week.Weekends have only about half the volume of requests, and the middle of the week is when the highest number of requests are occur. Maybe in the weekdays people tend to have more time or prefer to report rather than on weekends.\n\ntop7_req_types &lt;- sunset_df %&gt;% group_by(Service.request.type) %&gt;% summarise(Count = n()) %&gt;% arrange(desc(Count)) %&gt;% slice_head(n=7)\ntop7_req_types$Service.request.type\n\n[1] \"Missed Green Bin Pickup Case\"         \n[2] \"Building and Development Inquiry Case\"\n[3] \"Missed Garbage Bin Pickup Case\"       \n[4] \"Garbage Bin Request Case\"             \n[5] \"City and Park Trees Maintenance Case\" \n[6] \"Green Bin Request Case\"               \n[7] \"Abandoned Non-Recyclables-Small Case\" \n\nsunset_df_top7 &lt;- sunset_df %&gt;% filter(Service.request.type %in% top7_req_types$Service.request.type)\nnrow(sunset_df_top7)\n\n[1] 16088\n\n\nNow there are only 16088 records with top 7 service request types.\n\ndata2 &lt;- sunset_df_top7 %&gt;% \n  group_by(DayOfWeek = wday(Service.request.open.date, label = TRUE), Department) %&gt;% \n  summarise(Count = n(), .groups = \"drop\") %&gt;% \n  mutate(DayOfWeek = factor(DayOfWeek, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"), ordered = TRUE)) %&gt;%\n  arrange(DayOfWeek)\n\n\nggplot(data2, aes(x=DayOfWeek, y=Count, fill = Department)) + \n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +\n  labs(title = \"City Service requests by Day of Week and Department\",\n       x = \"Day of Week\",\n       y = \"Number of Requests\") +\n  theme_foundation()\n\n\n\n\n\n\n\n\nThe main part of service requests from “ENG-Sanitation Services” no matter which day is it. The “PR-Urban Forestry” requests stays the same during the days of week, while “DBL-Services Centre” requests drops significantly on weekends. What if DBL Services Centre offices are closed on weekends, so citizens know this and wait until the week to make the reports? But if Urban Forestry is set up differently, that might explain why it doesn’t show such a big change. Or maybe Urban Forestry requests can be depend on weather conditions, and occur not so often like Sanitation services.\n\ndata3 &lt;- sunset_df_top7 %&gt;% group_by(Month = month(Service.request.open.date, label = TRUE), Service.request.type) %&gt;% \n  summarise(Count = n(), .groups = \"drop\")\n\nggplot(data3, aes(x=Month, y=Count, fill = Service.request.type)) + \n  geom_bar(stat = \"identity\") +\n  labs(title = \"City Service requests by Month and Service Request Type\",\n       x = \"Month\",\n       y = \"Number of Requests\") +\n  theme_calc()\n\n\n\n\n\n\n\n\nJanuary has highest number of requests, followed by November and December. Missed Green Bin Pickup Case occurred in these 3 months more than other months. In January post-holiday waste can put extra pressure to collection system. Also, November and December which are holiday season can lead to increase of waste too. November is often peak time for leaf fall, it can also impact collection system. It’s interesting that Building and Development inquires spike in January, and stays high during the year. January is like a month of new beginnings, when people tend to start new projects. Maybe it can be one of the reasons of large number of requests.\n\nggplot(sunset_df_top7, aes(x=duration)) + \n  geom_histogram(binwidth = 50, fill = \"skyblue\", color = \"black\", boundary = 0) +\n  labs(title = \"Distribution of Service Request Duration\",\n       x = \"Duration (in days)\",\n       y = \"Number of Requests\"\n       ) +\n  theme_bw()\n\nWarning: Removed 236 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThe distribution is left-skewed. It’s interesting that for some requests to be closed took around 800 days, maybe it’s result of some technical issues, or these cases delayed because of legal issues. And in most cases to complete request took between 0 to 50 days. By setting binwidth = 50, I say that each bin represents 50 days.\n\nggplot(sunset_df_top7, aes(x=duration)) + \n  geom_histogram(binwidth = 30, fill = \"skyblue\", color = \"black\", boundary = 0) +\n  facet_wrap(~Service.request.type) +\n  labs(title = \"Distribution of Service Request Duration\",\n       x = \"Duration (in days)\",\n       y = \"Number of Requests\"\n       ) +\n  theme_bw()\n\nWarning: Removed 236 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nMost of the city requests have a same pattern, which can represent that city requests processes in a quick turnaround time. In some cases it can take longer than 30 days, maybe because of legal issues which can occur for the City and Trees Maintenance case (permits to make changes from multiple departments)\n\nggplot(sunset_df_top7, aes(x=Channel, fill = Service.request.type)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Distribution of Service Request Type by Channel\",\n       y = \"Proportion of requests\",\n       fill = \"Service Request Type\")\n\n\n\n\n\n\n\n\nThis plot shows the distribution of service request types across channels. Interesting, that Abandoned Non-Recyclables mostly reported via mobile app, while City and Park Maintenance dominate in Social Media and E-mail. Also, Building and Development Inquiry mostly reported via WEB. Different requests seems like have preferred channels. For instance, garbage and green bin requests prefer chat or phone, that do not require any additional resources like images. For the City and Park Maintenance social media is popular, maybe because people prefer post about their awareness of city to public discussion.\n\nlibrary(leaflet)\n\nsunset_df_map &lt;- sunset_df %&gt;% filter(!is.na(Latitude) & !is.na(Longitude))\n\nleaflet(data = sunset_df_map) %&gt;%\n  addTiles() %&gt;%\n  addCircles(~Longitude, ~Latitude)\n\n\n\n\n\n\nleaflet(data = sunset_df_map) %&gt;%\n  addProviderTiles(\"Esri.WorldImagery\") %&gt;% \n  addCircles(~Longitude, ~Latitude, radius = 5, color = \"gold\", popup = ~Department)\n\n\n\n\n\nCreated map with Esri World Imagery tiles, and added popup text which will display department name for each service request."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dinara Zhorabek",
    "section": "",
    "text": "Dinara Zhorabek\n\n\nBusiness & Data Analyst | Full Stack Software Engineer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHi, I’m Dinara.\nAn aspiring business & data analyst and middle software engineer, avid traveler, passionate about continuous learning, problem-solving, and empowering others through data-driven solutions.\nI am pursuing a Master’s degree in Applied Business Analytics at Boston University and received a B.S. in Information and Communication Technology from Kazakh-British Technical University.\nI’m skilled in Python, R, SQL, Tableau and Power BI, and I am deeply committed to using data and technology to solve business challenges.\n\n\n\n\n\n\n\nPortfolio\n\n\nA glimpse of the projects I’ve been working on\n\n\n\n\n\n\n\n\n\n\nYou Like This Song…But Will George Like It?\n\n\n\nUser Behavior Classification with Spotify Streaming Data.\n\n\n\n\n\n\n\n\n\n\nComplaint Classifier\n\n\n\nUtilized the Naïve Bayes algorithm to classify and predict consumer disputes.\n\n\n\n \n\n\n\n\n\n\nMarket Basket Insights\n\n\n\nAssociation rule mining.\n\n\n\n \n\n\n\n\n\n\nSkating through Data\n\n\n\nForecasting hockey player salaries by utilizing statistical analysis and machine learning methods.\n\n\n\n \n\n\n\n\n\n\nConsole to Category\n\n\n\nPredicting Video Game Sales Using Classification Trees.\n\n\n\n \n\n\n\n\n\n\nVoices of the City\n\n\n\nAnalyzing Vancouver’s Service Request Patterns.\n\n\n\n \n\n\n\n\n\n\nMET ABA Hackathon 2025\n\n\n\nVortex Sentiment Adaptive Volatility (VSAV) Strategy."
  }
]