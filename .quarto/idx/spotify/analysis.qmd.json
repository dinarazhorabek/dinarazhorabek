{"title":"K-Nearest Neighbors: You Like This Song...But Will George Like It?","markdown":{"yaml":{"title":"K-Nearest Neighbors: You Like This Song...But Will George Like It?","format":"html","editor":"visual"},"headingText":"Data Exploration","containsRefs":false,"markdown":"\n\nIn this project, I’ll use k-NN clustering analysis to find out whether George—a fictional character—would vibe with my song or not.\n\n```{r}\nlibrary(tidyverse)\nlibrary(dplyr)\n\nspotify_2023 <- read.csv('spotify-2023.csv')\nstr(spotify_2023)\n```\n\nMy song is Money Trees - Kendrick Lamar, Jay Rock. I'm a big fan of Kendrick's music, and especially songs from this album is one of the favorites of mine.\n\nHere is the values of this song from dataset:\\\ndanceability: 74\\\nenergy: 53\\\nspeechiness: 10\\\nacousticness: 7\\\nliveness: 21\\\nvalence: 37\\\nBPM: 144\n\n```{r}\nmy_song <- spotify_2023 %>% filter(track_name == 'Money Trees')\nmy_song\n```\n\n```{r}\nspotify <- read.csv('spotify.csv')\n\nstr(spotify)\n\nspotify$target <- factor(spotify$target)\nlevels(spotify$target)\n\ntable(spotify$target)\n```\n\n\nTarget variable is of type int, then I converted it to categorical variable (factor).\n\nThe target factor variable has 2 categories: 0 or 1. By counting total number of rows for each category, we get that George has 1020 favorite, and 997 disliked songs. Which is interesting, that number of disliked ones pretty close to liked. The music taste of George can be diverse, and Spotify's recommendation system might be actively adjusting to his preferences. Actually, when you dislike one song in Spotify, the system tries to not suggest you similar songs, and try other different options. To state this opinion constantly we need to explore more about song preferences of George. Furthermore, there are could be temporal patterns in George's preferences, for instance he prefer certain types of songs at different times of day, month or year.\n\n```{r}\ncolSums(is.na(spotify))\n```\n\nThere is no NA values in this dataset.\n\n```{r}\nsummary(spotify_2023)\n\nspotify_2023$danceability_. <- spotify_2023$danceability_./100\nspotify_2023$energy_. <- spotify_2023$energy_./100\nspotify_2023$speechiness_. <- spotify_2023$speechiness_./100\nspotify_2023$valence_. <- spotify_2023$valence_./100\nspotify_2023$acousticness_. <- spotify_2023$acousticness_./100\nspotify_2023$liveness_. <- spotify_2023$liveness_./100\n\nspotify_2023 <- spotify_2023 %>% rename(danceability=danceability_., energy=energy_., speechiness=speechiness_., valence=valence_., acousticness=acousticness_., liveness=liveness_., tempo=bpm)\n\nmy_song <- spotify_2023 %>% filter(track_name == 'Money Trees')\n```\n\nI converted the values in spotify_23 to decimal format. Also, applied the same changes to my_song by recreating it.\n\n### Data Partition\n\n```{r}\nset.seed(79)\nspotify.index <- sample(c(1:nrow(spotify)), nrow(spotify)*0.6)\nspotify_train.df <- spotify[spotify.index, ]\nspotify_valid.df <- spotify[-spotify.index, ]\n```\n\n```{r}\nliked <- spotify_train.df %>% filter(target==1)\ndisliked <- spotify_train.df %>% filter(target==0)\n\nt.test(liked$danceability, disliked$danceability)\nt.test(liked$tempo, disliked$tempo)\nt.test(liked$energy, disliked$energy)\nt.test(liked$speechiness, disliked$speechiness)\nt.test(liked$valence, disliked$valence)\nt.test(liked$acousticness, disliked$acousticness)\nt.test(liked$liveness, disliked$liveness)\n```\n\nBased on the results above, here is the list of variables that show significant difference: Danceability(p_value = 3.965e-09), speechiness(p-value = 3.461e-09), valence(p-value = 0.0005895), acousticness(p-value = 5.028e-05). Very low p-value suggests that, there is significant difference on this values between liked and disliked songs, making them main parameters to identify George's preferences in music. Other remaining variables have p-value more than typical threshold 0.05: tempo(p-value = 0.7416), energy(p-value = 0.3646), liveness(p-value = 0.1324).\n\n```{r}\nspotify_train.df <- spotify_train.df %>% select(-tempo, -energy, -liveness)\n```\n\nk-NN method draws information from similarities between the variables by measuring distance between records. Variables with similar values across different outcome classes cannot provide useful information for distinguishing between groups. Including such variables can lead to overfitting, where the model performs well on training data but fails to generalize to new data. These insignificant variables affect the distance calculation, making it harder to distinguish between groups.\n\n```{r}\nhead(spotify_train.df)\n```\n\n### Normalization\n\nIn this step we are normalizing only those columns that will be used in k-NN model building.\n\n```{r}\nlibrary(caret)\n\nspotify_train_norm.df <- spotify_train.df\nspotify_valid_norm.df <- spotify_valid.df\nspotify_norm.df <- spotify\nmy_song_norm <- my_song\n\n\nnorm_values <- preProcess(\n  spotify_train.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")], \n  method = c(\"center\", \"scale\"))\n\nspotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] <- \n  predict(norm_values, spotify_train.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_train_norm.df)\n\nspotify_valid_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] <- \n  predict(norm_values, spotify_valid.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_valid_norm.df)\n\nspotify_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] <- \n  predict(norm_values, spotify[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_norm.df)\n\nmy_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] <- \n  predict(norm_values, my_song[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\nmy_song_norm\n```\n\n### Clustering\n\n```{r}\n#| echo: false\nlibrary(FNN)\n# knn is all about numeric data, classification using numeric values\nnn <- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = my_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],##what we are classifying: like or dislike\n           k=7)\nnn\nnn_indexes <- row.names(spotify_train.df)[attr(nn, \"nn.index\")]\nspotify_train.df[nn_indexes, ] %>% select(song_title, artist, target, acousticness, danceability, speechiness, valence)\n```\n\nBy running k-NN model as a result get \"1\", which indicates that George will like my song. And by listing 7 nearest neighbors, I see that my song is also in this list and George already marked it as favorite. Within this songs, George marked only one song as disliked, which highlights not all similar songs are guaranteed to be liked. This disliked song has high valence value compared to others, but there is no difference in other variables. By running knn classification we get 7 nearest records with low distance value from our selected song. So if we just use numbers these songs look very similar to each other. But they are not. And the diversity of artists suggests George's musical preferences are varied.\n\n```{r}\naccuracy.df <- data.frame(k = seq(1,14,1), accuracy = rep(0,14))\n\nfor(i in 1:14) {\n  knn.pred <- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = spotify_valid_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],\n           k=i)\n  \n  accuracy.df[i, 2] <- confusionMatrix(knn.pred, spotify_valid_norm.df[ ,c(\"target\")])$overall['Accuracy']\n}\n\naccuracy.df[order(-accuracy.df$accuracy), ]\n```\n\nFrom the list above we can see accuracy for different k values between 1 and 14. We can see that the difference in accuracy between values is very small. k=14 has highest accuracy value 0.6183395, also k=5 provides very similar number 0.6022305.\n\n```{r}\n#| echo: false\nlibrary(ggplot2)\n\nggplot(accuracy.df, aes(x=k, y=accuracy)) + \n  geom_point() +\n  geom_line() +\n  labs(title = \"Scatterplot of k values vs Accuracy\",\n       x = \"k values\",\n       y = \"Accuracy\") +\n  scale_x_continuous(breaks = seq(min(accuracy.df$k), max(accuracy.df$k), by = 3))\n```\n\nThe graph clearly illustrates the differences in accuracy across various k-values. k = 10 has about 61% accuracy, similar to k = 12 and k = 13. Since they give the same result, k = 10 is a better choice to reduce noise. Additionally, the previously used k = 7 had one of the lowest accuracy scores at 59%. While k = 14 had the highest accuracy at 62%, k = 10 appears to be a more balanced choice. Selecting 10 nearest neighbors should provide a more reliable classification of my song.\n\n```{r}\nnn_10 <- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = my_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],##what we are classifying: like or dislike\n           k=10)\nnn_10\nnn_indexes_10 <- row.names(spotify_train.df)[attr(nn_10, \"nn.index\")]\nspotify_train.df[nn_indexes_10, ] %>% select(song_title, artist, target, acousticness, danceability, speechiness, valence)\n```\n\nI chose k=10 as optimal with moderate accuracy value. The output of model didn't change, it indicates George will like my song. But for now I got 10 nearest neighbors, and from this new list George disliked 3 songs. All these songs have high value of danceability around 70%, low speechiness and acousticness. All 3 disliked songs as for k=7, have higher value of valence compared to others. Higher valence indicates more positive, cheerful, or euphoric songs. It seems that George might prefer songs with lower valence, which are less positive, more neutral in mood or moodier over cheerful ones. Disliked songs have relatively low acousticness, this suggest that George prefer songs with slightly more acoustic elements. The danceability is quite similar for both groups, which implies this factor is not strong in determining preferences. The disliked songs have relatively low speechiness, and some liked songs have higher speechiness ('Pacifier' has 0.1240) indicating George prefer songs with more spoken lyrics or rap.\n\n### Limitations of model\n\nI think main limitation here is that we are relying on numerical variables to predict whether someone will like this song or not. There are can be other factors such as good memories or associations with a song which can make them favorite. Also lyrics play main role in connecting with listeners on an emotional level. For instance, I tend to prefer songs with meaningful lyrics, while rap elements often give me an energy boost. Additionally, music preferences can vary based on context—what I listen to at the gym or while walking differs from what I play in the evening when I can't sleep.","srcMarkdownNoYaml":"\n\nIn this project, I’ll use k-NN clustering analysis to find out whether George—a fictional character—would vibe with my song or not.\n\n```{r}\nlibrary(tidyverse)\nlibrary(dplyr)\n\nspotify_2023 <- read.csv('spotify-2023.csv')\nstr(spotify_2023)\n```\n\nMy song is Money Trees - Kendrick Lamar, Jay Rock. I'm a big fan of Kendrick's music, and especially songs from this album is one of the favorites of mine.\n\nHere is the values of this song from dataset:\\\ndanceability: 74\\\nenergy: 53\\\nspeechiness: 10\\\nacousticness: 7\\\nliveness: 21\\\nvalence: 37\\\nBPM: 144\n\n```{r}\nmy_song <- spotify_2023 %>% filter(track_name == 'Money Trees')\nmy_song\n```\n\n```{r}\nspotify <- read.csv('spotify.csv')\n\nstr(spotify)\n\nspotify$target <- factor(spotify$target)\nlevels(spotify$target)\n\ntable(spotify$target)\n```\n\n### Data Exploration\n\nTarget variable is of type int, then I converted it to categorical variable (factor).\n\nThe target factor variable has 2 categories: 0 or 1. By counting total number of rows for each category, we get that George has 1020 favorite, and 997 disliked songs. Which is interesting, that number of disliked ones pretty close to liked. The music taste of George can be diverse, and Spotify's recommendation system might be actively adjusting to his preferences. Actually, when you dislike one song in Spotify, the system tries to not suggest you similar songs, and try other different options. To state this opinion constantly we need to explore more about song preferences of George. Furthermore, there are could be temporal patterns in George's preferences, for instance he prefer certain types of songs at different times of day, month or year.\n\n```{r}\ncolSums(is.na(spotify))\n```\n\nThere is no NA values in this dataset.\n\n```{r}\nsummary(spotify_2023)\n\nspotify_2023$danceability_. <- spotify_2023$danceability_./100\nspotify_2023$energy_. <- spotify_2023$energy_./100\nspotify_2023$speechiness_. <- spotify_2023$speechiness_./100\nspotify_2023$valence_. <- spotify_2023$valence_./100\nspotify_2023$acousticness_. <- spotify_2023$acousticness_./100\nspotify_2023$liveness_. <- spotify_2023$liveness_./100\n\nspotify_2023 <- spotify_2023 %>% rename(danceability=danceability_., energy=energy_., speechiness=speechiness_., valence=valence_., acousticness=acousticness_., liveness=liveness_., tempo=bpm)\n\nmy_song <- spotify_2023 %>% filter(track_name == 'Money Trees')\n```\n\nI converted the values in spotify_23 to decimal format. Also, applied the same changes to my_song by recreating it.\n\n### Data Partition\n\n```{r}\nset.seed(79)\nspotify.index <- sample(c(1:nrow(spotify)), nrow(spotify)*0.6)\nspotify_train.df <- spotify[spotify.index, ]\nspotify_valid.df <- spotify[-spotify.index, ]\n```\n\n```{r}\nliked <- spotify_train.df %>% filter(target==1)\ndisliked <- spotify_train.df %>% filter(target==0)\n\nt.test(liked$danceability, disliked$danceability)\nt.test(liked$tempo, disliked$tempo)\nt.test(liked$energy, disliked$energy)\nt.test(liked$speechiness, disliked$speechiness)\nt.test(liked$valence, disliked$valence)\nt.test(liked$acousticness, disliked$acousticness)\nt.test(liked$liveness, disliked$liveness)\n```\n\nBased on the results above, here is the list of variables that show significant difference: Danceability(p_value = 3.965e-09), speechiness(p-value = 3.461e-09), valence(p-value = 0.0005895), acousticness(p-value = 5.028e-05). Very low p-value suggests that, there is significant difference on this values between liked and disliked songs, making them main parameters to identify George's preferences in music. Other remaining variables have p-value more than typical threshold 0.05: tempo(p-value = 0.7416), energy(p-value = 0.3646), liveness(p-value = 0.1324).\n\n```{r}\nspotify_train.df <- spotify_train.df %>% select(-tempo, -energy, -liveness)\n```\n\nk-NN method draws information from similarities between the variables by measuring distance between records. Variables with similar values across different outcome classes cannot provide useful information for distinguishing between groups. Including such variables can lead to overfitting, where the model performs well on training data but fails to generalize to new data. These insignificant variables affect the distance calculation, making it harder to distinguish between groups.\n\n```{r}\nhead(spotify_train.df)\n```\n\n### Normalization\n\nIn this step we are normalizing only those columns that will be used in k-NN model building.\n\n```{r}\nlibrary(caret)\n\nspotify_train_norm.df <- spotify_train.df\nspotify_valid_norm.df <- spotify_valid.df\nspotify_norm.df <- spotify\nmy_song_norm <- my_song\n\n\nnorm_values <- preProcess(\n  spotify_train.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")], \n  method = c(\"center\", \"scale\"))\n\nspotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] <- \n  predict(norm_values, spotify_train.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_train_norm.df)\n\nspotify_valid_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] <- \n  predict(norm_values, spotify_valid.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_valid_norm.df)\n\nspotify_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] <- \n  predict(norm_values, spotify[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\n# View(spotify_norm.df)\n\nmy_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")] <- \n  predict(norm_values, my_song[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")])\nmy_song_norm\n```\n\n### Clustering\n\n```{r}\n#| echo: false\nlibrary(FNN)\n# knn is all about numeric data, classification using numeric values\nnn <- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = my_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],##what we are classifying: like or dislike\n           k=7)\nnn\nnn_indexes <- row.names(spotify_train.df)[attr(nn, \"nn.index\")]\nspotify_train.df[nn_indexes, ] %>% select(song_title, artist, target, acousticness, danceability, speechiness, valence)\n```\n\nBy running k-NN model as a result get \"1\", which indicates that George will like my song. And by listing 7 nearest neighbors, I see that my song is also in this list and George already marked it as favorite. Within this songs, George marked only one song as disliked, which highlights not all similar songs are guaranteed to be liked. This disliked song has high valence value compared to others, but there is no difference in other variables. By running knn classification we get 7 nearest records with low distance value from our selected song. So if we just use numbers these songs look very similar to each other. But they are not. And the diversity of artists suggests George's musical preferences are varied.\n\n```{r}\naccuracy.df <- data.frame(k = seq(1,14,1), accuracy = rep(0,14))\n\nfor(i in 1:14) {\n  knn.pred <- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = spotify_valid_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],\n           k=i)\n  \n  accuracy.df[i, 2] <- confusionMatrix(knn.pred, spotify_valid_norm.df[ ,c(\"target\")])$overall['Accuracy']\n}\n\naccuracy.df[order(-accuracy.df$accuracy), ]\n```\n\nFrom the list above we can see accuracy for different k values between 1 and 14. We can see that the difference in accuracy between values is very small. k=14 has highest accuracy value 0.6183395, also k=5 provides very similar number 0.6022305.\n\n```{r}\n#| echo: false\nlibrary(ggplot2)\n\nggplot(accuracy.df, aes(x=k, y=accuracy)) + \n  geom_point() +\n  geom_line() +\n  labs(title = \"Scatterplot of k values vs Accuracy\",\n       x = \"k values\",\n       y = \"Accuracy\") +\n  scale_x_continuous(breaks = seq(min(accuracy.df$k), max(accuracy.df$k), by = 3))\n```\n\nThe graph clearly illustrates the differences in accuracy across various k-values. k = 10 has about 61% accuracy, similar to k = 12 and k = 13. Since they give the same result, k = 10 is a better choice to reduce noise. Additionally, the previously used k = 7 had one of the lowest accuracy scores at 59%. While k = 14 had the highest accuracy at 62%, k = 10 appears to be a more balanced choice. Selecting 10 nearest neighbors should provide a more reliable classification of my song.\n\n```{r}\nnn_10 <- knn( train = spotify_train_norm.df[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           test = my_song_norm[, c(\"acousticness\", \"danceability\", \"speechiness\", \"valence\")],\n           cl = spotify_train_norm.df[,c(\"target\")],##what we are classifying: like or dislike\n           k=10)\nnn_10\nnn_indexes_10 <- row.names(spotify_train.df)[attr(nn_10, \"nn.index\")]\nspotify_train.df[nn_indexes_10, ] %>% select(song_title, artist, target, acousticness, danceability, speechiness, valence)\n```\n\nI chose k=10 as optimal with moderate accuracy value. The output of model didn't change, it indicates George will like my song. But for now I got 10 nearest neighbors, and from this new list George disliked 3 songs. All these songs have high value of danceability around 70%, low speechiness and acousticness. All 3 disliked songs as for k=7, have higher value of valence compared to others. Higher valence indicates more positive, cheerful, or euphoric songs. It seems that George might prefer songs with lower valence, which are less positive, more neutral in mood or moodier over cheerful ones. Disliked songs have relatively low acousticness, this suggest that George prefer songs with slightly more acoustic elements. The danceability is quite similar for both groups, which implies this factor is not strong in determining preferences. The disliked songs have relatively low speechiness, and some liked songs have higher speechiness ('Pacifier' has 0.1240) indicating George prefer songs with more spoken lyrics or rap.\n\n### Limitations of model\n\nI think main limitation here is that we are relying on numerical variables to predict whether someone will like this song or not. There are can be other factors such as good memories or associations with a song which can make them favorite. Also lyrics play main role in connecting with listeners on an emotional level. For instance, I tend to prefer songs with meaningful lyrics, while rap elements often give me an energy boost. Additionally, music preferences can vary based on context—what I listen to at the gym or while walking differs from what I play in the evening when I can't sleep."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"analysis.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","theme":["cosmo","brand"],"title":"K-Nearest Neighbors: You Like This Song...But Will George Like It?","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}